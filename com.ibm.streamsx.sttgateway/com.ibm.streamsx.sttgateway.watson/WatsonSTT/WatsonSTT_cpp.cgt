/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2019
==============================================
*/

/*
============================================================
First created on: Jul/01/2018
Last modified on: Nov/06/2019

Please refer to the sttgateway-tech-brief.txt file in the 
top-level directory of this toolkit to read about 
what this toolkit does, how it can be built and 
how it can be used in the Streams applications.

High-level business need for this operator is explained here:
https://github.com/IBMStreams/administration/issues/136
============================================================
*/

/* Additional includes go here */
// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#advantages
// https://docs.websocketpp.org/index.html
// This operator heavily relies on the Websocket++ header only library.
// This C++11 library code does the asynchronous full duplex Websocket communication with
// the Watson STT service via a series of event handlers (a.k.a callback methods).
// Bulk of the logic in this operator class appears in those event handler methods below.
#include <websocketpp/config/asio_client.hpp>
#include <websocketpp/client.hpp>

#include <boost/algorithm/string/predicate.hpp>
#include <boost/exception/to_string.hpp>
#include <boost/thread/thread.hpp>

// A nice read in this URL about using property_tree for JSON parsing:
// http://zenol.fr/blog/boost-property-tree/en.html
#include <boost/property_tree/ptree.hpp>
#include <boost/property_tree/json_parser.hpp>

// Short alias for this namespace
namespace pt = boost::property_tree;

using websocketpp::lib::placeholders::_1;
using websocketpp::lib::placeholders::_2;
using websocketpp::lib::bind;

// Verify the input tuple attribute name and then read the operator parameters.
<%
	# Check the input port 0 i.e. the first input port. 
	my $inputPort = $model->getInputPortAt(0);
	my $inputTupleName = $inputPort->getCppTupleName();
	my $audioInputAsBlob = undef;	
	my $inputAttrs = $inputPort->getAttributes();
	my $speechAttributeFound = 0;
	
	foreach my $inputAttr (@$inputAttrs) {
		my $inAttrName = $inputAttr->getName();
		my $inAttrType = $inputAttr->getSPLType();
		
		if ($inAttrName eq "speech") {
			$speechAttributeFound = 1;
			
			if ($inAttrType eq "rstring") {
				# This tuple attribute will carry the name of a file containing the audio data.
				$audioInputAsBlob = 0;
			}
			
			if ($inAttrType eq "blob") {
				# This tuple attribute will carry a blob containing either full or partial audio data.
				$audioInputAsBlob = 1;
			}
		}
	}
	
	if ($speechAttributeFound == 0 ) {
		SPL::CodeGen::exitln("WatsonSTT_cpp.cgt: The required input tuple attribute 'speech' is missing in the first input port.");
	}
	
	if (!(defined($audioInputAsBlob))) {
		SPL::CodeGen::exitln("WatsonSTT_cpp.cgt: The required input tuple attribute 'speech' is not of type 'rstring' or 'blob' in the first input port.");
	}
	
	# Check the input port number 1 i.e. the second input port.
	my $inputPort2 = $model->getInputPortAt(1);
	my $inputTupleName2 = $inputPort2->getCppTupleName();
	my $accessTokenAsString = undef;	
	my $inputAttrs2 = $inputPort2->getAttributes();
	my $accessTokenAttributeFound = 0;
	
	foreach my $inputAttr2 (@$inputAttrs2) {
		my $inAttrName2 = $inputAttr2->getName();
		my $inAttrType2 = $inputAttr2->getSPLType();
		
		if ($inAttrName2 eq "access_token") {
			$accessTokenAttributeFound = 1;
			
			if ($inAttrType2 eq "rstring") {
				# This tuple attribute will carry the IAM access token.
				$accessTokenAsString = 1;
			}
		}
	}
	
	if ($accessTokenAttributeFound == 0 ) {
		SPL::CodeGen::exitln("WatsonSTT_cpp.cgt: The required input tuple attribute 'accessToken' is missing in the second input port.");
	}
	
	if (!(defined($accessTokenAsString))) {
		SPL::CodeGen::exitln("WatsonSTT_cpp.cgt: The required input tuple attribute 'accessToken' is not of type 'rstring' in the second input port.");
	}
	
	# Following are the operator parameters.
    my $uri = $model->getParameterByName("uri");
    $uri = $uri->getValueAt(0)->getCppExpression();

    my $baseLanguageModel = $model->getParameterByName("baseLanguageModel");
    $baseLanguageModel = $baseLanguageModel->getValueAt(0)->getCppExpression();

	# Record the optional parameter values if present. Otherwise, set the default value.
    my $contentType = $model->getParameterByName("contentType");
    $contentType = $contentType ? $contentType->getValueAt(0)->getCppExpression() : "";

    my $sttResultMode = $model->getParameterByName("sttResultMode");
	# Default: 3 for full text result
    $sttResultMode = $sttResultMode ? $sttResultMode->getValueAt(0)->getCppExpression() : 3;
            
    my $sttRequestLogging = $model->getParameterByName("sttRequestLogging");
    $sttRequestLogging = $sttRequestLogging ? $sttRequestLogging->getValueAt(0)->getCppExpression() : 0;
    
    my $baseModelVersion = $model->getParameterByName("baseModelVersion");
    $baseModelVersion = $baseModelVersion ? $baseModelVersion->getValueAt(0)->getCppExpression() : "";
    
    my $customizationId = $model->getParameterByName("customizationId");
    $customizationId = $customizationId ? $customizationId->getValueAt(0)->getCppExpression() : "";
    
    my $customizationWeight = $model->getParameterByName("customizationWeight");
    $customizationWeight = $customizationWeight ? $customizationWeight->getValueAt(0)->getCppExpression() : 9.9;
    
    my $acousticCustomizationId = $model->getParameterByName("acousticCustomizationId");
    $acousticCustomizationId = $acousticCustomizationId ? $acousticCustomizationId->getValueAt(0)->getCppExpression() : "";

    my $filterProfanity = $model->getParameterByName("filterProfanity");
    $filterProfanity = $filterProfanity ? $filterProfanity->getValueAt(0)->getCppExpression() : 0;

    my $sttJsonResponseDebugging = $model->getParameterByName("sttJsonResponseDebugging");
    $sttJsonResponseDebugging = $sttJsonResponseDebugging ? $sttJsonResponseDebugging->getValueAt(0)->getCppExpression() : 0;

    my $maxUtteranceAlternatives = $model->getParameterByName("maxUtteranceAlternatives");
	# Default: 1 for a single result in the JSON alternatives array.
    $maxUtteranceAlternatives = $maxUtteranceAlternatives ? $maxUtteranceAlternatives->getValueAt(0)->getCppExpression() : 1;

    my $wordAlternativesThreshold = $model->getParameterByName("wordAlternativesThreshold");
	# Default: 0.0 to distable the wordAlternatives feature (a.k.a Confusion Networks).
    $wordAlternativesThreshold = $wordAlternativesThreshold ? $wordAlternativesThreshold->getValueAt(0)->getCppExpression() : 0.0;

    my $wordConfidenceNeeded = $model->getParameterByName("wordConfidenceNeeded");
    $wordConfidenceNeeded = $wordConfidenceNeeded ? $wordConfidenceNeeded->getValueAt(0)->getCppExpression() : 0;    
    
    my $wordTimestampNeeded = $model->getParameterByName("wordTimestampNeeded");
    $wordTimestampNeeded = $wordTimestampNeeded ? $wordTimestampNeeded->getValueAt(0)->getCppExpression() : 0;

    my $identifySpeakers = $model->getParameterByName("identifySpeakers");
    $identifySpeakers = $identifySpeakers ? $identifySpeakers->getValueAt(0)->getCppExpression() : 0;

    my $smartFormattingNeeded = $model->getParameterByName("smartFormattingNeeded");
    $smartFormattingNeeded = $smartFormattingNeeded ? $smartFormattingNeeded->getValueAt(0)->getCppExpression() : 0;

    my $keywordsSpottingThreshold = $model->getParameterByName("keywordsSpottingThreshold");
	# Default: 0.0 to distable the keywords spotting feature.
    $keywordsSpottingThreshold = $keywordsSpottingThreshold ? $keywordsSpottingThreshold->getValueAt(0)->getCppExpression() : 0.0;
	
    my $keywordsToBeSpotted = $model->getParameterByName("keywordsToBeSpotted");
    $keywordsToBeSpotted = $keywordsToBeSpotted ? $keywordsToBeSpotted->getValueAt(0)->getCppExpression() : undef;

    my $websocketLoggingNeeded = $model->getParameterByName("websocketLoggingNeeded");
    $websocketLoggingNeeded = $websocketLoggingNeeded ? $websocketLoggingNeeded->getValueAt(0)->getCppExpression() : 0;

    my $cpuYieldTimeInAudioSenderThread = $model->getParameterByName("cpuYieldTimeInAudioSenderThread");
	# Default: 0.001 second i.e. 1 millisecond of CPU yield time inside a tight loop.
    $cpuYieldTimeInAudioSenderThread = $cpuYieldTimeInAudioSenderThread ? $cpuYieldTimeInAudioSenderThread->getValueAt(0)->getCppExpression() : 0.001;

    my $waitTimeBeforeSTTServiceConnectionRetry = $model->getParameterByName("waitTimeBeforeSTTServiceConnectionRetry");
	# Default: 3.0 seconds.
    $waitTimeBeforeSTTServiceConnectionRetry = $waitTimeBeforeSTTServiceConnectionRetry ? $waitTimeBeforeSTTServiceConnectionRetry->getValueAt(0)->getCppExpression() : 3.0;

    my $maxAllowedConnectionAttempts = $model->getParameterByName("maxAllowedConnectionAttempts");
	# Default: 10 attempts
    $maxAllowedConnectionAttempts = $maxAllowedConnectionAttempts ? $maxAllowedConnectionAttempts->getValueAt(0)->getCppExpression() : 10;
    
    my $sttLiveMetricsUpdateNeeded = $model->getParameterByName("sttLiveMetricsUpdateNeeded");
    $sttLiveMetricsUpdateNeeded = $sttLiveMetricsUpdateNeeded ? $sttLiveMetricsUpdateNeeded->getValueAt(0)->getCppExpression() : 1;    
%>

<%SPL::CodeGen::implementationPrologue($model);%>

// Constructor
MY_OPERATOR::MY_OPERATOR() 
{
	// Custom metrics for this operator are already defined in the operator model XML file. 
	// Hence, there is no need to explicitly create them here.
	// Simply get the custom metrics already defined for this operator. 
	// We will update the Counter kind custom metrics when the operator starts.
	// We will update the Gauge kind custom metrics during transcription only when the 
	// sttLiveMetricsUpdateNeeded optional operator parameter is set to true.
	OperatorMetrics  & opm = getContext().getMetrics();
	nSTTResultModeMetric = & opm.getCustomMetricByName("nSTTResultMode");
	nWebsocketConnectionAttemptsMetric = & opm.getCustomMetricByName("nWebsocketConnectionAttempts");
	nFullAudioConversationsReceivedMetric = & opm.getCustomMetricByName("nFullAudioConversationsReceived");
	nFullAudioConversationsTranscribedMetric = & opm.getCustomMetricByName("nFullAudioConversationsTranscribed");
	
	numberOfAudioBlobFragmentsReceivedInCurrentConversation = 0;
	numberOfFullAudioConversationsReceived = 0;
	numberOfFullAudioConversationsTranscribed = 0;
	
	// Initialize the member variables as needed from the operator parameter values read above.	
	<%
		if ($contentType eq "") {
			print "contentType = \"audio/wav\";\n";
		} else {
			print "contentType = $contentType;\n";
		}
	
		if ($baseModelVersion eq "") {
			print "baseModelVersion = \"\";\n";
		} else {
			print "baseModelVersion = $baseModelVersion;\n";
		}
	
		if ($customizationId eq "") {
			print "customizationId = \"\";\n";
		} else {
			print "customizationId = $customizationId;\n";
		}	
		
		if ($acousticCustomizationId eq "") {
			print "acousticCustomizationId = \"\";\n";
		} else {
			print "acousticCustomizationId = $acousticCustomizationId;\n";
		}		
	%>

	uri = <%=$uri%>;	
	// It must be set via the second input stream for this operator.
	accessToken = "";
	
	baseLanguageModel = <%=$baseLanguageModel%>;	
	customizationWeight = <%=$customizationWeight%>;
	
    if (customizationId == "") {
		// No customization id configured. Hence, set the customization weight to 
    	// 9.9 which will be ignored by the C++ logic later in the on_open method.
    	customizationWeight = 9.9;
    }
	
	filterProfanity = <%=$filterProfanity%>;
	sttJsonResponseDebugging = <%=$sttJsonResponseDebugging%>;
	sttRequestLogging = <%=$sttRequestLogging%>;
	
	sttResultMode = <%=$sttResultMode%>;
	
	if (sttResultMode < 1 || sttResultMode > 3) {
		throw std::runtime_error(
			"WatsonSTT_cpp.cgt: Invalid value of " + 
			boost::to_string(sttResultMode) + " is given for the sttResultMode parameter." +
			" Valid value must be either 1 or 2 or 3.");
	}
		
	maxUtteranceAlternatives = <%=$maxUtteranceAlternatives%>;
	
	if (maxUtteranceAlternatives <= 0) {
		throw std::runtime_error(
			"WatsonSTT_cpp.cgt: Invalid value of " + 
			boost::to_string(maxUtteranceAlternatives) + " is given for the maxUtteranceAlternatives parameter." +
			" Valid value must be greater than 0.");
	}
	
	wordAlternativesThreshold = <%=$wordAlternativesThreshold%>;
	
	if (wordAlternativesThreshold < 0.0 || wordAlternativesThreshold >= 1.0) {
		throw std::runtime_error(
			"WatsonSTT_cpp.cgt: Invalid value of " + 
			boost::to_string(wordAlternativesThreshold) + " is given for the wordAlternativesThreshold parameter." +
			" Valid value must be greater than or equal to 0.0 and less than 1.0.");		
	}
		
	wordConfidenceNeeded = <%=$wordConfidenceNeeded%>;	
	wordTimestampNeeded = <%=$wordTimestampNeeded%>;
	identifySpeakers = <%=$identifySpeakers%>;
	smartFormattingNeeded = <%=$smartFormattingNeeded%>;
	
	keywordsSpottingThreshold = <%=$keywordsSpottingThreshold%>;

	if (keywordsSpottingThreshold < 0.0 || keywordsSpottingThreshold >= 1.0) {
		throw std::runtime_error(
			"WatsonSTT_cpp.cgt: Invalid value of " + 
			boost::to_string(keywordsSpottingThreshold) + " is given for the keywordsSpottingThreshold parameter." +
			" Valid value must be greater than or equal to 0.0 and less than 1.0.");		
	}
	
	<% if (defined($keywordsToBeSpotted)) { %>
		keywordsToBeSpotted = <%=$keywordsToBeSpotted%>;
	<% } %>

	// If the keywords to be spotted list is empty, then disable keywords_spotting.
	if (keywordsToBeSpotted.size() == 0) {
		keywordsSpottingThreshold = 0.0;
	}
	
	audioInputAsBlob = <%=$audioInputAsBlob%>;
	websocketLoggingNeeded = <%=$websocketLoggingNeeded%>;
	cpuYieldTimeInAudioSenderThread = <%=$cpuYieldTimeInAudioSenderThread%>;

	if (cpuYieldTimeInAudioSenderThread < 0.0) {
		throw std::runtime_error(
			"WatsonSTT_cpp.cgt: Invalid value of " + 
			boost::to_string(cpuYieldTimeInAudioSenderThread) + " is given for the cpuYieldTimeInAudioSenderThread parameter." +
			" Valid value must be greater than or equal to 0.0.");		
	}
	
	waitTimeBeforeSTTServiceConnectionRetry = <%=$waitTimeBeforeSTTServiceConnectionRetry%>;
	
	if (waitTimeBeforeSTTServiceConnectionRetry < 1.0) {
		throw std::runtime_error(
			"WatsonSTT_cpp.cgt: Invalid value of " + 
			boost::to_string(waitTimeBeforeSTTServiceConnectionRetry) + " is given for the waitTimeBeforeSTTServiceConnectionRetry parameter." +
			" Valid value must be greater than or equal to 1.0.");		
	}
	
	maxAllowedConnectionAttempts = <%=$maxAllowedConnectionAttempts%>;

	if (maxAllowedConnectionAttempts < 1) {
		throw std::runtime_error(
			"WatsonSTT_cpp.cgt: Invalid value of " + 
			boost::to_string(maxAllowedConnectionAttempts) + " is given for the maxAllowedConnectionAttempts parameter." +
			" Valid value must be greater than or equal to 1.");	
	}	

	sttLiveMetricsUpdateNeeded = <%=$sttLiveMetricsUpdateNeeded%>;
	
	// We are not going to support the following utterance based 
	// features when the STT result mode is 3 (full transcript).
	// Many of these features return the results in individual arrays for a 
	// given utterance. When we assemble the full transcript using
	// multiple utterances, it will be too much data to deal with and
	// it will prove to be not very useful in processing multiple
	// arrays to make sense out of what happened in the context of
	// a full transcript. Hence, we are disabling these features for
	// the STT result mode 3 (full transcript).
	if (sttResultMode == 3) {
		// No n-best utterance alternative hypotheses.
		maxUtteranceAlternatives = 1;
		// No Confusion Networks.
		wordAlternativesThreshold = 0.0;
		// No individual word confidences.
		wordConfidenceNeeded = false;
		// No individual timestamps.
		wordTimestampNeeded = false;
		//No speaker identification for the individual words in an utterance.
		identifySpeakers = false;
		//No keyword spotting inside an utterance.
		keywordsSpottingThreshold = 0.0;
	}
	
	// Update the operator metric.
	nSTTResultModeMetric->setValueNoLock(sttResultMode);
}

// Destructor
MY_OPERATOR::~MY_OPERATOR() 
{
    // Finalization code goes here
}

// To set the static variable present inside this method, 
// call this method with a non-null object i.e. this object as input.
// To get the value of the static variable present inside this method,
// call this method with a null object as input.
//
// NOTE: I originally built this method for a different reason to have
// static member methods as callback functions for the Websocket event handlers.
// From those static methods, this method was called to get the object pointer of
// this operator class to access the member variables like this:
// MY_OPERATOR::myOp(NULL)->some_member_variable
// Later (on Aug/28/2018) I removed the need for the static methods. 
// Hence, this method is no longer in use.
/*
MY_OPERATOR* MY_OPERATOR::myOp(MY_OPERATOR* obj) {
	static MY_OPERATOR* thisObj = NULL;

	if (obj == NULL) {
		return(thisObj);
	}
		
	thisObj = obj;
	return(thisObj);
}
*/

// Notify port readiness
void MY_OPERATOR::allPortsReady() 
{
	/*
	// Starting on Aug/28/2018, we are no longer using any
	// static methods in this operator. Hence, this code segment is commented out.
	//
	// Save the self (this) pointer to this operator instance in a 
	// static variable held inside a static method.
	// This will be used by all the Websocket message related thread methods and
	// the Websocket event handler methods present below in this file.
	myOp(this);
	*/
	
	operatorPhysicalName = getContext().getName();
	// This operator is usually part of a UDP parallel region in many application scenarios.
	udpChannelNumber = getContext().getChannel();
	wsConnectionEstablished = false;
	makeNewWebsocketConnection = false;
	// 0 = NO_AUDIO_DATA_SENT_TO_STT, 1 = PARITAL_BLOB_FRAGMENTS_BEING_SENT_TO_STT,
	// 2 = FULL_AUDIO_DATA_SENT_TO_STT
	statusOfAudioDataTransmissionToSTT = NO_AUDIO_DATA_SENT_TO_STT;
	transcriptionErrorOccurred = false;
	websocketConnectionErrorOccurred = false;
	numberOfWebsocketConnectionAttempts = 0;
	sttResultTupleWaitingToBeSent = false;
	transcriptionResult = "";
	
	// Initialize Websocket with the Watson STT service for the
	// very first time when this operator comes up. We are going to 
	// create a new thread that will do the Websocket initialization and
	// keep doing the Boost asio run loop inside of that thread.
	// That will make the Websocket operations to run on its own 
	// thread away from our Streams tuple processing thread.
    try {
    	// This C++11 technique to pass a class member method as a
    	// callback function is discussed here:
    	// http://coliru.stacked-crooked.com/a/860dd7e0bb98502d
    	//
    	// C++ boost thread reference: http://www.cplusplus.com/forum/beginner/152951/
    	auto callback1 = std::bind(&MY_OPERATOR::ws_init, this);
    	boost::thread* myThread1 = new boost::thread(callback1);
    } catch(std::exception& e) {
    	SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
			"-->Channel " << boost::to_string(udpChannelNumber) <<
			"-->Exception in creating the ws_init thread. " << 
			e.what(), "ws_init thread creation");
    	// We can't do much without this thread. Abort this operator now.
    	SPL::Functions::Utility::abort(__FILE__, __LINE__);
    }
    
    // Create another thread to periodically watch for the
    // audio data availability so that the audio blob can be 
    // sent to the Watson STT service via Websocket.
    // This thread will also monitor for any Websocket 
    // connection termination due to inactivity/session timeout or
    // due to invalid audio data that was sent for transcription which 
    // forced the Watson STT service to terminate the connection. 
    try {
    	auto callback2 = std::bind(&MY_OPERATOR::ws_audio_blob_sender, this);
    	boost::thread* myThread2 = new boost::thread(callback2);
    } catch(std::exception& e) {
    	SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
			"-->Channel " << boost::to_string(udpChannelNumber) <<
			"-->Exception in creating the ws_audio_blob_sener thread. " << 
			e.what(), "ws_audio_blob_sender thread creation");
    	// We can't do much without this thread. Abort this operator now.
    	SPL::Functions::Utility::abort(__FILE__, __LINE__);
    }
}
 
// Notify pending shutdown
void MY_OPERATOR::prepareToShutdown() 
{
	// Close the Websocket connection to the Watson STT service.
	// wsClient->get_alog().write(websocketpp::log::alevel::app, "Client is closing the Websocket connection to the Watson STT service.");
	SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
		"-->Channel " << boost::to_string(udpChannelNumber) <<
		"-->Client is closing the Websocket connection to the Watson STT service.",
		"prepareToShutdown");
	wsClient->close(wsHandle,websocketpp::close::status::normal,"");
}

// Processing for source and threaded operators   
void MY_OPERATOR::process(uint32_t idx)
{
    // A typical implementation will loop until shutdown
    /*
      while(!getPE().getShutdownRequested()) {
          // do work ...
      }
    */
}

// Tuple processing for mutating ports 
// This operator has two input ports.
// Port 0: Audio data (a file name or a blob) arrives on this port.
// Port 1: It is a control port where the IAM access token is
//         sent into this operator for connecting to the
//         STT service in a secure manner.
//
// With the WebSocket interface, audio data is always streamed to the 
// STT service over the connection. You can pass the blob data through the 
// Websocket all at once, or you can pass data for the live-use case 
// as it becomes available.
//
// Since the Websocket interface keeps its request/response communication to 
// the STT service completely asynchronus on the full duplex TCP connection,
// our tuple processing here is going to be different from the way it is 
// usually done inside the Streams operators. 
// There will be multiple threads: One for receiving tuples on this 
// operator's input port, one for sending the audio data to the 
// STT service and another for receiving transcription responses from the 
// STT service as well as to send the output tuples on this operator's 
// output port. It is somewhat different from the usual stuff and 
// very involved in terms of the logic. So, read the code completely to 
// understand what is going on inside all these threads.
void MY_OPERATOR::process(Tuple & tuple, uint32_t port)
{
	// There are multiple methods (process, audio_blob_sender and on_message) that
	// regularly access (read, write and delete) the vector member variables.
	// All those methods work in parallel inside their own threads.
	// To make that vector access thread safe, we will use this mutex.
	SPL::AutoMutex autoMutex(sttMutex1);

	// Let is first process if the IAM access token is sent via port 1 i.e. second input port.
	if (port == 1) {
		// Use this variable if needed in the future.
		IPort1Type const & <%=$inputTupleName2%> = static_cast<IPort1Type const&>(tuple);
		// Save the access token for subsequent use within this operator.
		ValueHandle handle1 = tuple.getAttributeValue("access_token");
		rstring & accessToken_rstring = handle1;
		accessToken = accessToken_rstring.string();
		SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
			"-->Channel " << boost::to_string(udpChannelNumber) <<
			"-->Received new/refreshed access token.", "process");
		return;
	}
	
	// This must be the audio data arriving here via port 0 i.e. first input port.
	// If we have a non-empty IAM access token, process the audio data.
	// Otherwise, skip it.
	if (accessToken == "") {
		SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
			"-->Channel " << boost::to_string(udpChannelNumber) <<
			"-->Ignoring the received audio data at this time due " <<
			"to an empty IAM access token. User must first provide "
			" the IAM access token before sending any audio data to this operator.",
			"process");
		return;
	}

	// Use this variable if needed in the future.
	IPort0Type const & <%=$inputTupleName%> = static_cast<IPort0Type const&>(tuple);
	
	// If this operator is configured to receive audio blob data instead of 
	// an audio filename, then the caller could send the blob in different
	// fragments as the real time conversation is happening. So, keep a
	// count of the audio blob fragments in a given audio conversation.
	// In addition, save the blob data related information in a
	// vector member variable to be used later in the audio blob sender and
	// in the on_message methods.
	if (audioInputAsBlob == true) {
		// Speech data is sent into this operator via a blob buffer rather than via a file.
		// Release the blob data so that we can own the memory management of that data.
		// Get the blob's data pointer and store it in a vector for later use in the
		// Websocket data sender and receiver threads. In addition, store the size of
		// the blob as well in another vector which will be used later while sending the
		// blob data to the STT service.
		//
		// We can keep streaming the blob based speech data to the STT service.
		// So the signal for the end of the audio data will be sent to the
		// STT service with a special logic in the case of blob based speech data.
		// Here is how it works.
		// SPL code where this operator is invoked must send an emply blob 
		// immediately following the very last fragment of the speech blob data.
		// When an empty blob arrives, we will insert a null pointer into the vector.
		// When we encounter that NULL pointer in the audio blob sender method, 
		// we will inform the STT service by signaling the end of the audio data.
		// It is very important for the SPL code to send ONLY ONE empty blob at the 
		// end of transmitting all the non-empty blob content belonging to a 
		// given audio either in full or in partial fragments.
		//
		unsigned char * ptr = NULL;
		ValueHandle handle0 = tuple.getAttributeValue("speech");
		SPL::blob & speechBlob = handle0;
		uint64_t sizeOfBlob = SPL::Functions::Collections::blobSize(speechBlob);

		// We can allow an end of audio indicator (i.e. an empty blob) only 
		// if we are already in the process of receiving one or more 
		// blob fragments for an ongoing speech conversation. 
		// If an empty blob arrives from the caller of this operator 
		// (i.e. SPL code) abruptly when there is no ongoing speech conversation,
		// ignore that one without adding it to the vector.
		if (sizeOfBlob == 0 && 
			numberOfAudioBlobFragmentsReceivedInCurrentConversation == 0) {
			// This is not allowed. Ignore this empty blob.
			return;
		}
		
		if (sizeOfBlob > 0) {
			audioBytes.push_back(speechBlob.releaseData(sizeOfBlob));
			numberOfAudioBlobFragmentsReceivedInCurrentConversation++;
		} else {		
			// Append a NULL pointer to indicate the end of blob data for a given audio.
			audioBytes.push_back(ptr);
			// All blob fragments for the current audio conversation have been received.
			numberOfAudioBlobFragmentsReceivedInCurrentConversation = 0;
			numberOfFullAudioConversationsReceived++;
		}
		
		audioSize.push_back(sizeOfBlob);
	} else {
		// Speech data is sent into this operator via a file rather than via a blob buffer.
		ConstValueHandle handle0 = tuple.getAttributeValue("speech");
		rstring const & fileName_rstring = handle0;
		std::string const & audioFileName = fileName_rstring.string();
		
		// Push this file name to the vector so that we can 
		// get it back when the transcription result arrives.
		// This audio data will be picked up for processing by the
		// ws_audio_blob_sender (thread) method.
		audioFiles.push_back(audioFileName);
		numberOfFullAudioConversationsReceived++;
	} // End of if (audioInputAsBlob == true)

	// Update the operator metric only if the user asked for a live update.
	if (sttLiveMetricsUpdateNeeded == true) {
		nFullAudioConversationsReceivedMetric->setValueNoLock(numberOfFullAudioConversationsReceived);
	}
	
	// Let us do all the auto output tuple attribute assignments and store it in a
	// list to be used in the on_message event handler method below at the time of
	// either full or partial transcription gets completed for a given audio data.
	// We will create a dynamic oTuple object via the new C++ construct so that
	// the object pointer can be stored in the oTupleList below.
	// For every new audio (either a new audio filename or the very first 
	// audio blob fragment sent here, we will create a new oTuple object.
	if (audioInputAsBlob == false ||
		(audioInputAsBlob == true &&
		 numberOfAudioBlobFragmentsReceivedInCurrentConversation == 1)) {	
		OPort0Type *oTuple = new OPort0Type;
		
		if (oTuple == NULL) {
			// Object instantiation (memory allocation) error.
			throw std::runtime_error(
				"WatsonSTT_cpp.cgt: Unable to create a new oTuple object in the process method.");
		}
		
		<% 
			my $oport = $model->getOutputPortAt(0); 
		
			foreach my $attribute (@{$oport->getAttributes()}) { 
				my $name = $attribute->getName(); 
				# print "// $name\n";
				my $operation = $attribute->getAssignmentOutputFunctionName();
				# print "// $operation\n";
				if ($operation eq "AsIs") { 
					my $init = $attribute->getAssignmentOutputFunctionParameterValueAt(0)->getCppExpression();
		%> 
					oTuple->set_<%=$name%>(<%=$init%>);
		<%
				}
			}
		%>
	
		// Push this partially filled oTuple object's pointer to the 
		// vector so that we can get it back when the transcription result arrives.
		oTupleList.push_back(oTuple);
	} // End of if (audioInputAsBlob == false ||
}

// Tuple processing for non-mutating ports
void MY_OPERATOR::process(Tuple const & tuple, uint32_t port)
{
}

// Punctuation processing
void MY_OPERATOR::process(Punctuation const & punct, uint32_t port)
{
	  // Forward the punctuation markers on our first output port.
      if(punct==Punctuation::WindowMarker) {
        submit(punct, 0);
      } else if(punct==Punctuation::FinalMarker) {
        submit(punct, 0);
      }
}

// This method provides a thread for the Websocket audio blob sender
// as well as the Websocket connection termination monitor.
void MY_OPERATOR::ws_audio_blob_sender() {
	struct stat fileStat;   

	while (!getPE().getShutdownRequested()) {
		// Keep waiting in this while loop until
		// there is some work that needs to be performed.
		// Wait for a configured amount of time that is not 0.0 when there is
		// audio data actively available for processing.
		// When there is no audio data available, yield the CPU for
		// slightly a longer time.
		int32_t audioBytesVectorSize = audioBytes.size();
		int32_t audioFilesVectorSize = audioFiles.size();
		if(audioBytesVectorSize <= 0 && audioFilesVectorSize <= 0) {
			// There is no audio data available at this time.
			// Yield the CPU for slightly a longer time.
			// 1 second instead of 1 msec.
			SPL::Functions::Utility::block(1.0);
		} else if (cpuYieldTimeInAudioSenderThread > 0.0) {
			// Audio data available for processing. Yield the CPU briefly and get to work soon.
			// Even a tiny value of 1 millisecond (0.001 second) will yield the
			// CPU and will not show 0% idle in the Linux top command.
			SPL::Functions::Utility::block(cpuYieldTimeInAudioSenderThread);
		}
		
		// Check if the Websocket connection needs to be established.
		if ((audioBytesVectorSize > 0 || audioFilesVectorSize > 0) && 
			wsConnectionEstablished == false) {
			// When there is audio data waiting to be processed and
			// if the Websocket connection is not active at that time, 
			// it could be due to one of these reasons.
			//
			// 1) Very first connection to STT service has not yet been
			//	  made since the time this operator came alive.
			// 2) Due to inactivity or session timeout, STT service may have terminated the connection. 
			// 3) Invalid audio data was sent to STT earlier and STT rejected that 
			//    invalid data and terminated the Websocket connection.
			// 4) Other network or remote STT service system error may have
			//    caused a connection termination.
			//
			// In that case, we will try to reestablish the connection.
			// There is audio data waiting in the vector. Reestablish the connection.
			SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) <<
				"-->Audio data is waiting to be processed." << 
				" Establishing the Websocket connection to the Watson STT service now.",
				"establish_ws_connection");

			// If we have already done enough connection retry attempts, we will stop
			// making any more connection attempts. In this case, this operator will
			// simply go idle without doing any transcription work. (Alternatively,
			// we can throw an exception. That is something to do later if really needed.)
			if (numberOfWebsocketConnectionAttempts >= maxAllowedConnectionAttempts) {
				// Block for a minute or until the operator shutdown is requested.
				SPL::Functions::Utility::block(60.00);
				// In this situation, there is no work to do. Simply loop around.
				continue;
			}

			std::string msg = "Operator " + operatorPhysicalName +
				"-->Channel " + boost::to_string(udpChannelNumber) + 
				"-->Websocket connection attempt " +
				boost::to_string(++numberOfWebsocketConnectionAttempts) + 
				" to the Watson STT service.";
			SPLAPPTRC(L_ERROR,  msg, "ws_connection_attempt");
			websocketConnectionErrorOccurred = false;
			makeNewWebsocketConnection = true;
			// Update the operator metric.
			nWebsocketConnectionAttemptsMetric->setValueNoLock(numberOfWebsocketConnectionAttempts);
			
			// After a successful connection, makeNewWebsocketConnection will be
			// set to false in the ws_init method below.
			// Successful connection negotiation will invoke the on_open 
			// event handler below which will exchange the start session message
			// with the STT service. On a successful session establishment,
			// STT will send a response message which will be received in the
			// on_message event handler below which will set the 
			// wsConnectionEstablished to true.
			// Wait until the Websocket connection is fully made as explained above.
			while(wsConnectionEstablished == false) {				
				if (websocketConnectionErrorOccurred == true) {
					// We will do a total of max allowed connection attempts.
					if (numberOfWebsocketConnectionAttempts >= maxAllowedConnectionAttempts) {
						// This means some problem occurred during the attempt to
						// establish the Websocket connection in the on_open method.
						// We can't proceed further at this point.
						msg = "Operator " + operatorPhysicalName +
							"-->Channel " + boost::to_string(udpChannelNumber) + 
							std::string("-->WatsonSTT_cpp.cgt: Unable to establish a Websocket connection to ") +
							std::string("the STT service. It could be either due to network issues or ") +
							std::string("invalid service URL or invalid access token or ") +
							std::string("invalid configuration done for the STT service parameters.") +
							std::string(" Giving up after 10 attempts. This operator instance will ") + 
							std::string(" be idle from this point onwards without doing any transcription work.");
						SPLAPPTRC(L_ERROR, msg, "ws_connection_attempt_failure");
					} else {
						// Back off for a few seconds and then try to connect again.
						SPL::Functions::Utility::block(waitTimeBeforeSTTServiceConnectionRetry);
					} // End of if (numberOfWebsocketConnectionAttempts > maxAllowedConnectionAttempts)
					
					// Break from this inner loop and then continue the 
					// outer loop to retry the connection attempt.
					break;
				} // End of if (websocketConnectionErrorOccurred == true)
				
				SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
					"-->Channel " << boost::to_string(udpChannelNumber) <<
					"-->Reached 9", "reestablish_ws_connection");
				SPL::Functions::Utility::block(0.500);
				SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
					"-->Channel " << boost::to_string(udpChannelNumber) <<
					"-->Reached 10", "reestablish_ws_connection");
			} // End of the while loop.
			
			// Continue from the top of the outer loop.
			continue;
		} // End of the if segment

		// It is a special delay to give enough time for the 
		// Websocket thread's on_close event handler method below to 
		// complete its cleanup during a connection termination
		// happening as a result of an abnormal transcription error.
		// Read more commentary about it in the next if segment.
		if (transcriptionErrorOccurred == true) {
			// Wait for 600 milliseconds.
			SPL::Functions::Utility::block(0.600);
			transcriptionErrorOccurred = false;
			// Continue at the top of the while loop to reestablish the WS connection.
			continue;
		}
		
		// Check if there is audio data waiting to be sent to the STT service.
		if ((audioBytesVectorSize > 0 || audioFilesVectorSize > 0) && 
			wsConnectionEstablished == true &&
			statusOfAudioDataTransmissionToSTT != FULL_AUDIO_DATA_SENT_TO_STT) {
			// If an error happened in the STT processing (invalid audio data or
			// other system error), it most likely will force the Watson STT
			// service to terminate the Websocket connection. In that case, Websocket
			// thread may be currently inside its on_close event handler method
			// below in the middle of doing a cleanup. If this sender thread
			// accidentally enters into this if segment of code due to inter-thread
			// racing conditions, we shouldn't proceed further with sending 
			// audio data in order to avoid the Websocket "invalid state" exception.
			// We will simply continue our outer while loop thereby to force a 
			// short delay and then reestablish the Websocket connection.
			// This is simply defensive coding to avoid that "invalid state" exception.
			if (transcriptionErrorOccurred == true) {
				continue;
			}

			// There are multiple methods (process, audio_blob_sender and on_message) that
			// regularly access (read, write and delete) the vector member variables.
			// All those methods work in parallel inside their own threads.
			// To make that vector access thread safe, we will use this mutex.
			SPL::AutoMutex autoMutex(sttMutex1);
			
			if (audioInputAsBlob == false) {
				// Speech data is sent into this operator via a file rather than via a blob buffer.			
				std::string audioFileName = audioFiles.at(0);
				SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
					"-->Channel " << boost::to_string(udpChannelNumber) <<
					"-->Sending file " << audioFileName , "ws_audio_blob_sender");
				// Check for the file existence before attempting to read the audio data.
				// In C++, this is a very fast way to check file existence.
				// https://stackoverflow.com/questions/12774207/fastest-way-to-check-if-a-file-exist-using-standard-c-c11-c
				int32_t fileStatReturnCode = stat(audioFileName.c_str(), &fileStat);
				// int32_t fileSize = fileStat.st_size;
				
				if (fileStatReturnCode != 0) {
					// File doesn't exist.
					// Log this information and remove this audio data from the vector.
					std::string errorMsg = "Operator " + operatorPhysicalName +
						"-->Channel " + boost::to_string(udpChannelNumber) +
						"-->Audio file not found. Skipping STT task for this file: " +
						audioFileName;
					SPLAPPTRC(L_ERROR, errorMsg, "ws_audio_blob_sender");
					
					if (oTupleList.size() > 0) {
						// Set the error message attribute via the corresponding output function.
						<% 
						my $oport = $model->getOutputPortAt(0); 
						foreach my $attribute (@{$oport->getAttributes()}) { 
							my $name = $attribute->getName(); 
							my $paramValues = $attribute->getAssignmentOutputFunctionParameterValues();
							my $operation = $attribute->getAssignmentOutputFunctionName(); 

							if ($operation eq "getSTTErrorMessage") { 					  
						%> 
							oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(errorMsg)); 
							<%} 
						}%> 	  

						// Send this tuple that carries the audio file not found error message.
						if (sttJsonResponseDebugging == false) {
							submit(*(oTupleList.at(0)), 0);
						}
						
						// Free the oTuple object since it is no longer needed.
						delete oTupleList.at(0);
						// Remove that vector element as well.
						oTupleList.erase(oTupleList.begin() + 0);
						// We should account for this very rare missing file that 
						// we attempted to process it. That way, it will tally correctly
						// with the total conversations received versus the ones we handled.
						numberOfFullAudioConversationsTranscribed++;
					}					
				} else {				
					// Audio file exists. Read binary file into buffer
					std::ifstream input(audioFileName.c_str(), std::ios::binary);
					std::vector<char> buffer((std::istreambuf_iterator<char>(input)), 
						(std::istreambuf_iterator<char>()));
	
					// Send the blob read from an audio file to the STT service.
					// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSaudio
					// c->get_alog().write(websocketpp::log::alevel::app, "Sent binary Message: " + boost::to_string(buffer.size()));  
					wsClient->send(wsHandle, buffer.data(), buffer.size(), websocketpp::frame::opcode::binary);
					// Signal end of the audio data.
					// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSstop
					wsClient->send(wsHandle, "{\"action\" : \"stop\"}" , websocketpp::frame::opcode::text);
					// In a file based audio data, the entire file content has been sent to the STT service at this time.
					// So set this flag to indicate that.
					statusOfAudioDataTransmissionToSTT = FULL_AUDIO_DATA_SENT_TO_STT;
				} // End of if (stat(audioFileName.c_str(), &buffer) != 0)
				
				// Erase the item pushed into the vector member (cache) earlier in the 
				// process method for this audio data. It is no longer needed.
				audioFiles.erase(audioFiles.begin() + 0);
				// Continue from the top of the while loop.
				continue;
			} else {
				//
				// We can keep streaming the blob based speech data to the STT service.
				// So the signal for the end of the audio data will be sent to the
				// STT service with a special logic in the case of blob based speech data.
				// Here is how it works.
				// SPL code where this operator is invoked must send an emply blob 
				// immediately following the very last fragment of the speech blob data.
				// When an empty blob arrives, we will insert a null pointer into the vector.
				// When we encounter that NULL pointer in the audio blob sender method, 
				// we will inform the STT service by signaling the end of the audio data.
				// It is very important for the SPL code to send ONLY ONE empty blob at the 
				// end of transmitting all the non-empty blob content belonging to a 
				// given audio either in full or in partial fragments.
				//
				unsigned char * buffer = audioBytes.at(0);
				uint64_t sizeOfBlob = audioSize.at(0);
				
				if (buffer == NULL) {
					// We reached the end of the blob data as sent/streamed from the SPL application.
					// Signal end of the audio data.
					// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSstop
					wsClient->send(wsHandle, "{\"action\" : \"stop\"}" , websocketpp::frame::opcode::text);
					// In a blob based audio data, the entire blob has been sent to the STT service at this time.
					// So set this flag to indicate that.
					statusOfAudioDataTransmissionToSTT = FULL_AUDIO_DATA_SENT_TO_STT;
				} else {
					// Speech data is sent into this operator via a blob buffer rather than via a file.
					// Send the blob data (either in full or in a partial fragment) to the STT service.
					// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSaudio
					// c->get_alog().write(websocketpp::log::alevel::app, "Sent binary Message: " + boost::to_string(buffer.size()));  
					wsClient->send(wsHandle, buffer, sizeOfBlob, websocketpp::frame::opcode::binary);
					statusOfAudioDataTransmissionToSTT = AUDIO_BLOB_FRAGMENTS_BEING_SENT_TO_STT;
				} // End of if (buffer == NULL)
				
				// Remove the items from the vector. It is no longer needed. Also free the original 
				// data pointer that we obtained from the blob in the process method.
				if (buffer != NULL) {
					delete buffer;
				}
				
				audioBytes.erase(audioBytes.begin() + 0);
				audioSize.erase(audioSize.begin() + 0);
				// Continue from the top of the while loop.
				continue;
			} // End of if (audioInputAsBlob == false)
		} // End of if ((audioBytesVectorSize > 0 ||
	} // End of the while loop	
}

// This method initializes the Websocket driver, TLS and then
// opens a connection. This is going to run on its own thread.
// See the commentary in the allPortsReady method above to
// understand our need to run it in a separate thread.
void MY_OPERATOR::ws_init() {
	bool wsClientExists = false;

	while (!getPE().getShutdownRequested()) { 
		if (makeNewWebsocketConnection == false) {
			// Keep waiting in this while loop until
			// a need arises to make a new Websocket connection.
			// 1 second wait.
			SPL::Functions::Utility::block(1.0);
			continue;
		}
		
		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
		std::string uri = this->uri;
		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-input#models
		uri += "?model=" + baseLanguageModel;
		https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-input#logging
		uri += "&x-watson-learning-opt-out=" + std::string(sttRequestLogging ? "false" : "true");
		
		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-input#version
		if (baseModelVersion != "") {
			uri += "&base_model_version=" + baseModelVersion;
		}	
		
		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-input#custom
		// At a time, only one LM customization can be specified.
		// LM custom model chaining is not available as of Aug/2018.
		if (customizationId != "") {
			uri += "&customization_id=" + customizationId;
		}	  		

		if (acousticCustomizationId != "") {
			uri += "&acoustic_customization_id=" + acousticCustomizationId;
		}
		
		uri += "&access_token=" + accessToken;
		wsConnectionEstablished = false;

		if (wsClientExists == true) {
			// If we are going to do a reconnection, then free the
			// previously created Websocket client object.
			delete wsClient;
		}
  
		try {
			SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) <<
				"-->Going to connect to " << uri, "ws_init");
			wsClient = new client();
			wsClientExists = true;

			// https://docs.websocketpp.org/reference_8logging.html
			// Set the logging policy as needed
			// Turn off or turn on selectively all the Websocket++ access interface and 
			// error interface logging channels. Do this based on how the user has
			// configured this operator.
			if (websocketLoggingNeeded == true) {
				// Enable certain error logging channels and certain access logging channels.
				wsClient->set_access_channels(websocketpp::log::alevel::frame_header);
				wsClient->set_access_channels(websocketpp::log::alevel::frame_payload);				
			} else {
				// Turn off both the access and error logging channels completely.
				wsClient->clear_access_channels(websocketpp::log::alevel::all);
				wsClient->clear_error_channels(websocketpp::log::elevel::all);
			}
	
			// Initialize ASIO
			wsClient->init_asio();

			// IBM Watson STT service requires SSL based communication.
			// Set this TLS handler.
			// This technique to pass a class member method as a callback function is from here:
			// https://stackoverflow.com/questions/34757245/websocketpp-callback-class-method-via-function-pointer
			wsClient->set_tls_init_handler(bind(&MY_OPERATOR::on_tls_init,this,wsClient,::_1));
			
			// Register our other event handlers.
			wsClient->set_open_handler(bind(&MY_OPERATOR::on_open,this,wsClient,::_1));
			wsClient->set_fail_handler(bind(&MY_OPERATOR::on_fail,this,wsClient,::_1));
			wsClient->set_message_handler(bind(&MY_OPERATOR::on_message,this,wsClient,::_1,::_2));
			wsClient->set_close_handler(bind(&MY_OPERATOR::on_close,this,wsClient,::_1));

			// Create a connection to the given URI and queue it for connection once
			// the event loop starts
			SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) <<
				"-->Reached 1", "ws_init");
			websocketpp::lib::error_code ec;
			// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-basic-request#using-the-websocket-interface
			client::connection_ptr con = wsClient->get_connection(uri, ec);
			SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) <<
				"-->Reached 2", "ws_init");

			wsClient->connect(con);
			// A new Websocket connection has just been made. Reset this flag.
			makeNewWebsocketConnection = false;
			SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) <<
				"-->Reached 3", "ws_init");
	
			// Start the ASIO io_service run loop
			wsClient->run();
			SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) <<
				"-->Reached 4", "ws_init");
		} catch (const std::exception & e) {
			std::cout << "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) << 
				"-->" << e.what() << std::endl;
			SPL::Functions::Utility::abort(__FILE__, __LINE__);
		} catch (websocketpp::lib::error_code e) {
			std::cout << "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) << 
				"-->" << e.message() << std::endl;
			SPL::Functions::Utility::abort(__FILE__, __LINE__);
		} catch (...) {
			std::cout << "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) <<
				"-->Other exception in WatsonSTT operator's Websocket initializtion." << std::endl;
			SPL::Functions::Utility::abort(__FILE__, __LINE__);
		}	
	} // End of while loop.
}

// When the Websocket connection to the Watson STT service is made successfully, 
// this callback method will be called from the websocketpp layer.
void MY_OPERATOR::on_open(MY_OPERATOR::client* c, websocketpp::connection_hdl hdl) {
	  SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
		  "-->Channel " << boost::to_string(udpChannelNumber) <<
		  "-->Reached 6", "on_open");
	  // On Websocket connection open, establish a session with the STT service.
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSstart
	  // We have to form a proper JSON message structure for the  
	  // STT recognition request start message with all the output features we want. 
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#output
	  std::string msg = "{\"action\" : \"start\"";
	  msg += ", \"content-type\" : \"" + contentType + "\"";

	  std::string interimResultsNeeded = "false";
	  
	  if (sttResultMode == 1 || sttResultMode == 2) {
		  // User configured it for either partial utterance or completed utterance.
		  interimResultsNeeded = "true";
	  }
	 
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#interim
	  msg += ", \"interim_results\" : " + interimResultsNeeded;
	  
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-input#timeouts
	  msg += ", \"inactivity_timeout\": -1";
	  	  
	  // Customization weight of 9.9 indicates that the user never configured this parameter in SPL.
	  // In that case, we can ignore sending it to the STT service.
	  // If it is not 9.9, then we must send it to the STT service.
	  if (customizationWeight != 9.9) {
		  std::ostringstream strs;
		  strs << customizationWeight;
		  
		  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-input#custom
		  msg += std::string(", \"customization_weight\" : ") + strs.str();
	  }	  
	  
	  std::string profanityFilteringNeeded = "true";
	  
	  if (filterProfanity == false) {
		  profanityFilteringNeeded = "false";
	  }
	 
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#profanity_filter
	  msg += ", \"profanity_filter\" : " + profanityFilteringNeeded;	
	  
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#max_alternatives
	  msg += ", \"max_alternatives\" : " + boost::to_string(maxUtteranceAlternatives); 
	  
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#word_alternatives
	  if(wordAlternativesThreshold > 0.0) {
		  msg += ", \"word_alternatives_threshold\" : " + boost::to_string(wordAlternativesThreshold);
	  }
	  
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#word_confidence
	  if (wordConfidenceNeeded == true) {
		  msg += ", \"word_confidence\" : true"; 
	  }
	  
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#word_timestamps
	  if (wordTimestampNeeded == true) {
		  msg += ", \"timestamps\" : true"; 
	  }
	  
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#speaker_labels
	  if (identifySpeakers == true) {
		  msg += ", \"speaker_labels\" : true"; 
	  }

	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#smart_formatting
	  if (smartFormattingNeeded == true) {
		  msg += ", \"smart_formatting\" : true"; 
	  }
	  
	  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#keyword_spotting
	  if (keywordsSpottingThreshold > 0.0) {
		  msg += ", \"keywords_threshold\" : " + boost::to_string(keywordsSpottingThreshold);
		  // We have to send all the keywords to be spotted as a JSON list.
		  msg += ", \"keywords\" : ["; 
		  
		  // Iterate through the list of keywords and add them to the request message structure.
		  for (SPL::int32 idx = 0; 
			  idx < keywordsToBeSpotted.size(); idx++) {
			  // Add a comma after every keyword string.
			  if (idx > 0) {
				  msg += ", ";
			  }
			  
			  // Add the keyword now.
			  msg += std::string("\"") +
				  keywordsToBeSpotted.at(idx) + 
				  std::string("\"");
		  }		  
		  
		  msg += "]";
	  }
	  
	  msg += std::string("}");

	  if (sttJsonResponseDebugging == true) {
		  std::cout << "Operator " << operatorPhysicalName <<
				"-->Channel " << boost::to_string(udpChannelNumber) << 
				"-->X53 Websocket STT recognition request start message=" << 
				msg << std::endl;
	  }
	  	  
	  c->send(hdl,msg,websocketpp::frame::opcode::text);
	  // Store this handle to be used from process and shutdown methods of this operator.
	  wsHandle = hdl;
	  // c->get_alog().write(websocketpp::log::alevel::app, "Sent Message: "+msg);
	  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
		  "-->Channel " << boost::to_string(udpChannelNumber) <<
		  "-->A recognition request start message was sent to the Watson STT service: " <<
		  msg, "on_open");
}

// This recursive templatized function with c++11 syntax is from the 
// C++ boost Q&A (how-to) technical discussion here:
// https://stackoverflow.com/questions/48407925/boostproperty-treeptree-accessing-arrays-first-complex-element?noredirect=1&lq=1
// It helps us to directly index an element in a JSON array returned by the Watson STT service.
// To use c++11 syntax in a Streams C++ operator, it is required to add this
// sc (Streams Compiler) option: --c++std=c++11
template <typename Tree>
Tree query(Tree& pt, typename Tree::path_type path) {
    if (path.empty())
        return pt;

    auto const head = path.reduce();

    auto subscript = head.find('[');
    auto name      = head.substr(0, subscript);
    auto index     = std::string::npos != subscript && head.back() == ']'
        ? std::stoul(head.substr(subscript+1))
        : 0u;

    auto matches = pt.equal_range(name);
    if (matches.first==matches.second)
        throw std::out_of_range("name:" + name);

    for (; matches.first != matches.second && index; --index)
        ++matches.first;

    if (index || matches.first==matches.second)
        throw std::out_of_range("index:" + head);

    return query(matches.first->second, path);
}

// Whenever a message (transcription result, STT service message or an STT error message) is
// received from the STT service, this callback method will be called from the websocketpp layer.
// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSexample
void MY_OPERATOR::on_message(MY_OPERATOR::client* c, websocketpp::connection_hdl hdl, message_ptr msg) {
  // c->get_alog().write(websocketpp::log::alevel::app, "Received Reply: "+msg->get_payload());
  //
  // This debugging variable is enabled/disabled via an operator parameter.
  // When enabled, it will simply display the full JSON message and 
  // a few important fields resulting from parsing that JSON message.
  // After the display is done, it will return without doing any
  // further tuple submission. This debugging mode is useful during
  // toolkit development to verify any new fields in the STT result JSON.
  // The if segments around this variable  is like a sandbox to do 
  // experiments with the STT JSON result.
  // During the development of this toolkit, this debug variable helped 
  // immensely to test the JSON parsing as well as to prepare the data to be
  // sent to the caller. Hence, I decided to leave this code here for future
  // needs. It will simply cost a few extra bytes in the operator's compiled
  // binary code and it will not cause a major performance impact during 
  // normal operation when disabled.
  // In general, you can enable this debug mode and send one file at a time to the STT
  // service and carefully observe all the messages that get returned back in order to
  // develop and fine-tune the JSON message parsing logic.
  bool fullTranscriptionCompleted = false;
  bool transcriptionResultAvailableForParsing = false;
  std::string sttErrorString = "";
  std::string payload = msg->get_payload();
  std::size_t found = payload.find("\"state\": \"listening\"");
  // STT error will have the following message format.
  // {"error": "unable to transcode data stream audio/wav -> audio/x-float-array "}
  std::size_t sttError = payload.find("\"error\": \"");
  SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
	  "-->Channel " << boost::to_string(udpChannelNumber) <<
	  "-->Reached 7", "on_message");
  
  if (sttError != std::string::npos) {
	  sttErrorString = payload;

	  if (sttJsonResponseDebugging == true) {
		  std::cout << "Operator " << operatorPhysicalName <<
			  "-->Channel " << boost::to_string(udpChannelNumber) << 
			  "-->X3 STT error message=" << payload << std::endl;
	  }
  } else if (found != std::string::npos && wsConnectionEstablished == false) {
	  // This is the "listening" response from the STT service for the 
	  // new recognition request that was initiated in the on_open method above.
	  // This response is sent only once for every new Websocket connection request made and
	  // not for every new transcription request made.
	  wsConnectionEstablished = true;
	  websocketConnectionErrorOccurred = false;
	  numberOfWebsocketConnectionAttempts = 0;
	  SPLAPPTRC(L_ERROR,  "Operator " << operatorPhysicalName <<
		  "-->Channel " << boost::to_string(udpChannelNumber) <<
		  "-->Websocket connection established with the Watson STT service.", "on_message");
	  SPLAPPTRC(L_DEBUG, "Operator " << operatorPhysicalName <<
		  "-->Channel " << boost::to_string(udpChannelNumber) <<
		  "-->Reached 8", "on_message");
	  return;
  } else if (found == std::string::npos && wsConnectionEstablished == true) {
	  // This must be the response for our audio transcription request.
	  // Keep accumulating the transcription result.
	  transcriptionResult += payload;
	  
	  if (sttJsonResponseDebugging == true) {
		  std::cout << "Operator " << operatorPhysicalName <<
			  "-->Channel " << boost::to_string(udpChannelNumber) << 
			  "-->X1 STT response payload=" << payload << std::endl;
	  }
	  
	  if (sttResultMode == 1 || sttResultMode == 2) {
		  // We can parse this partial or completed 
		  // utterance result now since the user has asked for it.
		  transcriptionResultAvailableForParsing = true;
	  } else {
		  // User didn't ask for the partial or completed utterance result.
		  // We will parse the full transcription result later when it is fully completed.
		  return;
	  }
  } else if (found != std::string::npos && wsConnectionEstablished == true) {
	  // This is the "listening" response from the STT service for the 
	  // transcription completion for the audio data that was sent earlier.
	  // This response also indicates that the STT service is ready to do a new transcription.
	  fullTranscriptionCompleted = true;

	  if (sttJsonResponseDebugging == true) {
		  std::cout << "Operator " << operatorPhysicalName <<
			  "-->Channel " << boost::to_string(udpChannelNumber) << 
			  "-->X2 STT task completion message=" << payload << std::endl;
	  }	  
	  
	  if (sttResultMode == 3) {
		  // Neither partial nor completed utterance results were parsed earlier.
		  // So, parse the full transcription result now.
		  transcriptionResultAvailableForParsing = true;
	  }
  }
  
  std::string audioFileName = "";
  int32_t utteranceNumber = 0;
  std::string utteranceText = "";
  SPL::list<SPL::rstring> utteranceAlternatives;
  SPL::list<SPL::list<SPL::rstring>> wordAlternatives;
  SPL::list<SPL::list<SPL::float64>> wordAlternativesConfidences;
  SPL::list<SPL::float64> wordAlternativesStartTimes;
  SPL::list<SPL::float64> wordAlternativesEndTimes;
  SPL::list<SPL::rstring> utteranceWords;
  SPL::list<SPL::float64> utteranceWordsConfidences;
  SPL::list<SPL::float64> utteranceWordsEndTimes;
  SPL::float64 utteranceStartTime = 0.0;
  SPL::float64 utteranceEndTime = 0.0;
  SPL::map<SPL::rstring, 
  	  SPL::list<SPL::map<SPL::rstring, SPL::float64>>> keywordsSpottingResults;
  SPL::list<SPL::map<SPL::rstring, SPL::float64>> keywordsSpottingResultsList;
  SPL::map<SPL::rstring, SPL::float64> keywordsSpottingResultsMap;
  
  bool final = false;
  float confidence = 0.0;
  std::string sttErrorMsg = "";
  std::string fullTranscriptionText = "";
  float cumulativeConfidenceForFullTranscription = 0.0;
  int32_t idx1 = -1;
  int32_t idx2 = -1;
  int32_t idx3 = -1;
  
  // Create a boost property tree root
  pt::ptree root;
  std::stringstream ss;

  // There are multiple methods (process, audio_blob_sender and on_message) that
  // regularly access (read, write and delete) the vector member variables.
  // All those methods work in parallel inside their own threads.
  // To make that vector access thread safe, we will use this mutex.
  SPL::AutoMutex autoMutex(sttMutex1);
  
  // If there is valid result from the STT service, parse it now.
  // This parsing can be very involved depending on what kind of
  // output features are configured for the STT service in the
  // on_open method above. There is intricate logic here due to 
  // the order in which different sections of the response JSON message arrive. 
  // So read and test this code often in a methodical fashion.  
  if (transcriptionResultAvailableForParsing == true || sttErrorString != "") {
	  // Send either the partial utterance or completed utterance or 
	  // full text result as an output tuple now.
	  // If there is an STT error, send that as well.
	  // The oTupleList size check here will ensure we will process only
	  // the STT errors happening during transcription and not the 
	  // STT errors happening during idle time in the absence of any audio data.
	  // We will also add here a condition not to process the STT errors in
	  // this if segment when such errors occur at the time of establishing 
	  // a Websocket connection to the STT service.
	  if (sttErrorString != "" && wsConnectionEstablished == true && 
		  oTupleList.size() > 0) {
		  // Parse the STT error string.
		  ss << sttErrorString;
		  // Reset the trascriptionResult member variable.
		  transcriptionResult = "";
		  // Due to this STT error, transcription for the given audio data will not continue.
		  // This flag will be checked inside the ws_audio_blob_sender (thread) method.
		  transcriptionErrorOccurred = true;		

		  // Load the json data in the boost ptree
		  pt::read_json(ss, root);	 
		  sttErrorMsg = root.get<std::string>("error");
		  
		  // Set the STT error message attribute via the corresponding output function.
		  <% 
		  my $oport = $model->getOutputPortAt(0); 
		  foreach my $attribute (@{$oport->getAttributes()}) { 
			  my $name = $attribute->getName(); 
			  my $paramValues = $attribute->getAssignmentOutputFunctionParameterValues();
			  my $operation = $attribute->getAssignmentOutputFunctionName(); 

			  if ($operation eq "getSTTErrorMessage") { 					  
		  %> 
			  oTupleList.at(0)->set_<%=$name%>( 
					<%=$operation%>(sttErrorMsg)); 
			  <%} 
		  }%> 	  

		  // This prepared tuple with the assigned STT message will be
		  // sent in the very last if segment in this method below.
	  } else if(transcriptionResultAvailableForParsing == true) {
		  // Parse the relevant fileds from the JSON transcription result.
		  ss << transcriptionResult;
		  // Reset the trascriptionResult member variable.
		  transcriptionResult = "";
		  // Load the json data in the boost ptree
		  pt::read_json(ss, root);

		  // Successful STT result JSON will be at a minimum in the format as shown below.
		  // {"results": [{"alternatives": [{"transcript": "name "}], "final": false}], "result_index": 0}
		  // 
		  
		  // If speaker labels is enabled in our current STT session, we have to
		  // check if we received any speaker ids in the STT response message.
		  // When this feature is enabled along with interim_results (which is always the
		  // case for our stt result mode 1 and 2), STT JSON response sometimes will
		  // contain just the speaker_labels field without the results array and result_index fields.
		  // Read the Watson STT service speaker labels documentation about it.
		  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#speaker_labels
		  // So, we will always read the speaker_labels field first on its own and
		  // store it in the member variables of this operator class instead of
		  // local variables as it is done below for all the other JSON response fields.
		  //
		  // 
		  // A MUST READ IMPORTANT FINDING FROM THE LOCAL LAB TESTS: 
		  // The way this operator is coded with interim_results always set to true for
		  // the STT result mode 1 and 2, speaker_labels will always come as a standalone
		  // JSON message right after the finalized utterance result i.e. 
		  // "results.final" field set to true.
		  // In some rare cases, there will be two consecutive speaker_labels messages one after the
		  // other with the first one carrying the full set of speaker_labels and the
		  // second one carrying the label for the very last word with its final field set to true.
		  // In the logic below, we are going to ignore this second speaker_labels message.
		  bool secondSpeakerLabelsMessageReceived = false;
		  bool firstSpeakerLabelTimeMatched = false;

		  while(identifySpeakers == true) {
			  ++idx1;
			  SPL::int32 speakerId = -999;
			  SPL::float64 speakerIdConfidence = 0.0;
			  SPL::float64 fromTime = 0.0;

			  // In the lab tests, I noticed that the STT service sometimes returns
			  // extra speaker labels from the earlier utterances. We have to ignore them.
			  // So let us skip the extra ones until we match with the first entry in the
			  // timestamps list which got populated in the previous finalized 
			  // utterance JSON message that came just before the speaker_labels message.
			  if (firstSpeakerLabelTimeMatched == false) {
				  // Read the from field.
				  try {
					  fromTime = query(root,
						  "speaker_labels.[" + boost::to_string(idx1) + "].from").get_value<float>();
					  
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) <<
							  "-->X4 fromTime=" << fromTime << std::endl;
					  }
					  
				  } catch (std::exception const& e) {
					  if (sttJsonResponseDebugging == true) {
						  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) <<
							  "-->X5 JSON parsing error when reading the from field : " << e.what(), "STT_Result_Processing");
					  }

					  break;
				  }

				  // If the second speaker_labels message arrives, then we can
				  // ignore it as explained in the commentary above. Because,
				  // there will be no new speaker_labels data here except for the final field
				  // set to true for the very last word of the utterance.
				  if (idx1 == 0 && utteranceWordsSpeakers.size() > 0) {
					  // We already have populated the speaker id list during the first 
					  // speaker_labels message that came just before this one.
					  // This is clearly the second speaker_lables message.
					  secondSpeakerLabelsMessageReceived = true;
					  
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) <<
							  "-->X6 Received the second speaker_labels message." << std::endl;
					  }
					  
					  break;
				  }
				  
				  // Ensure that we start processing only from the startTime for the 
				  // very first word in the current utterance. If it has speaker label
				  // entries for words from previous utterances, skip them and 
				  // move to the next one until we find a match with the startTime of
				  // the first word belonging to the most recent finalized utterance.
				  if (fromTime == utteranceWordsStartTimes.at(0)) {
					  firstSpeakerLabelTimeMatched = true;
					  
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X7 Speaker labels 'from' time matched at idx1=" << idx1 << std::endl;
					  }
				  } else {
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X8 Speaker labels 'from' time mismatch at idx1=" << idx1 << std::endl;
					  }
					  
					  // Skip this one and continue the loop.
					  continue;
				  }
			  } // End of if (firstSpeakerLabelTimeMatched == false)
			  
			  // Read the speaker field.
			  try {
				  speakerId = query(root,
					  "speaker_labels.[" + boost::to_string(idx1) + "].speaker").get_value<int32_t>();
			  } catch (std::exception const& e) {
				  if (sttJsonResponseDebugging == true) {
					  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) <<
						  "-->X9 JSON parsing error when reading the speaker field : " << e.what(), "STT_Result_Processing");
				  }
				  
				  break;
			  }
			  
			  // Read the confidence field.
			  try {
				  speakerIdConfidence = query(root,
					  "speaker_labels.[" + boost::to_string(idx1) + "].confidence").get_value<float>();
			  } catch (std::exception const& e) {
				  if (sttJsonResponseDebugging == true) {
					  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) <<
						  "-->X10 JSON parsing error when reading the confidence field : " << e.what(), "STT_Result_Processing");
				  }
				  
				  break;
			  }

			  // Append them to the list.
			  utteranceWordsSpeakers.push_back(speakerId);
			  utteranceWordsSpeakersConfidences.push_back(speakerIdConfidence);
			  
			  if (sttJsonResponseDebugging == true) {
				  std::cout << "Operator " << operatorPhysicalName <<
					  "-->Channel " << boost::to_string(udpChannelNumber) << 
					  "-->X11 speaker_labels entry added to the list" << std::endl;
			  }
		  } // End of while loop to iterate through the speaker_labels array.

		  // If we have successfully parsed the previous STT JSON response and
		  // prepared an output tuple during the previous Websocket on_message event,
		  // it is time now to send that STT result tuple that is waiting to be sent.
		  // Do it only if the transcription is still in progress. If STT service 
		  // sent us a transcription completed signal via "listening" response message,
		  // then we will skip sending this tuple right here. Instead, we will send the
		  // final tuple for this audio in the very last if segment in this method after 
		  // setting the output tuple attribute (transcriptionCompleted) to true.
		  // If we just now received the second speaker_labels message in a row,
		  // that is redundant and we will not send the output tuple at that time.
		  if (sttResultTupleWaitingToBeSent == true &&
			  fullTranscriptionCompleted == false &&
			  secondSpeakerLabelsMessageReceived == false) {
			  sttResultTupleWaitingToBeSent = false;
			  // Send this tuple now.
			  // Dereference the oTuple object from the object pointer and send it.
			  if (sttJsonResponseDebugging == false) {
				  submit(*(oTupleList.at(0)), 0);
			  }
			  
			  if (sttJsonResponseDebugging == true) {
				  std::cout << "Operator " << operatorPhysicalName <<
					  "-->Channel " << boost::to_string(udpChannelNumber) << 
					  "-->X52a At the tuple submission point for reporting " <<
					  "interim transcription results." << std::endl;
			  }			  
			  // Since we are storing the speaker_labels results in lists that are 
			  // member variables of this class, let us clear them after we are
			  // done processing the most recently received JSON message from the STT service.
			  utteranceWordsSpeakers.clear();
			  utteranceWordsSpeakersConfidences.clear();
			  utteranceWordsStartTimes.clear();
		  }
		  
		  idx1 = -1;
 		  
		  try {
			  // Read the result_index field.
			  utteranceNumber = root.get<int32_t>("result_index");
		  } catch (std::exception const& e) {
			  if (sttJsonResponseDebugging == true) {
				  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
					  "-->Channel " << boost::to_string(udpChannelNumber) <<
					  "-->X12 JSON parsing error when reading the result_index field : " << e.what(), "STT_Result_Processing");
			  }
			  
			  // If speaker_labels is enabled, it is possible not to receive the 
			  // result_index field and the results array field in certain situations with 
			  // interim_results enabled for stt result mode 1 or 2. 
			  // Read the Watson STT service speaker labels documentation about it.
			  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#speaker_labels
			  // In this case, we will ignote this exception and continue to the next
			  // while loop which will also exit that loop due to missing "results" array field.
		  }

		  // If there are multiple elements in the "results" JSON array, iterate through all of them.
		  // If we encountered the speaker_labels message two in a row, ignore the
		  // second one and skip this entire loop.
		  while(secondSpeakerLabelsMessageReceived == false) {	  
			  ++idx1;;
			  bool tempFinal = false;
			  bool exitThisLoop = false;

			  // Read the finalized utterance field.
			  try {
				  tempFinal = query(root,
					  "results.[" + boost::to_string(idx1) + "].final").get_value<bool>();
			  } catch (std::exception const& e) {
				  if (sttJsonResponseDebugging == true) {
					  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) <<
						  "-->X13 JSON parsing error when reading the final field : " << e.what(), "STT_Result_Processing");
					  
					  if (idx1 == 0) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X14 idx1=0, size of speaker_labels list=" << 
							  utteranceWordsSpeakers.size() << std::endl;
					  }
				  }
				  
				  // It is either an invalid index or we received the 
				  // speaker_labels data in the previous while loop as a 
				  // standalone field without the result_index or 
				  // "results" array field. We must exit this while loop now.
				  // This means we processed all the elements in the "results" JSON array (OR)
				  // we couldn't process any element due to missing "results" array field.
				  exitThisLoop = true;
			  }

			  if ((idx1 == 0 && utteranceWordsSpeakers.size() > 0)) {
				  // We just now got a standalone speaker_labels message right after
				  // processing a finalized utterance (for stt result mode 1 and 2).
				  // We must set the final local variable to true now to meet the
				  // if conditional logic in the next code segment in order to
				  // prepare the output tuple to be sent out.
				  final = true;

				  if (sttJsonResponseDebugging == true) {
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X15 Forcing final = true after receiving the speaker_labels message." << std::endl;
				  }
			  }
			  
			  // At this time, we can prepare the output tuple for the 
			  // utterance we parsed in the previous iteration of this loop.
			  // Skip preparing an output tuple during the very first loop iteration.
			  // We are delaying the tuple sending by one iteration to set the 
			  // transcription completed field correctly at the very end of the
			  // STT processing of the current audio data. The very last tuple with
			  // the transcriptionCompleted attribute set to true will be sent below
			  // in the next section (if segment) of this method below.
			  //
			  // Auto assignment where needed was already done in the process method above.
			  // So, set only those attributes that have an explicit assignment via an output function.
			  // Do this only as needed depending on the user configured STT result mode.
			  //
			  // SPECIAL NOTE: We have to prepare an output tuple right after receiving
			  // the speaker_labels message right after a finalized utterance with idx1 = 0.
			  if ((idx1 == 0 && final == true) ||
				  ((idx1 > 0) && (sttResultMode == 1 ||
				  (sttResultMode == 2 && final == true) ||
				  (sttResultMode == 3 && exitThisLoop == true)))) {
				  // If the sttResultMode is 3, we must return only the full transcription text.
				  // Hence, reset the utterance related details.
				  if (sttResultMode == 3) {
					  // Compute the average confidence for the full transcription text.
					  confidence = 
						  cumulativeConfidenceForFullTranscription / (idx1);
					  
					  utteranceNumber = -1;
					  utteranceText = "";
					  final = false;
				  }
				  
				  // Assign the output attributes via the output functions as configured in the SPL code.
				  //
				  // 1) When the logic enters here right after receiving a speaker_labels message i.e.
				  // when idx1 == 0 && final == true, we must update ONLY the 
				  // speaker id related attributes. This will ensure that we will not
				  // overwrite the values of the non-speaker id related attributes whose values
				  // were properly set before the speaker_labels message arrived.
				  // 
				  // 2) At the other time i.e. when idx1 > 0, we must set all the attributes where 
				  // some will have non-empty results and some such as the speaker id 
				  // attributes will have empty results.
				  if (idx1 > 0) {
					  <% 
					  my $oport = $model->getOutputPortAt(0); 
					  foreach my $attribute (@{$oport->getAttributes()}) { 
						  my $name = $attribute->getName(); 
						  my $paramValues = $attribute->getAssignmentOutputFunctionParameterValues();
						  my $operation = $attribute->getAssignmentOutputFunctionName(); 
		
						  if ($operation eq "getUtteranceNumber") { 					  
					  %> 
						  // Utterance number starts from 0. Hence, add 1 to it.
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceNumber + 1));
						  <%} elsif ($operation eq "getUtteranceText") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceText));
						  <%} elsif ($operation eq "isFinalizedUtterance") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(final));
						  <%} elsif ($operation eq "getConfidence") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(confidence));
						  <%} elsif ($operation eq "getFullTranscriptionText") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(fullTranscriptionText));
						  <%} elsif ($operation eq "isTranscriptionCompleted") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(false));
						  <%} elsif ($operation eq "getUtteranceAlternatives") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceAlternatives));
						  <%} elsif ($operation eq "getWordAlternatives") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(wordAlternatives));
						  <%} elsif ($operation eq "getWordAlternativesConfidences") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(wordAlternativesConfidences));
						  <%} elsif ($operation eq "getWordAlternativesStartTimes") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(wordAlternativesStartTimes));
						  <%} elsif ($operation eq "getWordAlternativesEndTimes") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(wordAlternativesEndTimes));
						  <%} elsif ($operation eq "getUtteranceWords") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceWords));
						  <%} elsif ($operation eq "getUtteranceWordsConfidences") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceWordsConfidences));
						  <%} elsif ($operation eq "getUtteranceWordsStartTimes") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceWordsStartTimes));
						  <%} elsif ($operation eq "getUtteranceWordsEndTimes") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceWordsEndTimes));
						  <%} elsif ($operation eq "getUtteranceStartTime") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceStartTime));
						  <%} elsif ($operation eq "getUtteranceEndTime") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(utteranceEndTime));
						  <%} elsif ($operation eq "getUtteranceWordsSpeakers") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>());
					  	  <%} elsif ($operation eq "getUtteranceWordsSpeakersConfidences") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>());
						  <%} elsif ($operation eq "getKeywordsSpottingResults") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>(keywordsSpottingResults));
					  	  <%}
					  }%>
				  } else {
					  // When idx == 0, only these speaker id related 
					  // output functions must be called to update those two attributes.
					  // All other attributes shouldn't be touched since they were
					  // already set to proper values during the STT JSON messages that
					  // arrived before the speaker_labels message.
					  <% 
					  my $oport = $model->getOutputPortAt(0); 
					  foreach my $attribute (@{$oport->getAttributes()}) { 
						  my $name = $attribute->getName(); 
						  my $paramValues = $attribute->getAssignmentOutputFunctionParameterValues();
						  my $operation = $attribute->getAssignmentOutputFunctionName(); 

						  if ($operation eq "getUtteranceWordsSpeakers") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>());
						  <%} elsif ($operation eq "getUtteranceWordsSpeakersConfidences") { 
					  %> 
						  oTupleList.at(0)->set_<%=$name%>( 
								<%=$operation%>());
						  <%}
					  }%>										  
				  } // End of if (idx1 > 0) 

				  // Set this flag so that this tuple can be sent out during the
				  // next on_message Websocket event. In order to send the tuple,
				  // this flag is checked just outside of the while loop we are in now.
				  // We have to follow this approach so that we can correctly
				  // send the very last tuple for this audio with its 
				  // transcriptionCompleted attribute set to true.
				  // The following if conditional logic works as described below.
				  // 1) identifySpeakers == false will permit all the three STT result modes to
				  //    set the tupleWaiting flag to true.
				  //    speaker id feature is disabled.
				  // 2) final == false will take care of the non-finalized partial utterance when
				  //    stt result mode is 1. It will set the tupleWaiting flag to true in that case.
				  // 3) The other combined condition will take care of the finalized utterance when 
				  //    result mode is 1 or 2 with speaker id feature enabled.
				  if (identifySpeakers == false || final == false || 
					  (identifySpeakers == true && 
					  final == true && idx1 == 0)) { 
					  sttResultTupleWaitingToBeSent = true;
					  
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X16 Setting the tuple in waiting mode to be sent out." << std::endl;
					  }
				  }
			  }

			  if (exitThisLoop == true) {
				  if (sttJsonResponseDebugging == true) {
					  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) <<
						  "-->X17 Exiting the main loop after reaching array index " << idx1 << ".", "STT_Result_Processing");
				  }
				  
				  break;			  
			  }

			  std::string tempUtteranceText = "";			  
			  utteranceText = "";
			  final = tempFinal;
			  confidence = 0.0;
			  SPL::int32 idx2 = -1;
			  bool confidenceFound = false;

			  // If the user has configured that maxUtteranceAlternatives parameter in SPL with
			  // a value greater than 1, then STT service will return more than one
			  // item in the alternatives JSON array. We have to retrieve all of them to be
			  // returned to the user.
			  while(true) {
				  ++idx2;
				  // Read the utterance word confidences if present.
				  // Let us now iterate through the "word_confidence" array.
				  // This is an optional field within the "results.alternatives" JSON array.
				  // Refer to this URL for the correct JSON format:
				  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#word_confidence
				  idx3 = -1;
				  bool utteranceWordsPopulated = false;
				  
				  while(true) {
					  ++idx3;
					  // Read the results.alternatives.word_confidence[0] field i.e. word.
					  try {
						  std::string utteranceWord = query(root,
							  "results.[" + boost::to_string(idx1) + "].alternatives.[" +
							  boost::to_string(idx2) + "].word_confidence.[" +
							  boost::to_string(idx3) + "].[0]" ).get_value<std::string>();
						  
						  // Append this to the list.
						  utteranceWords.push_back(utteranceWord);
						  utteranceWordsPopulated = true;
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X18 utteranceWord=" << utteranceWord << std::endl;
						  }
					  } catch(std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X19 idx2=" << idx2 << ", idx3=" << idx3 <<
								  ". JSON parsing error when reading the word_confidence[0] word field: " << 
								  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }

					  // Read the results.alternatives.word_confidence[1] field i.e. confidence.
					  try {
						  SPL::float64 utteranceWordConfidence = query(root,
							  "results.[" + boost::to_string(idx1) + "].alternatives.[" +
							  boost::to_string(idx2) + "].word_confidence.[" +
							  boost::to_string(idx3) + "].[1]" ).get_value<float>();
						  
						  // Append this to the list.
						  utteranceWordsConfidences.push_back(utteranceWordConfidence);
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X20 utteranceWordConfidence=" << utteranceWordConfidence << std::endl;
						  }
					  } catch(std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
						  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) <<
							  "-->X21 idx2=" << idx2 << ", idx3=" << idx3 <<
							  ". JSON parsing error when reading the word_confidence[1] confidence field: " << 
							  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }
				  } // End of while for parsing "results.alternatives.word_confidence" JSON array.

				  if (sttJsonResponseDebugging == true) {
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X22 utteranceWords=" << boost::to_string(utteranceWords) << std::endl;
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X23 utteranceWordsConfidences=" << boost::to_string(utteranceWordsConfidences) << std::endl;
				  }

				  
				  // Read the utterance word timestamps if present.
				  // Let us now iterate through the "timestamps" array.
				  // This is an optional field within the "results.alternatives" JSON array.
				  // Refer to this URL for the correct JSON format:
				  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#word_timestamps
				  idx3 = -1;
				  
				  while(true) {
					  ++idx3;
					  // Read the results.alternatives.timestamps[0] field i.e. word.
					  // Minor optimization: Do this only if the individual words were not already 
					  // populated in the previous while loop for the word_confidence array.
					  if (utteranceWordsPopulated == false) {
						  try {
							  std::string utteranceWord = query(root,
								  "results.[" + boost::to_string(idx1) + "].alternatives.[" +
								  boost::to_string(idx2) + "].timestamps.[" +
								  boost::to_string(idx3) + "].[0]" ).get_value<std::string>();
							  
							  // Append this to the list.
							  utteranceWords.push_back(utteranceWord);

							  if (sttJsonResponseDebugging == true) {
								  std::cout << "Operator " << operatorPhysicalName <<
									  "-->Channel " << boost::to_string(udpChannelNumber) << 
									  "-->X24 utteranceWord=" << utteranceWord << std::endl;
							  }
						  } catch(std::exception const& e) {
							  if (sttJsonResponseDebugging == true) {
								  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
									  "-->Channel " << boost::to_string(udpChannelNumber) <<
									  "-->X25 idx2=" << idx2 << ", idx3=" << idx3 <<
									  ". JSON parsing error when reading the timestamps[0] word field: " << 
									  e.what(), "STT_Result_Processing");
							  }
							  
							  break;
						  }
					  } // End of if (utteranceWordsPopulated == false)

					  // Read the results.alternatives.timestamps[1] field i.e. startTime.
					  try {
						  SPL::float64 utteranceWordStartTime = query(root,
							  "results.[" + boost::to_string(idx1) + "].alternatives.[" +
							  boost::to_string(idx2) + "].timestamps.[" +
							  boost::to_string(idx3) + "].[1]" ).get_value<float>();
						  
						  // Append this to the list.
						  utteranceWordsStartTimes.push_back(utteranceWordStartTime);
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X26 utteranceWordStartTime=" << utteranceWordStartTime << std::endl;
						  }
					  } catch(std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X27 idx2=" << idx2 << ", idx3=" << idx3 <<
								  ". JSON parsing error when reading the timestamps[1] startTime field: " << 
								  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }
					  
					  // Read the results.alternatives.timestamps[2] field i.e. endTime.
					  try {
						  SPL::float64 utteranceWordEndTime = query(root,
							  "results.[" + boost::to_string(idx1) + "].alternatives.[" +
							  boost::to_string(idx2) + "].timestamps.[" +
							  boost::to_string(idx3) + "].[2]" ).get_value<float>();
						  
						  // Append this to the list.
						  utteranceWordsEndTimes.push_back(utteranceWordEndTime);

						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X28 utteranceWordEndTime=" << utteranceWordEndTime << std::endl;
						  }
					  } catch(std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
									  "-->Channel " << boost::to_string(udpChannelNumber) <<
									  "-->X29 idx2=" << idx2 << ", idx3=" << idx3 <<
									  ". JSON parsing error when reading the timestamps[2] endTime field: " << 
									  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }
				  } // End of while for parsing "results.alternatives.timestamps" JSON array.
				  
				  // Record the utterance start and end times now.
				  if (utteranceWordsStartTimes.size() > 0) {
					  utteranceStartTime = utteranceWordsStartTimes.at(0);
				  }
				  
				  if (utteranceWordsEndTimes.size() > 0) {
					  utteranceEndTime = utteranceWordsEndTimes.at(utteranceWordsEndTimes.size()-1);
				  }
				  
				  if (sttJsonResponseDebugging == true) {
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X30 utteranceWords=" << boost::to_string(utteranceWords) << std::endl;
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X31 utteranceWordsStartTimes=" << 
						  boost::to_string(utteranceWordsStartTimes) << std::endl;
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X32 utteranceWordsEndTimes=" << boost::to_string(utteranceWordsEndTimes) << std::endl;
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X33 utteranceStartTime=" << boost::to_string(utteranceStartTime) << std::endl;
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X34 utteranceEndTime=" << boost::to_string(utteranceEndTime) << std::endl;
				  }
				  
				  
				  // Read the utterance confidence value which will be
				  // available only for the finalized utterance.
				  if (final == true) {
					  try {
						  confidence = query(root,
							  "results.[" + boost::to_string(idx1) + "].alternatives.[" +
							  boost::to_string(idx2) + "].confidence").get_value<float>();
						  cumulativeConfidenceForFullTranscription += confidence;
						  confidenceFound = true;
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X35 confidence=" << confidence << std::endl;
						  }
					  } catch (std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X36 idx2=" << idx2 << 
								  ". JSON parsing error when reading the confidence field: " << e.what(), "STT_Result_Processing");
						  }
					  }
				  }

				  // Read either the partial or finalized utterance.
				  try {
					  tempUtteranceText = query(root,
						  "results.[" + boost::to_string(idx1) + "].alternatives.[" +
						  boost::to_string(idx2) + "].transcript").get_value<std::string>();

					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X37 idx2=" << idx2 << 
							  ". utteranceText=" << tempUtteranceText << std::endl;
					  }
					  
					  // If the STT result mode is full text, then keep accumulating it.
					  // Do it only if it is a finalized utterance. 
					  if (sttResultMode == 3 && 
						  final == true && confidenceFound == true) {
						  fullTranscriptionText += tempUtteranceText;
						  // Since it is a final utterance, add a period.
						  fullTranscriptionText += ". ";
					  }
					  
					  // In the alternatives JSON array, only one element will have the
					  // best result combined with confidence. We will store that as the
					  // finalized utterance in the n-best alternative hypothesis scenario.
					  if (sttResultMode == 2 && 
						  confidenceFound == true) {
						  utteranceText = tempUtteranceText;
					  }
					  
					  // In the case of partial utterance, we have to consider every
					  // utterance that is being sent irrespective of final or not.
					  if (sttResultMode == 1) {
						  utteranceText = tempUtteranceText;
					  }
					  					  
					  // If the STT result mode is 1 (partial utterance) or 2 (full utterance),
					  // we must collect the n-best utterance alternatives.
					  // For result mode 3 (full transcript), we don't support it.
					  if (sttResultMode != 3 && final == true) {
						  utteranceAlternatives.push_back(tempUtteranceText);
					  }
					  
					  confidenceFound = false;
				  } catch (std::exception const& e) {
					  if (sttJsonResponseDebugging == true) {
						  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) <<
							  "-->X38 JSON parsing error when reading the transcript field: " << e.what(), "STT_Result_Processing");
					  }
					  
					  // A special case for STT resule mode 1 (partial utterance) where
					  // we must set the utteranceText to the very first alternative
					  // in the list before we break from this inner loop. Because, that is the
					  // correct finalized utterance with the actual confidence field 
					  // set to a valid value.	
					  if (sttResultMode == 1 &&
						  final == true && utteranceAlternatives.size() > 0) {
						  utteranceText = utteranceAlternatives.at(0);
					  }
					  
					  // This means we have iterated through all the alternatives JSON array elements.
					  // We can leave the inner while loop now.
					  break;
				  }
			  } // End of inner while loop for iterating through the "alternatives" array elements.

			  if (sttJsonResponseDebugging == true) {
				  if (sttResultMode != 3) {
					  std::cout << "Operator " << operatorPhysicalName <<
						  "-->Channel " << boost::to_string(udpChannelNumber) << 
						  "-->X39 Utterance Alternatives=" << boost::to_string(utteranceAlternatives) << std::endl;
				  }
			  }
			  
			  // Let us now iterate through the "word_alternatives" array (Confusion Networks).
			  // This is an optional field within the "results" JSON array.
			  // Refer to this URL for the correct JSON format:
			  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#word_alternatives
			  idx2 = -1;
			  while(true) {
				  ++idx2;
				  SPL::float64 startTime = 0.0;
				  SPL::float64 endTime = 0.0;

				  // Read the results.word_alternatives.startTime field.
				  try {
					  startTime = query(root,
						  "results.[" + boost::to_string(idx1) + "].word_alternatives.[" +
						  boost::to_string(idx2) + "].start_time").get_value<float>();
					  
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X40 word_alternatives.startTime=" << startTime << std::endl;
					  }
				  } catch (std::exception const& e) {
					  if (sttJsonResponseDebugging == true) {
						  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) <<
							  "-->X41 idx2=" << idx2 << 
							  ". JSON parsing error when reading the word_alternatives.startTime field: " << 
							  e.what(), "STT_Result_Processing");
					  }
					  
					  break;
				  }
				  
				  // Read the results.word_alternatives.endTime field.
				  try {
					  endTime = query(root,
						  "results.[" + boost::to_string(idx1) + "].word_alternatives.[" +
						  boost::to_string(idx2) + "].end_time").get_value<float>();
					  
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X42 word_alternatives.endTime=" << endTime << std::endl;
					  }
				  } catch (std::exception const& e) {
					  if (sttJsonResponseDebugging == true) {
						  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X43 idx2=" << idx2 << 
								  ". JSON parsing error when reading the word_alternatives.endTime field: " << 
								  e.what(), "STT_Result_Processing");
					  }
					  
					  break;
				  }				  
				  
				  // Now iterate through the "alternatives" array which is a field
				  // within the "word_alternatives" array.
				  idx3 = -1;
				  SPL::float64 wordConfidence = 0.0;
				  std::string word = "";
				  bool wordAlternativeFound = false;
				  SPL::list<SPL::rstring> words;
				  SPL::list<SPL::float64> confidences;
				  
				  while(true) {
					  ++idx3;
					  wordConfidence = 0.0;
					  word = "";
					  
					  // Read the results.word_alternatives.alternatives.confidence field.
					  try {
						  wordConfidence = query(root,
							  "results.[" + boost::to_string(idx1) + "].word_alternatives.[" +
							  boost::to_string(idx2) + "].alternatives.[" +
							  boost::to_string(idx3) + "].confidence").get_value<float>();
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X44 word_alternatives.alternatives.confidence=" << wordConfidence << std::endl;
						  }
					  } catch (std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X45 idx2=" << idx2 << ", idx3=" << idx3 <<
								  ". JSON parsing error when reading the word_alternatives.alternatives.confidence field: " << 
								  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }
					  
					  // Read the results.word_alternatives.alternatives.confidence field.
					  try {
						  word = query(root,
							  "results.[" + boost::to_string(idx1) + "].word_alternatives.[" +
							  boost::to_string(idx2) + "].alternatives.[" +
							  boost::to_string(idx3) + "].word").get_value<std::string>();
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X46 word_alternatives.alternatives.word=" << word << std::endl;
						  }
					  } catch (std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X47 idx2=" << idx2 << ", idx3=" << idx3 <<
								  ". JSON parsing error when reading the word_alternatives.alternatives.word field: " << 
								  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }
					  
					  // We got an alternative word and its confidence.
					  wordAlternativeFound = true;
					  // Insert the word and its confidence into individual lists.
					  words.push_back(word);
					  confidences.push_back(wordConfidence);
				  } // End of inner while to read the results.word_alternatives.alternatives array.
				  
				  // If we have found at least one word alternative, 
				  // let us store it in the corresponding lists. 
				  if (wordAlternativeFound == true) {
					  wordAlternatives.push_back(words);
					  wordAlternativesConfidences.push_back(confidences);
					  wordAlternativesStartTimes.push_back(startTime);
					  wordAlternativesEndTimes.push_back(endTime);
				  }
			  } // End of outer while for reading the results.word_alternatives array.
			  
			  if (sttJsonResponseDebugging == true) {
				  std::cout << "Operator " << operatorPhysicalName <<
					  "-->Channel " << boost::to_string(udpChannelNumber) << 
					  "-->X48 wordAlternatives=" << boost::to_string(wordAlternatives) << std::endl;
				  std::cout << "Operator " << operatorPhysicalName <<
					  "-->Channel " << boost::to_string(udpChannelNumber) << 
					  "-->X49 wordAlternativesConfidences=" << boost::to_string(wordAlternativesConfidences) << std::endl;			  
				  std::cout << "Operator " << operatorPhysicalName <<
					  "-->Channel " << boost::to_string(udpChannelNumber) << 
					  "-->X50 wordAlternativesStartTimes=" << boost::to_string(wordAlternativesStartTimes) << std::endl;
				  std::cout << "Operator " << operatorPhysicalName <<
					  "-->Channel " << boost::to_string(udpChannelNumber) << 
					  "-->X51 wordAlternativesEndTimes=" << boost::to_string(wordAlternativesEndTimes) << std::endl;
			  }
			  
			  // Let us now iterate through the "keywords_result" associative array (Keyword Spotting).
			  // This is an optional field within the "results" JSON array.
			  // Refer to this URL for the correct JSON format:
			  // https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-output#keyword_spotting
			  idx2 = -1;
			  SPL::int32 sizeOfkeywordsToBeSpottedList = 
				  keywordsToBeSpotted.size();
			  
			  while(keywordsSpottingThreshold > 0.0) {
				  // If we have finished checking the results for all the keywords, we can exit from this loop.
				  if (++idx2 >= sizeOfkeywordsToBeSpottedList) {
					  if (sttJsonResponseDebugging == true) {
						  std::cout << "Operator " << operatorPhysicalName <<
							  "-->Channel " << boost::to_string(udpChannelNumber) << 
							  "-->X60 keywordsSpottingResults=" <<
							  boost::to_string(keywordsSpottingResults) << std::endl;
					  }
					  
					  break;
				  }
				  
				  std::string keyword = keywordsToBeSpotted.at(idx2);
				  bool keywordMatchFound = false;
				  
				  // For this keyword, there may be 0 or more keywordsSpotting matches.
				  // Get all the matching results for this keyword.
				  idx3 = -1;
				  while(true) {
					  ++idx3;
					  SPL::float64 matchStartTime = 0.0;
					  
					  try {
						  // Read the start_time field.
						  matchStartTime = query(root,
							  "results.[" + boost::to_string(idx1) + "].keywords_result." +
							  keyword + ".[" + boost::to_string(idx3) + "].start_time").get_value<float>();
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X54 keywords_result." <<
								  keyword << ".[" << idx3 << "].start_time=" << matchStartTime << std::endl;
						  }
					  } catch (std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X55 idx2=" << idx2 << ", idx3=" << idx3 <<
								  ". JSON parsing error when reading the " <<
								  "results.[" + boost::to_string(idx1) + "].keywords_result." <<
								  keyword << ".[" + boost::to_string(idx3) << "].start_time field. " <<
								  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }
					  
					  SPL::float64 matchEndTime = 0.0;
					  
					  try {
						  // Read the end_time field.
						  matchEndTime = query(root,
							  "results.[" + boost::to_string(idx1) + "].keywords_result." +
							  keyword + ".[" + boost::to_string(idx3) + "].end_time").get_value<float>();
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X56 keywords_result." <<
								  keyword << ".[" << idx3 << "].end_time=" << matchEndTime << std::endl;
						  }
					  } catch (std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X57 idx2=" << idx2 << ", idx3=" << idx3 <<
								  ". JSON parsing error when reading the " <<
								  "results.[" + boost::to_string(idx1) + "].keywords_result." <<
								  keyword << ".[" + boost::to_string(idx3) << "].end_time field. " <<
								  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }

					  SPL::float64 matchConfidence = 0.0;
					  
					  try {
						  // Read the confidence field.
						  matchConfidence = query(root,
							  "results.[" + boost::to_string(idx1) + "].keywords_result." +
							  keyword + ".[" + boost::to_string(idx3) + "].confidence").get_value<float>();
						  
						  if (sttJsonResponseDebugging == true) {
							  std::cout << "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) << 
								  "-->X58 keywords_result." <<
								  keyword << ".[" << idx3 << "].confidence=" << matchConfidence << std::endl;
						  }
					  } catch (std::exception const& e) {
						  if (sttJsonResponseDebugging == true) {
							  SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
								  "-->Channel " << boost::to_string(udpChannelNumber) <<
								  "-->X59 idx2=" << idx2 << ", idx3=" << idx3 <<
								  ". JSON parsing error when reading the " <<
								  "results.[" + boost::to_string(idx1) + "].keywords_result." <<
								  keyword << ".[" + boost::to_string(idx3) << "].confidence field. " <<
								  e.what(), "STT_Result_Processing");
						  }
						  
						  break;
					  }

					  // We got all the three values. Store them in a map.
					  keywordsSpottingResultsMap["start_time"] = matchStartTime;
					  keywordsSpottingResultsMap["end_time"] = matchEndTime;
					  keywordsSpottingResultsMap["confidence"] = matchConfidence;
					  // Add this map to the list.
					  keywordsSpottingResultsList.push_back(keywordsSpottingResultsMap);
					  // Since we added it to the list, clear the map now.
					  keywordsSpottingResultsMap.clear();	
					  keywordMatchFound = true;
				  } // End of while(true)

				  if (keywordMatchFound == true) {
					  // If we have found a result for a given keyword, add the
					  // list containing the results for that keyword into the final results map.
					  keywordsSpottingResults[keyword] = keywordsSpottingResultsList;
					  // Since we added the results list to the final results map, 
					  // we can now delete the list.
					  keywordsSpottingResultsList.clear();
				  }
			  } // End of while(keywordsSpottingThreshold > 0.0)
		  } // End of while looping through the "results" array elements.
	  } // End of the else segment in if (sttErrorString != "")
  } // End of if (transcriptionResultAvailableForParsing == true || sttErrorString != "")
    
  if (fullTranscriptionCompleted == true || sttErrorString != "") {
	  if (sttResultTupleWaitingToBeSent == true) {
		  // Send the final tuple with transcriptionCompleted field set to true.
		  // Set the STT error message attribute via the corresponding output function.
		  <% 
		  my $oport = $model->getOutputPortAt(0); 
		  foreach my $attribute (@{$oport->getAttributes()}) { 
			  my $name = $attribute->getName(); 
			  my $paramValues = $attribute->getAssignmentOutputFunctionParameterValues();
			  my $operation = $attribute->getAssignmentOutputFunctionName(); 
	
			  if ($operation eq "isTranscriptionCompleted") { 					  
		  %> 
			  oTupleList.at(0)->set_<%=$name%>( 
					<%=$operation%>(true)); 
			  <%} 
		  }%> 	  		  
	  }
	  
	  // Reset these member variables since we fully completed the 
	  // transcription or encountered an STT error.
	  transcriptionResult = "";
	  sttResultTupleWaitingToBeSent = false;
	  utteranceWordsSpeakers.clear();
	  utteranceWordsSpeakersConfidences.clear();
	  utteranceWordsStartTimes.clear();
	  
	  // If there is any STT error during Websocket connection establishment time,
	  // that could be mostly due to invalid recognition request start parameters 
	  // in the on_open method. That means our STT connection has not yet been 
	  // established. In that case, we should perform the entire logic in this 
	  // if segment only if our Websocket connection is currently established.
	  if (wsConnectionEstablished == true && oTupleList.size() > 0) {
		  // Send either the very last tuple with transcriptionCompleted set to true or
		  // with the STT error message set.
		  // Dereference the oTuple object from the object pointer and send it.
		  if (sttJsonResponseDebugging == false) {
			  submit(*(oTupleList.at(0)), 0);
		  }

		  numberOfFullAudioConversationsTranscribed++;
		  // Update the operator metric only if the user asked for a live update.
		  if (sttLiveMetricsUpdateNeeded == true) {
			  nFullAudioConversationsTranscribedMetric->setValueNoLock(numberOfFullAudioConversationsTranscribed);
		  }
			  
		  if (sttJsonResponseDebugging == true) {
			  std::string tempString = "transcription completion.";
			  
			  if (sttErrorString != "") {
				  tempString = "STT error.";
			  }
			  
			  std::cout << "Operator " << operatorPhysicalName <<
				  "-->Channel " << boost::to_string(udpChannelNumber) << 
				  "-->X52b At the tuple submission point for reporting " << 
				  tempString << " Total audio conversation received=" <<
				  numberOfFullAudioConversationsReceived <<
				  ", Total audio conversations transcribed=" <<
				  numberOfFullAudioConversationsTranscribed << std::endl;
		  }
		  
		  // Free the oTuple object since it is no longer needed.
		  delete oTupleList.at(0);
		  // Remove that vector element as well.
		  oTupleList.erase(oTupleList.begin() + 0);
		  
		  // Some important cleanup logic here that needs to be understood and
		  // validated for its correctness.
		  // If this operator is configured to receive and process audio blob fragments instead of
		  // reading and processing the entire audio content from an audio file and if we
		  // removed the oTuple object above due to an STT error and the audio blob sender
		  // thread above has not fully sent all the audio blob fragements for the
		  // audio transcription that we just stopped due to an STT error, it is important for
		  // us to clean up the remaining audio blob fragments from that audio converstion that are
		  // still waiting in the vector to be sent to the STT service.
		  if (sttErrorString != "" && audioInputAsBlob == true &&
			  statusOfAudioDataTransmissionToSTT == AUDIO_BLOB_FRAGMENTS_BEING_SENT_TO_STT) {
			  // We got an STT error in the middle of a transcription of the
			  // partial audio blob fragments. If all the blob fragments
			  // have not yet been sent to the STT service, we will clear the
			  // remaining audio blob fragments in the vector that are
			  // waiting to be sent to the STT service.
			  while(audioBytes.size() > 0) {
				  unsigned char * buffer = audioBytes.at(0);
				  // Remove the items from the vector. It is no longer needed. Also free the original 
				  // data pointer that we obtained from the blob in the process method.
				  audioBytes.erase(audioBytes.begin() + 0);
				  audioSize.erase(audioSize.begin() + 0);
				  
				  if (buffer != NULL) {
					  delete buffer;
				  } else {
					  // We removed all the remaininng audio blob fragments from the 
					  // audio conversation for which we got an STT error.
					  break;
				  }
			  } // End of while(audioBytes.size() > 0)
			  
			  // If there are no more audio fragments left in the vector, reset the fragment count to 0.
			  if(audioBytes.size() <= 0) {
				  numberOfAudioBlobFragmentsReceivedInCurrentConversation = 0;
			  }
		  } // End of if (sttErrorString != "" && 
	  } // End of if (oTupleList.size() > 0)
	  	  
	  if (wsConnectionEstablished == false && sttErrorString != "") {
		  websocketConnectionErrorOccurred = true;
		  // Always display this STT error message happening during the
		  // Websocket connection establishment phase.
		  std::cout << "Operator " << operatorPhysicalName <<
			  "-->Channel " << boost::to_string(udpChannelNumber) << 
			  "-->Error received from the Watson Speech To Text service: " <<
			  sttErrorString << std::endl;
	  }
	  
	  // Reset this flag to indicate that STT service has no full audio data at this time.
	  // i.e. no active transcription in progress now.
	  statusOfAudioDataTransmissionToSTT = NO_AUDIO_DATA_SENT_TO_STT;
  } // End of if (fullTranscriptionCompleted == true || sttErrorString != "")
} // End of the on_message method.

// Whenever our existing Websocket connection to the Watson STT service is closed,
// this callback method will be called from the websocketpp layer.
void MY_OPERATOR::on_close(MY_OPERATOR::client* c, websocketpp::connection_hdl hdl) {
	// In the lab tests, I noticed that occasionally a Websocket connection can get
	// closed right after an on_open event without actually receiving the "listening" response
	// in the on_message event from the Watson STT service. This condition clearly means 
	// that this is not a normal connection closure. Instead, the connection attempt has failed.  
	// We must flag this as a connection error so that a connection retry attempt 
	// can be triggered inside the ws_audio_blob_sender method.
	if (wsConnectionEstablished == false) {
		// This connection was not fully established before.
		// This closure happened during an ongoing connection attempt.
		// Let us flag this as a connection error.
		websocketConnectionErrorOccurred = true;
		SPLAPPTRC(L_ERROR,  "Operator " << operatorPhysicalName <<
			"-->Channel " << boost::to_string(udpChannelNumber) <<
			"-->Partially established Websocket connection closed with the Watson STT service during an ongoing connection attempt.",
			"on_close");
	} else {
		wsConnectionEstablished = false;
		// c->get_alog().write(websocketpp::log::alevel::app, "Websocket connection closed with the Watson STT service.");
		SPLAPPTRC(L_ERROR,  "Operator " << operatorPhysicalName <<
			"-->Channel " << boost::to_string(udpChannelNumber) <<
			"-->Fully established Websocket connection closed with the Watson STT service.", "on_close");
	}
}

// When a Websocket connection handshake happens with the Watson STT serice for enabling
// TLS security, this callback method will be called from the websocketpp layer.
MY_OPERATOR::context_ptr MY_OPERATOR::on_tls_init(MY_OPERATOR::client* c, websocketpp::connection_hdl) {
	//m_tls_init = std::chrono::high_resolution_clock::now();
	//context_ptr ctx = websocketpp::lib::make_shared<boost::asio::ssl::context>(boost::asio::ssl::context::tlsv1);
	context_ptr ctx = 
		websocketpp::lib::make_shared<boost::asio::ssl::context>(boost::asio::ssl::context::sslv23);

	try {
		ctx->set_options(boost::asio::ssl::context::default_workarounds |
			boost::asio::ssl::context::no_sslv2 |
			boost::asio::ssl::context::no_sslv3 |
			boost::asio::ssl::context::single_dh_use);
	} catch (std::exception& e) {
		std::cout << "Operator " << operatorPhysicalName <<
			"-->Channel " << boost::to_string(udpChannelNumber) << 
			"-->" << e.what() << std::endl;
	}
  
	return ctx;
}

// When a connection attempt to the Watson STT service fails, then this
// callback method will be called from the websocketpp layer.
void MY_OPERATOR::on_fail(MY_OPERATOR::client* c, websocketpp::connection_hdl hdl) {
	websocketConnectionErrorOccurred = true;	
	// c->get_alog().write(websocketpp::log::alevel::app, "Websocket connection to the Watson STT service failed.");
	SPLAPPTRC(L_ERROR, "Operator " << operatorPhysicalName <<
		"-->Channel " << boost::to_string(udpChannelNumber) <<
		"-->Websocket connection to the Watson STT service failed.", "on_fail");
}

// STTGateway Output Functions that are needed to set the output tuple attributes with their values.
SPL::int32 MY_OPERATOR::getUtteranceNumber(int32_t const & utteranceNumber) {
	return(utteranceNumber);
}

SPL::rstring MY_OPERATOR::getUtteranceText(std::string const & utteranceText){
	return(utteranceText);
}

SPL::boolean MY_OPERATOR::isFinalizedUtterance(bool const & finalizedUtterance) {
	return(finalizedUtterance);
}

SPL::float32 MY_OPERATOR::getConfidence(float const & confidence) {
	return(confidence);
}

SPL::rstring MY_OPERATOR::getFullTranscriptionText(std::string const & fullText) {
	return(fullText);
}

SPL::rstring MY_OPERATOR::getSTTErrorMessage(std::string const & errorMessage) {
	return(errorMessage);
}

SPL::boolean MY_OPERATOR::isTranscriptionCompleted(bool const & transcriptionCompleted) {
	return(transcriptionCompleted);
}

SPL::list<SPL::rstring> MY_OPERATOR::getUtteranceAlternatives(SPL::list<SPL::rstring> const & utteranceAlternatives) {
	return(utteranceAlternatives);
}

SPL::list<SPL::list<SPL::rstring>> MY_OPERATOR::getWordAlternatives(SPL::list<SPL::list<SPL::rstring>> const & wordAlternatives) {
	return(wordAlternatives);
}

SPL::list<SPL::list<SPL::float64>> MY_OPERATOR::getWordAlternativesConfidences(SPL::list<SPL::list<SPL::float64>> const & wordAlternativesConfidences) {
	return(wordAlternativesConfidences);
}

SPL::list<SPL::float64> MY_OPERATOR::getWordAlternativesStartTimes(SPL::list<SPL::float64> const & wordAlternativesStartTimes) {
	return(wordAlternativesStartTimes);
}

SPL::list<SPL::float64> MY_OPERATOR::getWordAlternativesEndTimes(SPL::list<SPL::float64> const & wordAlternativesEndTimes) {
	return(wordAlternativesEndTimes);
}

SPL::list<SPL::rstring> MY_OPERATOR::getUtteranceWords(SPL::list<SPL::rstring> const & utteranceWords) {
	return(utteranceWords);
}

SPL::list<SPL::float64> MY_OPERATOR::getUtteranceWordsConfidences(SPL::list<SPL::float64> const & utteranceWordsConfidences) {
	return(utteranceWordsConfidences);
}

SPL::list<SPL::float64> MY_OPERATOR::getUtteranceWordsStartTimes(SPL::list<SPL::float64> const & utteranceWordsStartTimes) {
	return(utteranceWordsStartTimes);
}

SPL::list<SPL::float64> MY_OPERATOR::getUtteranceWordsEndTimes(SPL::list<SPL::float64> const & utteranceWordsEndTimes) {
	return(utteranceWordsEndTimes);
}

SPL::float64 MY_OPERATOR::getUtteranceStartTime(SPL::float64 const & utteranceStartTime) {
	return(utteranceStartTime);
}

SPL::float64 MY_OPERATOR::getUtteranceEndTime(SPL::float64 const & utteranceEndTime) {
	return(utteranceEndTime);
}

SPL::list<SPL::int32> MY_OPERATOR::getUtteranceWordsSpeakers() {
	return(utteranceWordsSpeakers);
}

SPL::list<SPL::float64> MY_OPERATOR::getUtteranceWordsSpeakersConfidences() {
	return(utteranceWordsSpeakersConfidences);
}

SPL::map<SPL::rstring, SPL::list<SPL::map<SPL::rstring, SPL::float64>>> MY_OPERATOR::getKeywordsSpottingResults(
		SPL::map<SPL::rstring, SPL::list<SPL::map<SPL::rstring, SPL::float64>>> const & keywordsSpottingResults) {
	return(keywordsSpottingResults);
}

<%SPL::CodeGen::implementationEpilogue($model);%>
