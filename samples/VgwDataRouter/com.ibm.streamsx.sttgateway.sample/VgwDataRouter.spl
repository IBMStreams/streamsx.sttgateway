/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2021
==============================================
*/

/*
==============================================
First created on: Nov/24/2020
Last modified on: Feb/10/2021

A) What does this example application do?
   --------------------------------------
This example demonstrates the integration of the following three products to
achieve Real-Time Speech-To-Text transcription to get the text ready for
any further analytics. The task of receiving the voice calls' speech data 
from the Voice Gateway product and distributing it to one or more 
speech processor jobs is solely done by this VgwDataRouter application. 

1) IBM Voice Gateway v1.0.3.0 or higher
2) IBM Streams v4.2.1.6 or higher
3) IBM Watson Speech 2 Text (Embedded in an IBM Streams WatsonS2T operator v2.12.0) (OR)
   IBM Watson Speech To Text running on IBM public cloud or on IBM Cloud Pak for Data.

These three components will work in the following sequence:

Your Telephony SIPREC-->IBM Voice Gateway-->IBM Streams-->WatsonS2T Operator
(OR)
Your Telephony SIPREC-->IBM Voice Gateway-->IBM Streams-->WatsonSTT Operator

B) Other IBM Streams toolkits you will need for this application
   -------------------------------------------------------------
1) You must first ensure that your Streams machine where you will be compiling
this example application has these toolkits fully built and ready. 
This application has a dependency on these toolkits.
   a) streamsx.sttgateway toolkit (v2.2.3 or higher)
      https://github.com/IBMStreams/streamsx.sttgateway
   b) com.ibm.streamsx.websocket (v1.0.6 or higher)
      https://github.com/IBMStreams/streamsx.websocket
   c) STTGatewayUtils
      --> This is another SPL project available in the samples directory of
          the streamsx.sttgateway toolkit.
         
C) Building this example application
   ---------------------------------
You can build this example in a Linux terminal window via the make command by using the
Makefile available in the top-level directory of this example. Please follow
these steps to build this application:

1) streamsx.sttgateway toolkit also needs a specific version of the C++ boost library and 
the C++ websocketpp library. Please follow the instructions available in the documentation of 
that toolkit to download and prepare those libraries via the ant tool before you can 
compile this example application.

2) You have to either set the following environment variables on a terminal window where
you will run the make command or you can edit the Makefile and change these toolkit directories
to point to the correct location on your IBM Streams machine. 

   a) It will be necessary to export the STREAMS_STTGATEWAY_TOOLKIT (v2.2.5 or higher) environment variable by
      pointing it to the full path of your com.ibm.streamsx.sttgateway directory. 

   b) You have to export the STREAMS_WEBSOCKET_TOOLKIT (v1.0.6 or higher) and point to the correct directory.

   c) You have to export the STT_GATEWAY_UTILS and point it to the STTGatewayUtils directory available in
      the samples directory of the streamsx.sttgateway toolkit.

   d) If you don't want to set the above-mentioned environment variables every time you want to
      build this application, you can also edit the Makefile and enter these directories there directly.

3) If you prefer to build this example inside the Streams Studio instead of the command line based
Makefile, there are certain build configuration settings needed. Please refer to the streamsx.sttgateway
toolkit documentation to learn more about those Streams Studio configuration settings.

D) Running this example application
   --------------------------------
1) In order to run this example, you must first configure the IBM Voice Gateway v1.0.3.0 or higher 
to send the binary speech data to this IBM Streams application. That involves configuring the 
IBM Voice Gateway media relay and the IBM Voice Gateway SIP integrator components and then
deploying them. Please refer to the streamsx.sttgateway documentation section titled 
"Requirements for this toolkit".

2) Once you are sure that the IBM Voice Gateway can send the speech data to this
application, then you can give the following command to deploy this application in distributed mode.

There are certain importat submisstion time parameters that are required while
launching this application.

Mandatory submission time parameter(s) for the VgwDataRouter application:
Total number of speech processor jobs. e-g: -P totalNumberOfSpeechProcessorJobs=10
Number of Speech Engines in each speech processor job. e-g:  -P numberOfSpeechEnginesPerSpeechProcessorJob=30
TLS port number for VGW to send speech data to. e-g: -P tlsPortForVgwDataRx=8443
TLS port number for STT jobs to connect to for receiving speech data. e-g: -P tlsPortForVgwDataTx=8444

It would be better if we can make the IBMVoiceGatewaySource and the WebSocketSink (both WebSocket based)
server operators in this application to always land in the same machine in the IBM Streams cluster.
By doing that, we can make sure that the speech processor jobs can be scripted to point to
the well known host name where the WebSocketSink operator is running. This can be done via the
Job Configuration Overlay (JCO) file at the time of launching this application. In order to
achieve this function, you have to create a host tag VgwDataRouter on any one of your IBM Streams
cluster machines. Please look for more details in the etc/run-vgw-data-router.sh.

st submitjob -d  <YOUR_STREAMS_DOMAIN>  -i  <YOUR_STREAMS_INSTANCE>  -g etc/VgwDataRouter.jco.json -P tlsPortForVgwDataRx=8443 -P tlsPortForVgwDataTx=8444 -P numberOfSpeechEnginesPerSpeechProcessorJob=30 -P totalNumberOfSpeechProcessorJobs=10 -P vgwSessionLoggingNeeded=false -P ipv6Available=false -C fusionScheme=legacy com.ibm.streamsx.sttgateway.sample.VgwDataRouter.sab

3) After starting this application, now you must go ahead and launch either the VgwDataRouterToWatsonS2T or
the VgwDataRouterToWatsonSTT application one or more times to exactly match with the number that you
specified for the -P TotalNumberOfSpeechProcessorJobs submission time parameter in the above-mentioned command.
Please read the detailed commentary available at the top of those applications to learn more about
how to launch those applications correctly.

E) Running this example without IBM Voice Gateway
   ----------------------------------------------
You can use the VoiceDataSimulator test application from the samples directory to emulate Voice Gateway.
In this case the VoiceDataSimulator must read audio files with 8-bit pcm u-law samples.
You can convert the samples from the audio-files directory using the following command:

sndfile-convert -ulaw 01-call-center-10sec.wav 01-call-center-10sec.gsm

Convert all files using this Linux shell command:
for x in *.wav; do y="${x%.wav}"; echo "$y"; sndfile-convert -ulaw "$x" "${y}.gsm"; done

The command sndfile-convert is part of the package libsndfile-utils.
http://www.mega-nerd.com/libsndfile/

Play the gsm-file with this command:
play -t raw -r 8k -e mu-law -b 8 -c 1 JFKulaw.gsm
==============================================
*/
namespace com.ibm.streamsx.sttgateway.sample;

// This namespace contains the IBMVoiceGatewaySource operator that we use below.
use com.ibm.streamsx.sttgateway.watson::*;
// A few  C++ native functions are used from this namespace.
use com.ibm.streamsx.sttgateway.utils::*;
use spl.file::*;

// WebSocketSink comes from this namespace..
use com.ibm.streamsx.websocket.op::*;

// com.ibm.streamsx.json toolkit shipped with your IBM Streams installation is sufficient.
use com.ibm.streamsx.json::*;

// The following block of commentary is needed for the spldoc creation during the ant build process.
/**
 * 
 * What does this example application do?
 * 
 *  * This example demonstrates the integration of the following three products to
 * achieve Real-Time Speech-To-Text transcription to get the text ready for
 * any further analytics.
 * 
 * 1) IBM Voice Gateway v1.0.3.0 or higher
 * 2) IBM Streams v4.2.1.6 or higher
 * 3) IBM Watson Speech 2 Text (Embedded in an IBM Streams WatsonS2T operator v2.12.0) (OR)
 *    IBM Watson Speech To Text running on IBM public cloud or on IBM Cloud Pak for Data.
 * 
 * @param	tlsPortForVgwDataRx	TLS port on which this application will listen for receiving speech data from the IBM Voice Gateway.
 * 
 * @param	tlsPortForVgwDataTx	TLS port on which this application will accept connections from the specch processor jobs to distribute the sppech data.
 * 
 * @param	totalNumberOfSpeechProcessorJobs	Specify the number of speech processor jobs to which this application must route/distribute the speech data to.
 *
 * @param	numberOfSpeechEnginesPerSpeechProcessorJob	Specify the number of speech engines configured to run in each speech processor job.
 
 * @param	nonTlsEndpointNeeded	User can optionally specify whether they want a non-TLS endpoint.
 * 
 * @param	nonTlsPort	Non-TLS (Plain) port on which this application will (optionally) listen for communicating with the IBM Voice Gateway.
 * 
 * @param	certificateFileName		Server side certificate (.pem) file for the WebSocket server. 
 * It is necessary for the users to create a Root CA signed 
 * server side certificate file and point to that file at the time of 
 * starting this application. If the user doesn't point to this file 
 * at the time of starting the application, then the application will 
 * look for a default file named ws-server.pem inside the etc sub-directory 
 * of the application. This certificate will be presented to the 
 * IBM Voice Gateway for validation when it establishes a WebSocket 
 * connection with this application. For doing quick tests, you may save 
 * time and effort needed in getting a proper Root CA signed certificate 
 * by going with a simpler option of creating your own self-signed 
 * certificate. Please ensure that using a self-signed certificate is 
 * allowed in your environment. We have provided a set of instructions to 
 * create a self signed certificate. Please refer to the following 
 * file in the etc sub-directory of this application: 
 * etc/creating-a-self-signed-certificate.txt
 * 
 * @param   certificatePassword This parameter specifies a password needed for decrypting the WebSocket server's private key in the PEM file. Default is an empty string.
 *
 * @param	vgwLiveMetricsUpdateNeeded	Is live metrics needed for the IBMVoiceGatewaySource operator?
 * 
 * @param	vgwWebsocketLoggingNeeded	Is WebSocket library low level logging needed?
 * 
 * @param	vgwSessionLoggingNeeded	Is IBM Voice Gateway message exchange logging needed for debugging?
 * 
 * @param	initDelayBeforeSendingDataToSpeechProcessors	Time in seconds to wait before sending data to the speech processors.
 * 
 * @param	vgwStaleSessionPurgeInterval	Time interval in seconds during which the VGW source operator below should 
 * do memory cleanup of any Voice Gateway sessions that end abruptly in the 
 * middle of a voice call.
 * 
 * @param	ipv6Available	Is ipv6 protocol stack available in the Streams machine where the 
 * IBMVoiceGatewaySource operator is going to run? 
 * Most of the Linux machines will have ipv6. In that case, 
 * you can keep the following line as it is. 
 * If you don't have ipv6 in your environment, you can set the 
 * following submission time value to false. 
 * 
*/
// This is the main composite for this application.
public composite VgwDataRouter {
	param
		// IBM Voice Gateway related submission time values are defined below.
		// TLS port on which this application will listen for
		// communicating with the IBM Voice Gateway for receiving speech data.
		expression<uint32> $tlsPortForVgwDataRx : 
			(uint32)getSubmissionTimeValue("tlsPortForVgwDataRx", "443");
		// TLS port on which this application will listen to accept connections from
		// the speech processor job in order to route/distribute/send speech data to them.
		expression<uint32> $tlsPortForVgwDataTx : 
			(uint32)getSubmissionTimeValue("tlsPortForVgwDataTx", "444");		
		// User can optionally specify whether they want a non-TLS endpoint.
		expression<boolean> $nonTlsEndpointNeeded : 
			(boolean)getSubmissionTimeValue("nonTlsEndpointNeeded", "false");
		// Non-TLS (Plain) port on which this application will
		// (optionally) listen for communicating with the IBM Voice Gateway.
		expression<uint32> $nonTlsPort : 
			(uint32)getSubmissionTimeValue("nonTlsPort", "80");
		// Non-TLS (Plain) port on which this application will
		// (optionally) listen for communicating with the IBM Voice Gateway.
		expression<uint32> $nonTlsPortForWebSocketSink : 
			(uint32)getSubmissionTimeValue("nonTlsPortForWebSocketSink", "8081");
		// Server side certificate (.pem) file for the WebSocket server.
		// It is necessary for the users to create a Root CA signed 
		// server side certificate file and point to that file at the time of
		// starting this application. If the user doesn't point to this file
		// at the time of starting the application, then the application will
		// look for a default file named ws-server.pem inside the etc sub-directory
		// of the application. This certificate will be presented to the
		// IBM Voice Gateway for validation when it establishes a WebSocket 
		// connection with this application. For doing quick tests, you may save
		// time and effort needed in getting a proper Root CA signed certificate 
		// by going with a simpler option of creating your own self-signed 
		// certificate. Please ensure that using a self-signed certificate is 
		// allowed in your environment. We have provided a set of instructions to
		// create a self signed certificate. Please refer to the following
		// file in the etc sub-directory of this application:
		// etc/creating-a-self-signed-certificate.txt
		expression<rstring> $certificateFileName :
			getSubmissionTimeValue("certificateFileName", "");
		// Is a password needed for the private key in the certificate file?
		expression<rstring> $certificatePassword : 
			getSubmissionTimeValue("certificatePassword", "");
		// Do you want to specifiy a file name that contains the public certificates of
		// the trusted client(s). If this file name is not empty, then the
		// WebSocketSink operator will perform a client (mutual) authentication.
		expression<rstring> $trustedClientCertificateFileName :
			getSubmissionTimeValue("trustedClientCertificateFileName", "");	
		// Do you want to specify a list of identifiers present in the 
		// trusted client's X509 certificate's subject line. If that certificate is
		// self signed, then it will help during the client (mutual) authentication to approve
		// that client's identity as a known one.
		// 
		// Following are some examples of the subject line as it appears in an X509 public certificate.
		// /C=US/ST=NY/L=Yorktown Heights/O=IBM/OU=AI/CN=websocket.streams/emailAddress=websocket.streams@ibm.com
		// /C=US/O=Let's Encrypt/CN=Let's Encrypt Authority X3
		// /C=BE/O=GlobalSign nv-sa/CN=GlobalSign CloudSSL CA - SHA256 - G3
		// /C=US/O=Google Trust Services/CN=GTS CA 1O1
		// /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA
		// /C=US/ST=New York/L=Armonk/O=IBM/CN=www.ibm.com
		//
		// So your value for this submission time parameter can be as shown here.
		// ['emailAddress=websocket.streams@ibm.com', 'CN=www.ibm.com']
		expression<list<rstring>> $trustedClientX509SubjectIdentifiers :
			(list<rstring>)getSubmissionTimeValue("trustedClientX509SubjectIdentifiers", "[]");	
		// Do you want to use a specific URL context path for the WebSocketSink operator?
        // It can either be a single or a multi-part path.
        // e-g: Orders (OR) MyServices/Banking/Deposit
        // With that example, WebSocket server URL should either be 
        // https://host:port/Orders   (OR)
        // https://host:port/MyServices/Banking/Deposit
        // Default is an empty list to indicate no url context path.
        // You can expose any number of context paths for the 
        // remote clients to access this WebSocket server endpoint. 
        // e-g: []    (OR)    ['Orders', '', 'MyServices/Banking/Deposit']
        //
        // We will allow different speech processor jobs with their own
        // unique ids to connect to this application for getting the
        // speech data to get routed for processing. Such speech processor jobs will
        // use their respectivce unique speech processor id at the end end of the
        // URL as context path. Hence, we will prepare the WebSocketSink operator
        // invoked in this application to allow a set of context paths.
        // e-g: wss://MyHostName:8444/14  This URL must be used by a 
        // speech processor whose unique id  is 14.
		expression<list<rstring>> $webSocketSinkurlContextPath : (list<rstring>)
			getSubmissionTimeValue("webSocketSinkurlContextPath", 
			"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', " +
			"'11', '12', '13', '14', '15', '16', '17', '18', '19', '20']");
		// Is live metrics needed for the WebSocketSink operator?
		expression<boolean> $websocketLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("websocketLiveMetricsUpdateNeeded", "true");
		// Is WebSocket library low level logging needed?
		expression<boolean> $websocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("websocketLoggingNeeded", "false");
		// Is WebSocket client connection logging needed?
		expression<boolean> $wsConnectionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("wsConnectionLoggingNeeded", "false");
		// Is client message exchange logging needed for debugging?
		expression<boolean> $wsClientSessionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("wsClientSessionLoggingNeeded", "false");
		// NOTE: In most application scenarios, this feature will not be used at all.
		expression<uint32> $websocketStaleConnectionPurgeInterval :(uint32)
			getSubmissionTimeValue("websocketStaleConnectionPurgeInterval", "0");
		// Whitelist to accept connections only from specific
		// IP addresses belonging to the remote WebSocket clients.
		// Default is an empty list to indicate all client connections
		// are accepted without any restrictions. If there is a need to
		// accept connections only from certain clients, then a list
		// as shown below can be used  by including wild cards as needed.
		// e-g: "['172.34.18.212', '10.5.23.17', '172.*.42.*', '10.29.42.*']" 
		expression<list<rstring>> $clientWhitelist : (list<rstring>)
			getSubmissionTimeValue("clientWhitelist", "[]");
		// Specify a maximum number of concurrent client connections to be
		// allowed by the WebSocket server available inside the WebSocketSink operator.
		expression<uint32> $maxClientConnectionsAllowed : (uint32)
			getSubmissionTimeValue('maxClientConnectionsAllowed', "64");	
		// Is live metrics needed for the IBMVoiceGatewaySource operator?
		expression<boolean> $vgwLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("vgwLiveMetricsUpdateNeeded", "true");
		// Is WebSocket library low level logging needed?
		expression<boolean> $vgwWebsocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("vgwWebsocketLoggingNeeded", "false");
		// Is IBM Voice Gateway message exchange logging needed for debugging?
		expression<boolean> $vgwSessionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("vgwSessionLoggingNeeded", "false");
		// Under some cicumstances, if the IBMVoiceGatewaySource operator sends 
		// only one EOCS (End Of Call Sigmal) tuple instead of two as required for
		// the two voice channels, that may eventually cause the application logic
		// below not be able to release the speech processor jobs properly at the end of
		// a voice call for a given VGW session id. We have seen it in certain
		// customer environments. To avoid that condition, such customers can
		// configure this application to treat the very first EOCS tuple as 
		// sufficient to treat a voice call as a "completed call". In that case,
		// it will simply ignore if and when a second EOCS tuple arrives.
		// This feature can be activated to compensate for the situation described
		// above if it happens in some customer environments.
		// (Senthil added this on Feb/01/2021).
		expression<int32> $numberOfEocsNeededForVoiceCallCompletion : 
			(int32)getSubmissionTimeValue("numberOfEocsNeededForVoiceCallCompletion", "2");
		//
		// IBM Watson STT related submission time values are defined below.
		expression<int32> $totalNumberOfSpeechProcessorJobs :(int32)
			getSubmissionTimeValue("totalNumberOfSpeechProcessorJobs", "10") ;		
		expression<int32> $numberOfSpeechEnginesPerSpeechProcessorJob :(int32)
			getSubmissionTimeValue("numberOfSpeechEnginesPerSpeechProcessorJob", "10") ;
		// Time in seconds to wait before sending data to the speech processor.
		expression<float64> $initDelayBeforeSendingDataToSpeechProcessors :
			(float64)getSubmissionTimeValue("initDelayBeforeSendingDataToSpeechProcessors", "15.0"); 
		// Time interval in seconds during which the VGW source operator below should
		// do memory cleanup of any Voice Gateway sessions that end abruptly in the
		// middle of a voice call.
		expression<uint32> $vgwStaleSessionPurgeInterval : (uint32)
			getSubmissionTimeValue("vgwStaleSessionPurgeInterval", "10800");
		// Is ipv6 protocol stack available in the Streams machine where the
		// IBMVoiceGatewaySource operator is going to run?
		// Most of the Linux machines will have ipv6. In that case,
		// you can keep the following line as it is.
		// If you don't have ipv6 in your environment, you can set the
		// following submission time value to false.
		expression<boolean> $ipv6Available : (boolean)
			getSubmissionTimeValue("ipv6Available", "true");
		
		// Replaying the pre-recorded voice calls (A differentiating feature that we offer for free).
		// User can specify a directory name in which they can keep
		// copying pre-recorded speech files, call metadata files along with a signal file
		// to initiate transcription by using the speech data stored in them.
		// User must first ensure that the pre-recorded raw speech files, call metadata files 
		// for both the voice channels of a given call exist in this directory.
 		// After ensuring that those files for both the voice channels of a 
 		// given call exist, user can execute the following Linux command inside that directory.
		// touch  <vgwSessionId>.process-mulaw
		// e-g: touch  73269584.process-mulaw
		// This command will initiate the replay of the audio data as it   
		// gets read from the pre-recorded voice calls.
		expression<rstring> $callRecordingReadDirectory : 
			getSubmissionTimeValue("callRecordingReadDirectory", "/dev");

		// Users can configure a required number of call replayers. 
		// A given replayer will represent a single voice call by playing
		// speech data for the two voice channels (agent and customer).
		// It is important to ensure that there are equal number or 
		// more speech to text engines available to absorb the replay load 
		// from the configured number of call replayers. A single replayer
		// will need two speech to text engines for both the voice channels.
		// You must also plan to have additional number of speech to text engines to
		// satisfy your need to support the planned number of concurrent
		// real-time voice calls.
		expression<int32> $numberOfCallReplayEngines :(int32)
			getSubmissionTimeValue("numberOfCallReplayEngines", "1") ;
			
	graph
		// Ingest the speech data coming from the IBM Voice Gateway.
		// Such speech data arrives here in multiple fragments directly from
		// a live voice call. This operator is capable of receiving speech data
		// from multiple calls that can all happen at the very same time between
		// different pairs of speakers.
		// It is very important to note that the IBM Voice Gateway will keep
		// sending the speech data of the caller and the agent on two 
		// voice channels i.e. one for the caller and the other for the agent.
		// Irrespective of those two speakers talk or remain silent during the
		// call, their assigned voice channel will always carry some binary
		// data. That means, there is no way to know who is currently
		// talking. This constraint limits us from sending only one of the
		// channel's data to a single speech engine at any given time.
		// Instead, this constraint forces us to dedicate a single speech engine
		// per voice channel in a given voice call and keep sending the
		// data being received on that channel continuously to that
		// dedicated speech engine irrespective of whether that channel carries
		// silence or active speech data. In summary, we will need two
		// speech engines to do the Speech To Text for every ongoing voice call.
		// So, you have to plan ahead of time about the number of speech engines
		// you will start for handling the maximum number of concurrent calls.
		// As an example, for handling a maximum of 100 concurrent voice calls,
		// you will have to start 200 speech engines.
		//
		// In your own real-life applications, you may want to simply 
		// copy and reuse the code from this example and then make the 
		// changes only where it is really needed.
		// This example presents the following application design pattern:
		// IBMVoiceGatewaySource-->Speech Data Router-->Speech Processor->Speech Engine--> Speech Result Processor
		// You should be fine to simply use the entire pattern as it is except for
		// making changes in the Speech Result Processor composite and beyond to address
		// your own needs of further analytics on the Speech results as well as
		// specific ways of delivering the Speech results to other 
		// downstream systems rather than only writing to files as this example does below.
		(stream<BinarySpeech_t> BinarySpeechData as BSD) as VgwDataRouterSource = 
			IBMVoiceGatewaySource() {
			logic
				state: {
					// Initialize the default TLS certificate file name if the 
					// user didn't provide his or her own.
					rstring _certificateFileName = 
						($certificateFileName != "") ?
						$certificateFileName : getThisToolkitDir() + "/etc/ws-server.pem";
				}
				
			param
				tlsPort: $tlsPortForVgwDataRx;
				certificateFileName: _certificateFileName;
				nonTlsEndpointNeeded: $nonTlsEndpointNeeded;
				nonTlsPort: $nonTlsPort;
				// Initial delay before generating the very first tuple.
				// This is a one time delay when this operator starts up.
				// This delay should give sufficient time for the
				// speech processor jpbs to come up and be ready to
				// receive the speech data tuples sent by this operator.
				initDelay: $initDelayBeforeSendingDataToSpeechProcessors;
				vgwLiveMetricsUpdateNeeded: $vgwLiveMetricsUpdateNeeded;
				websocketLoggingNeeded: $vgwWebsocketLoggingNeeded;
				vgwSessionLoggingNeeded: $vgwSessionLoggingNeeded;
				vgwStaleSessionPurgeInterval: $vgwStaleSessionPurgeInterval;
				ipv6Available: $ipv6Available;
			
			// Get these values via custom output functions	provided by this operator.
			output
				BSD: vgwSessionId = getIBMVoiceGatewaySessionId(),
				    callStartDateTime = getCallStartDateTime(), 
					isCustomerSpeechData = isCustomerSpeechData(),
					vgwVoiceChannelNumber = getVoiceChannelNumber(),
					callerPhoneNumber = getCallerPhoneNumber(),
					agentPhoneNumber = getAgentPhoneNumber(),
					speechDataFragmentCnt = getTupleCnt(),
					totalSpeechDataBytesReceived = getTotalSpeechDataBytesReceived();

			config
				// Always place it on a machine that carries the VgwDataRouter host tag.
				placement: host(VgwPool[0]);
		}

		// Scan the call recording read directory for a 
		// replay signal file for every pre-recorded call.
		(stream<rstring fileName> CallReplaySignalFileName) as 
		 ReplaySignalFileNameReader = DirectoryScan() {
			param
				directory: $callRecordingReadDirectory;
				ignoreDotFiles: true;
				pattern: ".*\\.process-mulaw";
				sleepTime: 20.0;
		}		

		// The following code block invokes a composite operator to do the
		// replay of the pre-recorded calls when signaled by the user.
		// Please read the commentary about this voice call replay feature in the
		// code at the bottom of this file where this operator logic is implemented.
		//
		// Users can configure a required number of call replayers. 
		// A given replayer will represent a single voice call by playing
		// speech data for the two voice channels (agent and customer).
		// It is important to ensure that there are equal number or 
		// more speech to text engines available to absorb the replay load 
		// from the configured number of call replayers. A single replayer
		// will need two speech to text engines for both the voice channels.
		// You must also plan to have additional number of speech to text engines to
		// satisfy your need to support the planned number of concurrent
		// real-time voice calls.
		//
		@parallel(width=$numberOfCallReplayEngines)
		(stream<BinarySpeech_t> PreRecordedBinarySpeechData) as 
		 VoiceCallReplayer = CallRecordingReplay(CallReplaySignalFileName) {
		 	param
		 		callRecordingReadDirectory: $callRecordingReadDirectory;
		 		// Pass these stream types as composite operator parameters.
		 		callMetaData_t: CallMetaData_t;
		 		binarySpeech_t: BinarySpeech_t;
		 }

		// We have to always route the speech data bytes (fragments) coming from  
		// a given vgwSessionId_vgwVoiceChannelNumber to a particular 
		// speech processor job. 
		// We already explained in detail in the previous operator's
		// commentary section about why it must be done this way.
		// This idea of pairing up a vgwSessionId_vgwVoiceChannelNumber combo
		// to a particular parallel region channel is a must for the 
		// speech data bytes of a given speaker in a voice call to always land in 
		// the same speech processor job. This stickiness (a.k.a channel affinity) is
		// important to continuously transcribe the speech data arriving on both the
		// voice channels at all the time including the silence time of a speaker.
		// This is needed because the IBM Voice Gateway keeps sending the 
		// speech data bytes of both the speakers (whether active or silent) at 
		// all the time on two voice channels by dedicating one channel to an
		// agent and the other channel to the caller. So, this requires 
		// extra logic to locate an unused speech processor 
		// i.e. an idle speech engine in a speech processor job to be assigned for a 
		// given vgwSessionId_vgwVoiceChannelNumber.
		// That special speech data distribution logic happens inside this operator.
		(stream<WebSocketSinkSendData_t> CallDataForSpeechProcessor as CDFSP) as
			VoiceCallDataRouter = Custom(
			BinarySpeechData, PreRecordedBinarySpeechData as BSD) {
			logic
				state: {
					// This variable tells us how many total concurrent calls can be
					// handled by the way in which the user has deployed the application.
					// Every voice call has two voice channels (agent and customer).
					// So, any given call will require two dedicated speech engines.
					int32 _numberOfConcurrentCallsAllowedPerSpeechProcessor = 
						getNumberOfConcurrentCallsAllowedPerSpeechProcessor(
						$numberOfSpeechEnginesPerSpeechProcessorJob);
					// This list tells us how many voice calls are being processed at any 
					// given time by all the given speech processors that are configured to run.
					mutable list<int32> _speechProcessorStatusList = 
						prepareIdleSpeechProcessorsList($totalNumberOfSpeechProcessorJobs);
					// This map tells us which speech processor is processing a given vgwSessionId.
					// Key=vgwSessionId, Value=Speech Processor Id.
					mutable map<rstring, int32> _vgwSessionIdToSpeechProcessorMap = {};
					// This map tells us which speech processor can be released after completing
					// the speech to text work for a given vgwSessionId_vgwVoiceChannelNumber.
					// After getting released, such speech processors will become available for 
					// doing speech to text work for any new voice calls.
					mutable map<rstring, boolean> _vgwSessionVgwVoiceChannelNumberCompletedMap = {};
					mutable DataFromVgwRouter_t _serializedTuple = {};
					mutable WebSocketSinkSendData_t _oTuple = {};
					mutable rstring _key = "";
				}
			
				// Process the Binary Speech Data.
				onTuple BSD: {
					// Get the sessionId + channelNumber combo string.
					_key = BSD.vgwSessionId + "_" + (rstring)BSD.vgwVoiceChannelNumber;

					// We will get the regular binary speech data and the End Of Call Signal (EOCS) in
					// the same input stream. This design change was done on Feb/09/2021 to avoid any
					// any port locks and/or tuple ordering issues that may happen if we choose to 
        			// do it using two different output ports. The incoming tuple has an attribute
        			// that is set to true or false by the IBMVoiceGatewaySource operator to indicate
        			// whether it is sending binary speech data or an EOCS.
        			if(BSD.endOfCallSignal == false) {
        				// The incoming tuple contains binary speech data.
        				//
						// We have to first check if this speech data belongs to a 
						// brand new voice call or an already ongoing voice call.
						if(has(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId) == false) {
							// This is a brand new voice call. Get a speech processor id to
							// send the speech data belonging to this call.
							// Store this VGW session id to change the status of this 
							// voice call from "brand new" to "ongoing".
							int32 speechProcessorId = 
								getSpeechProcessorIdForNewCallProcessing(
								_numberOfConcurrentCallsAllowedPerSpeechProcessor,
								_speechProcessorStatusList);
							
							if(speechProcessorId == -1) {
								// This condition should not happen as long as there are enough
								// number of speech processors with more than sufficient number of
								// speech engines configured to run in them.
								appTrc(Trace.error, 
									"_XXXXX No speech processor job is available at this time for the " +
									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
									" We are not going to process the currently received speech data bytes" +
									" of this speaker in this voice call." +
									" Please start sufficient number of speech processor jobs " +
									" next time to handle your maximum expected concurrent calls." +
									" A rule of thumb is to have two S2T engines to process" +
									" two speakers in every given concurrent voice call.");
								return;
							}
							
							// Let us store the speech processor id of this call for
							// future use as the speech data keeps coming.
							insertM(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId, speechProcessorId);
							appTrc(Trace.error, "A new call with vgwSessionId=" + BSD.vgwSessionId +
								" is being assigned to speech processor id " + (rstring)speechProcessorId);						
						}
						
						// We can prepare this speech data to be sent to the chosen speech processor id.
						// We will do double serialization of the speech data as shown below.
						//
						// 1) Let us serialize the received speech data tuple.
						// msgType = 1 means that it carries the binary speech data tuple.
						_serializedTuple.msgType = 1;
						clearM(_serializedTuple.payload);
						// Call a native function to do the Tuple-->Blob conversion.
						serializeTuple(BSD, _serializedTuple.payload);
						
						// 2) Let us now serialize it one more time to be sent as the final tuple.
						_oTuple.strData = "";
						// We can tell the downstream WebSocketSink operator about to which
						// speech processor id our serialized data should be sent.
						// That is done by specifying the URL context path of the WebSocketSink
						// operator where a selected speech processor has connected to.
						// As shown in the param section of the downstream sink operator,
						// we have configured it to allow URL context path such as "1","2","3" and so on.
						// With this arrangement, remote speech processor jobs can connect to our
						// WebSocketSink via a distinct URL that carries their respective speech processor ids.
						_oTuple.sendToUrlContextPaths = 
							[(rstring)_vgwSessionIdToSpeechProcessorMap[BSD.vgwSessionId]];
						serializeTuple(_serializedTuple, _oTuple.blobData);
						submit(_oTuple, CDFSP);
					} else {
						//The incoming tuple contains an End of Call Signal (EOCS).
						//
						// Process the end of voice call signal.
						// Since there are two channels in every voice call,
						// those two channels will carry their own "End STT session"
						// message from the Voice Gateway. The logic below takes care of
						// handling two End of Call Signals for every voice call.
						//
						// Get the allocated speech processor id for a given vgwSessionId.
						// We should always have a speech processor id. If not, that is a 
						// case where the user didn't provision sufficient number of 
						// Speech engines and there was no idle speech processor available for that given vgwSessionId. 
						// This situation can be avoided by starting the application with a 
						// sufficient number of speech processors along with sufficient
						// speech engines needed for the anticipated maximum concurrent voice calls. 
						// A rule of thumb is to have two speech engines to process 
						// two speakers in every given concurrent voice call.
						//					
						if (has(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId) == true) {
							int32 speechProcessorId = 
								_vgwSessionIdToSpeechProcessorMap[BSD.vgwSessionId];
							// Let us send the EOCS to the chosen speech processor id.
							// We will do double serialization of the data as shown below.
							// 1) Let us serialize the received speech data tuple.
							// msgType = 2 means that it carries the EOCS tuple.
							_serializedTuple.msgType = 2;
							clearM(_serializedTuple.payload);
							// Call a native function to do the Tuple-->Blob conversion.
							serializeTuple(BSD, _serializedTuple.payload);
							
							// 2) Let us now serialize it one more time to be sent as the final tuple.
							_oTuple.strData = "";
							// We can tell the downstream WebSocketSink operator about to which
							// speech processor id our serialized data should be sent.
							// That is done by specifying the URL context path of the WebSocketSink
							// operator where a selected speech processor has connected to.
							// As shown in the param section of the downstream sink operator,
							// we have configured it to allow URL context path such as "1","2","3" and so on.
							// With this arrangement, remote speech processor jobs can connect to our
							// WebSocketSink via a distinct URL that carries their respective speech processor ids.
							_oTuple.sendToUrlContextPaths = [(rstring)speechProcessorId];
							serializeTuple(_serializedTuple, _oTuple.blobData);
							submit(_oTuple, CDFSP);										
							// Add the _key to the call completed list for the speech processor id to be
							// released later in the following if block only after receiving EOCS for 
							// both the voice channels of this call.
							insertM(_vgwSessionVgwVoiceChannelNumberCompletedMap, _key, true);
							
							rstring key1 = BSD.vgwSessionId + "_" + "1";
							rstring key2 = BSD.vgwSessionId + "_" + "2";						
							// Since this voice call is ending, let us release the speech processor id 
							// that was allocated above for this voice call.
							// Remove the speech processor id only if the EOCS signal
							// was sent for both of the voice channels. That must first 
							// happen before we can release the speech processor id.
							boolean key1Exists = has(_vgwSessionVgwVoiceChannelNumberCompletedMap, key1);
							boolean key2Exists = has(_vgwSessionVgwVoiceChannelNumberCompletedMap, key2);
							
							if ($numberOfEocsNeededForVoiceCallCompletion == 2 &&
								(key1Exists == true && key2Exists == true)) {
								// Since the voice call for this VGW session id has ended completely,
								// we can also release the speech processor id assigned for this call so that 
								// it can be repurposed for handling any new future calls.
								// We can go ahead and release the speech processor id by adding it back to 
								// the speech processor status list.
								// Let us decrement the given speech processor's current call handling count.
								// It is a zero based indexed array. Hence, we have to subtract by 1 to get the
								// current index in that array.
								_speechProcessorStatusList[speechProcessorId-1] = 
									_speechProcessorStatusList[speechProcessorId-1] - 1;
								
								// We can now do the clean-up in our state variables.
								removeM(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId);
								removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key1);
								removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key2);
								appTrc(Trace.error, "i) A call with vgwSessionId=" + BSD.vgwSessionId +
									" ended and its speech processor id " + (rstring)speechProcessorId + 
									" got released.");						
							} else if ($numberOfEocsNeededForVoiceCallCompletion == 1 &&
								(key1Exists == true || key2Exists == true)) {
								// If the user configured this application to handle
								// a single EOCS as sufficient to consider a voice call
								// completed for a given VGW session id, we will use this
								// block of code. Please refer to the constant i.e. expression
								// declaration section above to read the commentary about this idea.
								//
								// Since the voice call for this VGW session id has ended completely,
								// we can also release the speech processor id assigned for this call so that 
								// it can be repurposed for handling any new future calls.
								// We can go ahead and release the speech processor id by adding it back to 
								// the speech processor status list.
								// Let us decrement the given speech processor's current call handling count.
								// It is a zero based indexed array. Hence, we have to subtract by 1 to get the
								// current index in that array.
								_speechProcessorStatusList[speechProcessorId-1] = 
									_speechProcessorStatusList[speechProcessorId-1] - 1;
								
								// We can now do the clean-up in our state variables.
								removeM(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId);
	
								if(key1Exists == true) {
									removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key1);
								}
								
								if(key2Exists == true) {
									removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key2);
								}
								
								appTrc(Trace.error, "ii) A call with vgwSessionId=" + BSD.vgwSessionId +
									" ended and its speech processor id " + (rstring)speechProcessorId + 
									" got released.");						
							}
						} else {
							// Flag an error only when the user configured for two
							// EOCS tuples to be received for considering a voice call
							// as complted.
							if ($numberOfEocsNeededForVoiceCallCompletion == 2) {
								appTrc(Trace.error, 
									"_YYYYY No speech processor id is available at this time for the " +
									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
									" We are not going to process the currently received EOCS " +
									" of this speaker in this voice call. This is a serious error.");
							}					
						}						
					} // End of if(BSD.endOfCallSignal == false)				
				} // End of onTuple BSD
								
			config
				threadedPort: queue(BSD, Sys.Wait);
		} // End of Custom operator.

		// Invoke one or more instances of the WebSocketSink operator.
		// This operator listens on a TLS port and is configured with
		// multiple URL context paths to aceept connections from the
		// remote speech processor jobs that will each connect to a
		// particular context path expresded by their speech processor id.
		// It then routes the voice cslls' speech data and EOCS to the
		// corresponding speech processor job.
		// For now, we will have a parallel width of 1.
		// If it doesn't perform well, we can think of increasing the 
		// width later which will require some rework on the 
		// speech processor application code.
		@parallel(width = 1)
		() as VgwDataRouterSink = WebSocketSink(CallDataForSpeechProcessor as CDFSP) {
			logic
				state: {
					// Initialize the default TLS certificate file name if the 
					// user didn't provide his or her own.
					rstring _certificateFileName = 
						($certificateFileName != "") ?
						$certificateFileName : getThisToolkitDir() + "/etc/ws-server.pem";
				}
				
			param
				tlsPort: $tlsPortForVgwDataTx;
				certificateFileName: _certificateFileName;
				certificatePassword: $certificatePassword;
				trustedClientCertificateFileName: $trustedClientCertificateFileName;
				// Use this only when you have trouble authenticating clients that 
				// have self signed certificates.
				trustedClientX509SubjectIdentifiers: $trustedClientX509SubjectIdentifiers;
				nonTlsEndpointNeeded: $nonTlsEndpointNeeded;
				nonTlsPort: $nonTlsPortForWebSocketSink;
				urlContextPath: $webSocketSinkurlContextPath;
				websocketLiveMetricsUpdateNeeded: $websocketLiveMetricsUpdateNeeded;
				websocketLoggingNeeded: $websocketLoggingNeeded;
				wsConnectionLoggingNeeded: $wsConnectionLoggingNeeded;
				wsClientSessionLoggingNeeded: $wsClientSessionLoggingNeeded;
				websocketStaleConnectionPurgeInterval: $websocketStaleConnectionPurgeInterval;
				ipv6Available: $ipv6Available;
				clientWhitelist: $clientWhitelist;
				maxClientConnectionsAllowed: $maxClientConnectionsAllowed;
				
			config
				// Always place it on a machine that carries the VgwDataRouter host tag.
				placement: host(VgwPool[0]);
				threadedPort: queue(CDFSP, Sys.Wait);
		}
		
	// This is a composite level configuration to declare a hostpool using host tags.
	config
		// To learn about how to create host tags and assign them to the machines in
		// your Streams instance, you can read tips-on-using-streams-host-tags.txt available 
		// in the SPL-Examples-For-Beginners/047_streams_host_tags_at_work/host.tags directory.
		hostPool: 
			VgwPool = createPool({size=1u, tags=["VgwDataRouter"]}, Sys.Shared); /*Sys.Exclusive*/
} // End of the main composite.

// The following analytic composite receives a signal file name
// from an upstream operator. It will look for
// call meta data and call speech data of pre-recorded 
// voice calls in the same directory as that of the signal file.
// Then, it will send the call speech data combined with the
// call meta data for transcription by the downstream operators.
//
// Please note that this composite can have its own 
// parallel region for the purpose of load testing by 
// replaying many pre-recorded voice calls at the same time.
public composite CallRecordingReplay(input CallReplaySignalFileNameIn;
	output PreRecordedBinarySpeechData) {
	param
		expression <rstring> $callRecordingReadDirectory;
		// This composite operator receives externally
		// defined stream types via operator parameters.
		type $callMetaData_t;
		type $binarySpeech_t;
	
	// Replaying the pre-recorded voice calls.
	// The graph below will perform the logic necessary to
	// read call meta data and speech data from pre-recorded calls and
	// then do a replay. User must override the default value (/dev) for the 
	// callRecordingReadDirectory submission time parameter with their
	// own valid directory name. In that user-specified directory,
	// they can keep copying pre-recorded audio files, 
	// call metadata file along with a signal file to initiate transcription 
	// by using the speech data stored in them.
	// User must first ensure that the pre-recorded raw speech files, call metadata files 
	// for both the voice channels of a given call exist in that directory.
	// After ensuring that those files for both the voice channels of a 
	// given call exist, user can execute the following Linux command inside that directory.
	// touch  <vgwSessionId>.process-mulaw
	// e-g: touch  73269584.process-mulaw
	// This command will initiate the replay of the audio data as it   
	// gets read from the pre-recorded voice calls.	
	graph		
		(stream<CallReplaySignalFileNameIn> CallReplaySignalFileName as CRSF) as 
			 ReplaySignalFileNameFilter = Custom(CallReplaySignalFileNameIn as CRSFI) {
			logic
				onTuple CRSFI: {
					// Simply keep forwarding it to the downstream operator.
					submit(CRSFI, CRSF);
				}
				
			config
				threadedPort: queue(CRSFI, Sys.Wait);
		}
		
		// Control the replay of only one pre-recorded voice call at a time.
		// Release only one replay signal file at a time.
		(stream<rstring fileName> GatedCallReplaySignalFileName) as 
		 CallReplaySignalGate = Gate(CallReplaySignalFileName; Acknowledgement) {
		 	param
		 		// Allow only 1 tuple to go through at a time
		 		maxUnackedTupleCount : 1u;  
		 		// Acknowledge the specified number of tuples.
		 		// In our case here, we will do one tuple at a time.
		 		numTuplesToAck : Acknowledgement.count; 
		 }
		
		// Form the call meta data and call speech data file names and
		// then send them out for reading the data stored inside of them.
		(stream<rstring fileName> CallMetaDataFileNameVC1 as CMDFVC1;
		 stream<rstring fileName> CallSpeechDataFileNameVC1 as CSDFVC1;
		 stream<rstring fileName> CallMetaDataFileNameVC2 as CMDFVC2;
		 stream<rstring fileName> CallSpeechDataFileNameVC2 as CSDFVC2) as 
		 PreRecordedCallFileNameCreator = Custom(GatedCallReplaySignalFileName as CRSFN) {
		 logic
		 	onTuple CRSFN: {
		 		// User is initiating a replay through a signal file.
		 		// This file name will have this format.
		 		// <vgwSessionId>.process-mulaw
				// e-g: 73269584.process-mulaw
				// We can parse just the VGW session id from this file name.
				//
				// Tokenize the fully qualified signal file name,
				list<rstring> tokens = tokenize(CRSFN.fileName, "/", true);
				// Get the very last token which is just the signal file name.
				rstring signalFileName = tokens[size(tokens)-1];
				// Parse the VGW session id from the file name.
				int32 idx = findFirst(signalFileName, ".process-mulaw");
				rstring vgwSessionId = substring(signalFileName, 0, idx);
				
				// We can remove the signal file now.
				mutable int32 rc = 0;
				mutable uint64 fileSize = 0ul;
				remove(CRSFN.fileName, rc);
				
				// At this point, we must ensure that we have a call metadata file and
				// a call speech data file for voice channels 1 and 2 in the same directory where 
				// the signal file is present (i.e. call recording read directory).
				// Those file names will be in the following format.
				// File name format:  <vgwSessionId>-<voiceChannelNumber>-metadata.json
				// e-g: 73269584-1-metadata.json  and  73269584-2-metadata.json				
				//
				// File name format:  <vgwSessionId>-<voiceChannelNumber>-mulaw.bin
				// e-g: 73269584-1-mulaw.bin  and  73269584-2-mulaw.bin
				//
				// Prepare file names for voice channel 1 and check if they exist.
				rstring callMetaDataFileNameForVC1 = $callRecordingReadDirectory + "/" + 
					vgwSessionId + "-1-metadata.json";
				// Check if this file exists.
				fstat(callMetaDataFileNameForVC1, "size", fileSize, rc);
				
				if (rc != 0) {
					appTrc(Trace.error, "Skipping call replay for VGW session id " + 
						vgwSessionId + ". File not found: " + callMetaDataFileNameForVC1);
					return; 
				}
				
				rstring callSpeechDataFileNameForVC1 = $callRecordingReadDirectory + "/" + 
					vgwSessionId + "-1-mulaw.bin";
				// Check if this file exists.
				fstat(callSpeechDataFileNameForVC1, "size", fileSize, rc);
				
				if (rc != 0) {
					appTrc(Trace.error, "Skipping call replay for VGW session id " + 
						vgwSessionId + ". File not found: " + callSpeechDataFileNameForVC1);
					return; 
				}				
				
				// Prepare file names for voice channel 2 and check if they exist.
				rstring callMetaDataFileNameForVC2 = $callRecordingReadDirectory + "/" + 
					vgwSessionId + "-2-metadata.json";
				// Check if this file exists.
				fstat(callMetaDataFileNameForVC2, "size", fileSize, rc);
				
				if (rc != 0) {
					appTrc(Trace.error, "Skipping call replay for VGW session id " + 
						vgwSessionId + ". File not found: " + callMetaDataFileNameForVC2);
					return; 
				}
				
				rstring callSpeechDataFileNameForVC2 = $callRecordingReadDirectory + "/" + 
					vgwSessionId + "-2-mulaw.bin";
				// Check if this file exists.
				fstat(callSpeechDataFileNameForVC2, "size", fileSize, rc);
				
				if (rc != 0) {
					appTrc(Trace.error, "Skipping call replay for VGW session id " + 
						vgwSessionId + ". File not found: " + callSpeechDataFileNameForVC2);
					return; 
				}				
				
				// We have all the required call meta data and call speech data files 
				// for both the voice channels of a given voice call. We can proceed by
				// sending those file names for reading by the downstream operators.
				// Send the meta data file names for both the voice channels first.
				mutable CallMetaDataFileNameVC1 oTuple1 = {};
				mutable CallSpeechDataFileNameVC1 oTuple2 = {};
				oTuple1.fileName = callMetaDataFileNameForVC1;
				submit(oTuple1, CMDFVC1);
				oTuple1.fileName = callMetaDataFileNameForVC2;
				submit(oTuple1, CMDFVC2);
				
				// Give a sufficient amount of delay for the call metadata JSON to 
				// get read before we send the speech data.
				block(30.0);

				// Send the speech data file names for both the voice channels now.				
				oTuple2.fileName = callSpeechDataFileNameForVC1;
				submit(oTuple2, CSDFVC1);
				oTuple2.fileName = callSpeechDataFileNameForVC2;
				submit(oTuple2, CSDFVC2);
		 	}
		}
	
		// This operator will read the call meta data JSON for voice channel 1.
		(stream<Json> CallMetaDataJsonVC1) as 
		 CallMetaDataJsonReader1 = FileSource(CallMetaDataFileNameVC1) {
		 	param
		 		format: line;
		 }
		 
		// This operator will read the call meta data JSON for voice channel 1.
		(stream<Json> CallMetaDataJsonVC2) as 
		 CallMetaDataJsonReader2 = FileSource(CallMetaDataFileNameVC2) {
		 	param
		 		format: line;
		 }
		 
		 // This operator will read the call binary speech data for voice channel 1.
		(stream<blob speech, rstring fileName> CallSpeechDataVC1 as CSD) as 
		 CallSpeechDataReader1 = FileSource(CallSpeechDataFileNameVC1) {
			param
				format: block;
				// Typical file size for a two minute call recording is 1MB.
				// We will support replay of up an hour long call i.e. 30MB file size.
				// If user wants to do a replay of longer calls than that, he/she must
				// change the block size below accordingly.
				blockSize: (uint32)(30 * 1024 * 1024);
				
			output 
				CSD: fileName = FileName();
		}
		
		 // This operator will read the call binary speech data for voice channel 2.
		(stream<blob speech, rstring fileName> CallSpeechDataVC2 as CSD) as 
		 CallSpeechDataReader2 = FileSource(CallSpeechDataFileNameVC2) {
			param
				format: block;
				// Typical file size for a two minute call recording is 1MB.
				// We will support replay of up an hour long call i.e. 30MB file size.
				// If user wants to do a replay of longer calls than that, he/she must
				// change the block size below accordingly.
				blockSize: (uint32)(30 * 1024 * 1024);

			output 
				CSD: fileName = FileName();
		}
		
		// This operator will convert the call meta data JSON to tuple for voice channel 1.
		(stream<$callMetaData_t> CallMetaDataVC1) as 
		 CallMetaDataConverter1 = JSONToTuple(CallMetaDataJsonVC1) {
		 	param
		 		ignoreParsingError: true;
		}
		 
		// This operator will convert the call meta data JSON to tuple for voice channel 2.
		(stream<$callMetaData_t> CallMetaDataVC2) as 
		 CallMetaDataConverter2 = JSONToTuple(CallMetaDataJsonVC2) {
		 	param
		 		ignoreParsingError: true;
		}

		(stream<boolean signal> TimerSignal) as
			ReplayHangDetectionTimer = Beacon() {
			param
				// In theory, replay of a pre-recorded call should keep
				// happening much quicker as long as there are enough
				// number of available speech to text engines. 
				// If a replay for a call gets stuck due to file read 
				// errors or JSON conversion errors, this timer signal will
				// help us to cancel the stuck replay and get ready to
				// replay the next available pre-recorded call.
				//
				// Beacon will send a signal right away when it starts up and
				// then keep sending more tuples steadily at a periodic interval.
				// 
				// Send a signal for every 60 minutes.
				period: 60.0 * 60.0; 	
				initDelay: 10.0;
		}

		// This operator will receive both the call meta data and the
		// call speech data fragments for voice channels 1 and 2. It will 
		// mix both of them and send out a tuple for transcription by
		// downstream operators.
		(stream<$binarySpeech_t> PreRecordedBinarySpeechData as PRBSD;
		 stream<uint32 count> Acknowledgement as Ack) as 
		 PreRecordedCallReplayer = Custom(CallMetaDataVC1, CallMetaDataVC2 as CMD;
		 	CallSpeechDataVC1, CallSpeechDataVC2 as CSD; TimerSignal as TS) {
		 	logic
		 		state: {
		 			// Key = vgwSessionId-vgwVoiceChannelNumber
		 			// Value = Call meta data tuple.
		 			mutable map<rstring, $callMetaData_t> _callMetaDataMap = {};
		 			// Key = vgwSessionId-vgwVoiceChannelNumber
		 			// Value = Cumulative speech data fragment count.
		 			mutable map<rstring, int32> _speechDataFragmentCount = {};
		 			// Key = vgwSessionId-vgwVoiceChannelNumber
		 			// Value = Cumulative speech data bytes count.
		 			mutable map<rstring, int32> _speechDataBytesCount = {};
		 			// Key = vgwSessionId-vgwVoiceChannelNumber
		 			// Value = Call speech data file size.
		 			mutable map<rstring, int32>  _speechDataFileSize = {};
		 			// Key = vgwSessionId-vgwVoiceChannelNumber
		 			// Value = Call speech data tuples sent.
		 			mutable map<rstring, int32>  _speechDataTuplesSentCount = {};		 			
					mutable int32 _voiceChannelsCompletedCnt = 0;
					// It is used for detecting any replay that is stuck
					// waiting for data to be read from the pre-recorded files.
					mutable rstring lastObservedReplayMapKey = "abcxyz";
		 			mutable $binarySpeech_t _oTuple1 = {};
		 		}
		 		
		 		onTuple CMD: {
		 			// When the call meta data for a given voice channel arrives here,
		 			// simply insert into the state map.
		 			rstring key = CMD.vgwSessionId + "-" + 
		 				(rstring)vgwVoiceChannelNumber;
		 				
					appTrc(Trace.error, 
					    "Pre-Recorded call replay channel=" + (rstring)getChannel() + 
						". Received a replay request for a pre-recorded voice call " + 
						key + ".");
						
					// Let us check if this call meta data is arriving late i.e.
					// we already received the speech data before even receiving this
					// call meta data. This can happen since the JSON based call meta data and
					// the binary based speech data are read by different FileSource operators above.
					if(has(_speechDataFileSize, key) == true &&
					   _speechDataFileSize[key] != 0) {
						// This is not good. Speech data arrived here ahead of the call meta data.
						appTrc(Trace.error, "*** A T T E N T I O N *** " +
						    "Pre-Recorded call replay channel=" + (rstring)getChannel() + 
							". Speech data seems to have arrived ahead of the call meta data for " + 
							key + ". Speech data file zie=" + (rstring)_speechDataFileSize[key]);
					}
					
		 			insertM(_callMetaDataMap, key, CMD);	
		 			insertM(_speechDataFragmentCount, key, 0);
		 			insertM(_speechDataBytesCount, key, 0);
		 			insertM(_speechDataFileSize, key, 0);	 			
		 			insertM(_speechDataTuplesSentCount, key, 0);
		 		}
		 		
		 		onTuple CSD: {
		 			// When the call speech data for a given voice channel arrives here,
		 			// we must ensure that we already received the call meta data for
		 			// this voice channel.
		 			// The fileName attribute will carry the name of the current
		 			// speech data file in the following format.
					// File name format:  <vgwSessionId>-<voiceChannelNumber>-mulaw.bin
					// e-g: 73269584-1-mulaw.bin  and  73269584-2-mulaw.bin
					//
					// Tokenize the fully qualified signal file name,
					// e-g: /home/streamsadmin/call-recording-read/442786-1.process-mulaw
					list<rstring> tokens = tokenize(CSD.fileName, "/", true);
					// Get the very last token which is just the speech data file name.
					rstring speechDataFileName = tokens[size(tokens)-1];
					// Parse the <vgwSessionId>-<voiceChannelNumber> from the file name.
					int32 idx = findFirst(speechDataFileName, "-mulaw.bin");
					rstring key = substring(speechDataFileName, 0, idx);
					
		 			// Update the voice channel specific counters.
		 			_speechDataFragmentCount[key] = 
		 				_speechDataFragmentCount[key] + 1;
		 			_speechDataBytesCount[key] = 
		 				_speechDataBytesCount[key] + size(CSD.speech);

					// When we receive the very first speech data fragment for a
					// given <vgwSessionId>-<voiceChannelNumber>, we will save the
					// size of the speech data file. We need that information to 
					// detect the condition when we complete the reception of all the data
					// from that file. (I tried with window marker onPunct and I had
					// some race conditions with that. Hence, I'm using this approach.)
					if (_speechDataFragmentCount[key] == 1) {
						// Get the file size and store it only once at the very beginning.
						mutable int32 rc = 0;
						mutable uint64 fileSize = 0ul;
						// Get the file size.
						fstat(CSD.fileName, "size", fileSize, rc);
						
						if (rc != 0) {
							appTrc(Trace.error, "Unable to get fize size for " +  CSD.fileName + 
								". Serious error. Skipping the replay for this file now.");
							return; 
						}	
						
						_speechDataFileSize[key] = (int32)fileSize;			
					}

					// Have we already received the call meta data for this voice call?
					if (has(_callMetaDataMap, key) == false) {
						// We have not yet received the call meta data.
						// That is not good. We must skip this speech data fragment.
						appTrc(Trace.error, "Skipping a speech data fragment for " + 
							key + " since we have not yet received the call meta data.");
						return;
					}

					// We have the call meta data. We can create a new 
					// binary speech data tuple now and send it out for transcription.
					// Copy all the call meta data attributes to the outgoing tuple.
					_oTuple1 = ($binarySpeech_t){};
					assignFrom(_oTuple1, _callMetaDataMap[key]);
					_oTuple1.speech = CSD.speech;
					_oTuple1.endOfCallSignal = false;
					_oTuple1.speechDataFragmentCnt = _speechDataFragmentCount[key];
					_oTuple1.totalSpeechDataBytesReceived = _speechDataBytesCount[key];
					submit(_oTuple1, PRBSD);
					// Update the speech data tuples sent count for a given key.
					_speechDataTuplesSentCount[key] = 
						_speechDataTuplesSentCount[key] + 1;
					
					// Have we replayed all the speech data for this voice channel?
					if(_speechDataBytesCount[key] >= _speechDataFileSize[key]) {
						_voiceChannelsCompletedCnt++;
					}
					
					//  Have we replayed the speech data in full for both the voice channels?
					if (_voiceChannelsCompletedCnt == 2) {
						// Wait for a sufficient amount of time for the submit command carrying the
						// final speech data fragment for this voice call to get
						// processed. After that delay, we can send the EOCS.
						// if we don't do this delay, I have seen in my testing that 
						// EOCS getting ahead of the final speech data fragment and 
						// causing racing conditions and other logic issues in the
						// upstream composite that does the call recording write activity.
						block(30.0);
						
						// We can send an end of call signal for the just  
						// finished replay of the pre-recorded voice call.
						//
						// Reset the voice channels completed count.
						_voiceChannelsCompletedCnt = 0;
						
						// Send two EOCS signals one for each voice channel in the given call.
						for (rstring str in _callMetaDataMap) {
							_oTuple1 = ($binarySpeech_t){};
							_oTuple1.vgwSessionId = _callMetaDataMap[str].vgwSessionId;
							_oTuple1.isCustomerSpeechData = 
								_callMetaDataMap[str].isCustomerSpeechData;
							_oTuple1.vgwVoiceChannelNumber = 
								_callMetaDataMap[str].vgwVoiceChannelNumber;
							_oTuple1.endOfCallSignal = true;
							submit(_oTuple1, PRBSD);
						}
						
						// Clear all the state maps to be ready for the
						// next pre-recorded voice call replay.
						clearM(_callMetaDataMap);
						clearM(_speechDataFragmentCount);
						clearM(_speechDataBytesCount);
						clearM(_speechDataFileSize);
						clearM(_speechDataTuplesSentCount);

						// Let us send an acknowledgement to the upstream
						// Gate operator via the feedback path so that 
						// the next pre-recorded call can be sent here for replay.
						// Tell the upstream Gate operator to allow the next tuple.
						submit({count = 1u}, Ack); 
					}
		 		}
		 		
		 		onTuple TS: {
					// In theory, replay of a pre-recorded call should keep
					// happening much quicker as long as there are enough
					// number of available speech to text engines. 
					// If a replay for a call gets stuck due to file read 
					// errors or JSON conversion errors, this timer signal will
					// help us to cancel the stuck replay and get ready to
					// replay the next available pre-recorded call.
					//
					// As long as replay activity keeps going correctly,
					// we have nothing much to do in this timer handler.
					// Check if the same replay map key that we observed during the
					// previous timer tick is still there. Since our timer interval is
					// sufficiently longer (60 minutes), if we still see the same 
					// replay map key, that means it is stuck.
					if (has(_callMetaDataMap, lastObservedReplayMapKey) == false) {
						// It is a new replay map key now. That means all going well.
						// Let us refresh our last observed map key and leave this timer handler..
						lastObservedReplayMapKey = "abcxyz";
						
						// Simply set to any first key available in that map.
						for(rstring key in _callMetaDataMap) {
							lastObservedReplayMapKey = key;
							break;
						}
						
						// Replay is not stuck at this time.
						return;
					} else {
						// We still see the same map key that was observed in the
						// previous timer tick. That means this replay activity has been
						// sitting there for a while. Let us get it unstuck by
						// canceling that replay.
						appTrc(Trace.error, 
						    "Pre-Recorded call replay channel=" + (rstring)getChannel() + 
							". Detected a hung replay for a pre-recorded voice call " + 
							lastObservedReplayMapKey  + ". Canceling that replay now.");
													
						// Reset our last observed map key to a dummy value.
						lastObservedReplayMapKey = "abcxyz";
						
						// If there was at least one replay tuple got sent, then
						// we are required to send an EOCS for that voice channel.
						for (rstring str in _callMetaDataMap) {
							if (_speechDataTuplesSentCount[str] > 0) {
								_oTuple1 = ($binarySpeech_t){};
								_oTuple1.vgwSessionId = _callMetaDataMap[str].vgwSessionId;
								_oTuple1.isCustomerSpeechData = 
									_callMetaDataMap[str].isCustomerSpeechData;
								_oTuple1.vgwVoiceChannelNumber = 
									_callMetaDataMap[str].vgwVoiceChannelNumber;
								_oTuple1.endOfCallSignal = true;
								submit(_oTuple1, PRBSD);
							}
						}
						
						// Reset the voice channels completed count.
						_voiceChannelsCompletedCnt = 0;

						// Clear all the state maps to be ready for the
						// next pre-recorded voice call replay.
						clearM(_callMetaDataMap);
						clearM(_speechDataFragmentCount);
						clearM(_speechDataBytesCount);
						clearM(_speechDataFileSize);
						clearM(_speechDataTuplesSentCount);

						// Let us send an acknowledgement to the upstream
						// Gate operator via the feedback path so that 
						// the next pre-recorded call can be sent here for replay.
						// Tell the upstream Gate operator to allow the next tuple.
						submit({count = 1u}, Ack); 
					}
		 		}
			
			config
				threadedPort: queue(CSD, Sys.Wait);
		 } 
} // End of the composite CallRecordingReplay

// Following are the common utility functions for 
// use within the composites declared above.
//
// This function computes the number of allowed concurrent calls per speech processor job.
public stateful int32 getNumberOfConcurrentCallsAllowedPerSpeechProcessor(
	int32 numberOfSpeechEnginesPerSpeechProcessorJob) {
	// Check if there is an even number of speech engines conffigured for each speech processor job.
	if(numberOfSpeechEnginesPerSpeechProcessorJob % 2 != 0) {
		// This is not a correct configuration. Let us abort now.
		appTrc(Trace.error, 
			"_XXXXX Incorrect configuration for the number of speech engines per speech processor job. " + 
			"There should be an even number of speech engines running in every speech processor job.");
		abort();
	}
	
	// There are two voice channels in a voice call.
	// Each voice channel will require a dedicated speech engine.
	// So, for a given voice call, two speech engines will be required.
	int32 x = numberOfSpeechEnginesPerSpeechProcessorJob / 2;
	// That means each speech processor can handle a maximum of x concurrent calls.
	return(x);
}

// This function creates a new list with as many elements as 
// the number of speech processors configured to run.
// Every element will be initialized to 0 to mean that   
// all such speech processors are idle at this time. 
public list<int32> prepareIdleSpeechProcessorsList(int32 speechProcessorsCount) {
	mutable list<int32> myList = [];
	mutable int32 idx = 1;
	
	while(idx <= speechProcessorsCount) {
		// Initialize it with 0 to indicate that no voice csll 
		// is currently being processed by a given speech processor.
		appendM(myList, 0);
		idx++;
	}
	
	return(myList);
}

// This function takes the speech processors status list as an input and
// returns a speech processor id that has unused call processing capacity.
// If all the speech processors are busy at this time, it will return -1.
public int32 getSpeechProcessorIdForNewCallProcessing(
	int32 numberOfConcurrentCallsAllowedPerSpeechProcessor,
	mutable list<int32> speechProcessorStatusList) {
	if (size(speechProcessorStatusList) <= 0) {
		return(-1);
	}
	
	mutable int32 idx = -1;
	mutable boolean speechProcessorAvailable = false;
	
	for(int32 x in speechProcessorStatusList) {
		idx++;
		
		if(x < numberOfConcurrentCallsAllowedPerSpeechProcessor) {
			// This speech processor is not handling its max allowed concurrent calls.
			speechProcessorAvailable = true;
			break;
		}
	} // End of for loop.
	
	if(speechProcessorAvailable == false) {
		// All the speech processors are busy at this time.
		// This is not good news for a newly arrived voice call.
		return(-1);
	}
	
	// A newly arrived call's processing will be assigned to this speech processor.
	// Let us increment the given speech processor's current call handling count.
	speechProcessorStatusList[idx] = speechProcessorStatusList[idx] + 1;
	// Since it is a zero based index array, we will return a value by adding 1 to it.
	return(idx + 1);
}
