/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2019
==============================================
*/

/*
==============================================
First created on: Oct/28/2019
Last modified on: Nov/14/2019

A) What does this example application do?
   --------------------------------------
This example demonstrates the integration of the following three products to
achieve Real-Time Speech-To-Text transcription to get the text ready for
any further analytics.

1) IBM Voice Gateway v1.0.3.0 or higher
2) IBM Streams v4.2.1.6 or higher
3) IBM Watson Speech 2 Text (Embedded in an IBM Streams WatsonS2T operator v2.12.0)

These three components will work in the following sequence:

Your Telephony SIPREC-->IBM Voice Gateway-->IBM Streams-->WatsonS2T Operator

IMPORTANT NOTE
--------------
This example is applicable only for a few IBM Streams customers who
have bought licenses for IBM Streams and in turn obtained the 
com.ibm.streams.speech2text toolkit from the IBM official 
software download URL for doing speech analytics. That particular
toolkit (with restricted availability) embeds the Watson speech to text
engine via low-level speech APIs inside an IBM Streams operator. 
All other customers who are interested in using the 
Watson Speech To Text on the IBM public cloud or on the 
IBM Cloud Pak for Data (CP4D) can simply ignore this example application and
refer to a different example named VoiceGatewayToStreamsToWatsonSTT.

B) Other IBM Streams toolkits you will need for this application
   -------------------------------------------------------------
1) You must first ensure that your Streams machine where you will be compiling
this example application has these toolkits ready. This application has a 
dependency on these toolkits.
   a) streamsx.sttgateway toolkit (v1.0.5 or higher)
      https://github.com/IBMStreams/streamsx.sttgateway
   b) com.ibm.streamsx.json (v1.4.6 or higher)
      [This toolkit is already available in your Streams machine's $STREAMS_INSTALL/toolkits directory.]
   c) streamsx.network toolkit (v3.0.5)  
      https://github.com/Alex-Cook4/streamsx.network/tree/rtp-dev
      [You must make a minor code change in this toolkit before using it.
       For more details about the code change that you must make, see the 
       commentary below where this toolkit is brought in via the use statement.]
   d) com.ibm.streams.speech2text (v2.12.0)
      [This toolkit must be obtained from the same URL from where you 
       downloaded the official licensed version of the IBM Streams product.
       Only those customers who have a licensed version of IBM Streams can get this toolkit.]
   e) com.ibm.streamsx.inet.http (v3.0.0 or higher)
      [This toolkit is already available in your Streams machine's $STREAMS_INSTALL/toolkits directory.]
      
C) Building this example application
   ---------------------------------
You can build this example in a Linux terminal window via the make command by using the
Makefile available in the top-level directory of this example. Please follow
these steps to build this application:

1) streamsx.sttgateway toolkit also needs a specific version of the C++ boost library and 
the C++ websocketpp library. Please follow the instructions available in the documentation of 
that toolkit to download and prepare those libraries before you can compile this example application.

2) You have to either set the following environment variables on a terminal window where
you will run the make command or you can edit the Makefile and change these toolkit directories
to point to the correct location on your IBM Streams machine. 

   a) It will be necessary to export the STREAMS_STTGATEWAY_TOOLKIT (v1.0.5 or higher) environment variable by
      pointing it to the full path of your com.ibm.streamsx.sttgateway directory. 

   b) You have to export the STREAMS_NETWORK_TOOLKIT (v3.0.5) and point to the correct directory.
  
   c) You have to export the STREAMS_JSON_TOOLKIT (v1.4.6 or higher) and point to the correct directory.
   
   d) You have to export the STREAMS_SPEECH2TEXT_TOOLKIT (v2.12.0) and point to the correct directory.

   e) You have to export the STREAMS_INET_TOOLKIT (v3.0.0) and point to the correct directory.

   f) If you don't want to set the above-mentioned environment variables every time you want to
      build this application, you can also edit the Makefile and enter these directories there directly.

3) If you prefer to build this example inside the Streams Studio instead of the command line based
Makefile, there are certain build configuration settings needed. Please refer to the streamsx.sttgateway
toolkit documentation to learn more about those Streams Studio configuration settings.

D) Running this example application
   --------------------------------
1) In order to run this example, you must first configure the IBM Voice Gateway v1.0.3.0 or higher 
to send the binary speech data to this IBM Streams application. That involves configuring the 
IBM Voice Gateway media relay and the IBM Voice Gateway SIP integrator components and then
deploying them. Please refer to the streamsx.sttgateway documentation section titled 
"Requirements for this toolkit".

2) Once you are sure that the IBM Voice Gateway can send the speech data to IBM Streams,
you can give the following command to deploy this application in distributed mode.

st submitjob -d  <YOUR_STREAMS_DOMAIN>  -i  <YOUR_STREAMS_INSTANCE>  -P tlsPort=9443 -P vgwSessionLoggingNeeded=false -P numberOfS2TEngines=14 -P WatsonS2TConfigFile=<FULL_PATH_TO_YOUR_S2T_CONFIG_FILE> -P WatsonS2TModelFile=<FULL_PATH_TO_YOUR_S2T_MODEL_FILE> -P ipv6Available=false -P writeTranscriptionResultsToFiles=true -P sendTranscriptionResultsToHttpEndpoint=true -P httpEndpointForSendingTranscriptionResults=<YOUR_HTTP_ENDPOINT_URL> output/com.ibm.streamsx.sttgateway.sample.watsons2t.VoiceGatewayToStreamsToWatsonS2T.sab

NOTE: Just to prove that the STT JSON results are getting sent via HTTP,
you can use a test http receiver Streams application included in the
samples directory of the streamsx.sttgateway toolkit.
stt_restults_http_receiver is the name of that test application.

3) When the live calls are happening, you can watch the data directory in your copy of
this application for specific files per call that will show you just the utterances or 
the full transcription details for every voice call.
==============================================
*/

namespace com.ibm.streamsx.sttgateway.sample.watsons2t;

// This namespace contains the IBMVoiceGatewaySource operator that we use below.
use com.ibm.streamsx.sttgateway.watson::*;

// This application needs a special toolkit containing the RTP (Real Time Protocol) operators. 
// Version 3.0.5 of the streamsx.network toolkit has to be downloaded from the main page of the URL shown below.
// https://github.com/Alex-Cook4/streamsx.network/tree/rtp-dev
// [Click the "Clone or download" Green button at the top right of this page and then click on "Download ZIP".
// After extracting the downloaded ZIP file, you must first make a minor
// code change in that toolkit as explained below.
// a) Edit this file in a text editor:
//    com.ibm.streamsx.network/com.ibm.streamsx.network.rtp/RtpDecode/RtpDecode_cpp.cgt
// b) Search for malloc(5000) and comment out that line that does a
//    hardcoded amount of memory allocation as shown below.
//    unsigned char * decompressed = (unsigned char *)malloc(5000);
// c) Just right below that newly commented line, you will see the following statement:
//    int length = <%=$inputTuple%>.get_payloadLength();
// d) Add the following line right after that statement which 
//    calculates the length of the payload and then save that file.
//    unsigned char * decompressed = (unsigned char *)malloc(length*2);
//
// Now, you can run the "ant all" command from inside the top-level
// directory of this toolkit i.e. streamsx.network-rtp-dev directory.
use com.ibm.streamsx.network.rtp::*; 

// Speech-2-Text i.e. WatsonS2T operator comes from this namespace.
use com.ibm.streams.speech2text.watson::*;

// com.ibm.streamsx.inet toolkit shipped with your IBM Streams installation is sufficient.
use com.ibm.streamsx.inet.http::*;
// com.ibm.streamsx.json toolkit shipped with your IBM Streams installation is sufficient.
use com.ibm.streamsx.json::*;

// This is a schema that will get used in two different 
// composites in this file. So, we have to define it
// outside of those composites in order for the compiler to
// resolve this type correctly.
// 
// This S2T result type contains many attributes to
// demonstrate all the basic and very advanced features of 
// the Watson S2T engine. Not all real-life applications will need 
// all these attributes. You can decide to include or omit these
// attributes based on the specific S2T features your application will need. 
// Trimming the unused attributes will also help in 
// reducing the S2T processing overhead and in turn 
// help in receiving the S2T results faster.
// Read the com.ibm.streams.speech2text toolkit documentation to learn about
// what features are available, how they work and how different attributes are 
// related to those features.
type S2TResult_t = rstring vgwSessionId, boolean isCustomerSpeechData, 
	int32 vgwVoiceChannelNumber, 
	rstring callerPhoneNumber, rstring agentPhoneNumber,
	int32 speechDataFragmentCnt, int32 totalSpeechDataBytesReceived, 
	int32 s2tEngineId, int32 s2tResultProcessorId,
	rstring id, float64 utteranceStartTime,
	float64 utteranceEndTime, int32 utteranceNumber, 
	rstring utteranceText, float64 utteranceConfidence, 
	list<rstring> utteranceWords, list<int32> utteranceSpeakers,
	list<float64> utteranceWordConfidences, list<rstring> nBestHypotheses, 
	list<rstring> wordAlternatives, 
	list<list<float64>> wordAlternativesConfidences,
	list<float64> wordAlternativesStartTimes;

// This is a very minimal set of STT Results you can have.
/*
type S2TResult_t = rstring vgwSessionId, boolean isCustomerSpeechData, 
	int32 vgwVoiceChannelNumber, 
	rstring callerPhoneNumber, rstring agentPhoneNumber,
	int32 speechDataFragmentCnt, int32 totalSpeechDataBytesReceived, 
	int32 s2tEngineId, int32 s2tResultProcessorId,
	rstring id, rstring utteranceText, 
	boolean transcriptionCompleted;
*/
	
type S2TDiagnosticsResult_t = rstring log;

// This is the main composite for this application.
public composite VoiceGatewayToStreamsToWatsonS2T {
	param
		// IBM Voice Gateway related submission time values are defined below.
		// TLS port on which this application will listen for
		// communicating with the IBM Voice Gateway.
		expression<uint32> $tlsPort : 
			(uint32)getSubmissionTimeValue("tlsPort", "443");
		// User can optionally specify whether they want a non-TLS endpoint.
		expression<boolean> $nonTlsEndpointNeeded : 
			(boolean)getSubmissionTimeValue("nonTlsEndpointNeeded", "false");
		// Non-TLS (Plain) port on which this application will
		// (optionally) listen for communicating with the IBM Voice Gateway.
		expression<uint32> $nonTlsPort : 
			(uint32)getSubmissionTimeValue("nonTlsPort", "80");
		// Server side certificate (.pem) file for the WebSocket server.
		// It is necessary for the users to create a Root CA signed 
		// server side certificate file and point to that file at the time of
		// starting this application. If the user doesn't point to this file
		// at the time of starting the application, then the application will
		// look for a default file named ws-server.pem inside the etc sub-directory
		// of the application. This certificate will be presented to the
		// IBM Voice Gateway for validation when it establishes a WebSocket 
		// connection with this application. For doing quick tests, you may save
		// time and effort needed in getting a proper Root CA signed certificate 
		// by going with a simpler option of creating your own self-signed 
		// certificate. Please ensure that using a self-signed certificate is 
		// allowed in your environment. We have provided a set of instructions to
		// create a self signed certificate. Please refer to the following
		// file in the etc sub-directory of this application:
		// etc/creating-a-self-signed-certificate.txt
		expression<rstring> $certificateFileName :
			getSubmissionTimeValue("certificateFileName", "");
		// Is live metrics needed for the IBMVoiceGatewaySource operator?
		expression<boolean> $vgwLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("vgwLiveMetricsUpdateNeeded", "true");
		// Is WebSocket library low level logging needed?
		expression<boolean> $vgwWebsocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("vgwWebsocketLoggingNeeded", "false");
		// Is IBM Voice Gateway message exchange logging needed for debugging?
		expression<boolean> $vgwSessionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("vgwSessionLoggingNeeded", "false");
		//
		// IBM Watson S2T related submission time values are defined below.
		expression<int32> $numberOfS2TEngines :(int32)
			getSubmissionTimeValue("numberOfS2TEngines", "10") ;
		// Time in seconds to wait before sending data to the S2T engines.
		expression<float64> $initDelayBeforeSendingDataToS2TEngines :
			(float64)getSubmissionTimeValue("initDelayBeforeSendingDataToS2TEngines", "15.0"); 
		// Time interval in seconds during which the VGW source operator below should
		// do memory cleanup of any Voice Gateway sessions that end abruptly in the
		// middle of a voice call.
		expression<uint32> $vgwStaleSessionPurgeInterval :(uint32)
			getSubmissionTimeValue("vgwStaleSessionPurgeInterval", "10800");
		// Is ipv6 protocol stack available in the Streams machine where the
		// IBMVoiceGatewaySource operator is going to run?
		// Most of the Linux machines will have ipv6. In that case,
		// you can keep the following line as it is.
		// If you don't have ipv6 in your environment, you can set the
		// following submission time value to false.
		expression<boolean> $ipv6Available : (boolean)
			getSubmissionTimeValue("ipv6Available", "true");

	type
		// The following is the schema of the first output stream for the
		// IBMVoiceGatewaySource operator. The first four attributes are
		// very important and the other ones are purely optional if some
		// scenarios really require them.
		// blob speech --> Speech fragments of a live conversation as captured and sent by the IBM Voice Gateway.
		// rstring vgwSessionId --> Unique identifier of a voice call. 
		// boolean isCustomerSpeechData --> Every voice call will have a customer channel and an agent channel.
		//                                  This attribute tells whether this output stream carries customer speech data or not.
		// int32 vgwVoiceChannelNumber --> This indicates the voice channel number i.e. 1 or 2.
		//                                 Whoever (caller or agent) sends the first round of 
		//                                 speech data bytes will get assigned a voice channel of 1. 
		//                                 The next one to follow will get assigned a voice channel of 2.
		// rstring id --> This attribute is needed by the WatsonS2T operator. 
		//                It is set to vgwSessionId_vgwVoiceChannelNumber
		// rstring callerPhoneNumber --> Details about the caller's phone number.
		// rstring agentPhoneNumber --> Details about the agent's phone number.
		// int32 speechDataFragmentCnt --> Number of fragments (tuples) emitted so far on a given channel (customer or agent) for a given vgwSessionId.
		// int32 totalSpeechDataBytesReceived --> Number of speech bytes received so far on a given channel (customer or agent) for a given vgwSessionId.
		// int32 s2tEngineId --> This attribute will be set in the next operator. (Please, read the comments there.)
		// int32 s2tResultProcessorId --> This attribute will be set in the next operator. (Please, read the comments there.)
		// blob payload --> This is strictly needed only in the VoiceGatewayToStreamsToWatsonS2T application since it requires the
		//                  RtpDecode operator which expects this attribute to contain the mulaw formatted speech blob fragment.
		//                  In order to support this application's need, we are forced to copy the contents of the 
		//                  blob speech attribute into this payload attribute. This memory copy is an unavoidable 
		//                  performance overhead for the VoiceGatewayToStreamsToWatsonS2T application. We have to live with it.
		// int32 payloadLength --> Description above for the payload attribute is applicable here as well.
		BinarySpeech_t = blob speech, rstring vgwSessionId, boolean isCustomerSpeechData, 
			int32 vgwVoiceChannelNumber, rstring id,
			rstring callerPhoneNumber, rstring agentPhoneNumber, 
			int32 speechDataFragmentCnt, int32 totalSpeechDataBytesReceived, 
			int32 s2tEngineId, int32 s2tResultProcessorId,
			blob payload, int32 payloadLength;
		// The following schema is for the second output stream of the
		// IBMVoiceGatewaySource operator. It has three attributes indicating
		// the speaker channel (vgwVoiceChannelNumber) of a given voice call (vgwSessionId) who
		// got completed with the call as well as an indicator (isCustomerSpeechData) to 
		// denote whether the speech data we received on this channel belonged
		// to a caller or an agent.
		EndOfCallSignal_t = rstring vgwSessionId, 
			boolean isCustomerSpeechData, int32 vgwVoiceChannelNumber;

	graph
		// Ingest the speech data coming from the IBM Voice Gateway.
		// Such speech data arrives here in multiple fragments directly from
		// a live voice call. This operator is capable of receiving speech data
		// from multiple calls that can all happen at the very same time between
		// different pairs of speakers.
		// It is very important to note that the IBM Voice Gateway will keep
		// sending the speech data of the caller and the agent on two 
		// voice channels i.e. one for the caller and the other for the agent.
		// Irrespective of those two speakers talk or remain silent during the
		// call, their assigned voice channel will always carry some binary
		// data. That means, there is no way to know who is currently
		// talking. This constraint limits us from sending only one of the
		// channel's data to a single S2T engine at any given time.
		// Instead, this constraint forces us to dedicate a single S2T engine
		// per voice channel in a given voice call and keep sending the
		// data being received on that channel continuously to that
		// dedicated S2T engine irrespective of whether that channel carries
		// silence or active speech data. In summary, we will need two
		// S2T engines to do the Speech 2 Text for every ongoing voice call.
		// So, you have to plan ahead of time about the number of S2T engines
		// you will start for handling the maximum number of concurrent calls.
		// As an example, for handling a maximum of 100 concurrent voice calls,
		// you will have to start 200 S2T engines.
		//
		// In your own real-life applications, you may want to simply 
		// copy and reuse the code from this example and then make the 
		// changes only where it is really needed.
		// This example presents the following application design pattern:
		// IBMVoiceGatewaySource-->Speech Data Router-->S2T Engine->S2T Result Processor
		// You should be fine to simply use the entire pattern as it is except for
		// making changes in the S2T Result Processor composite and beyond to address
		// your own needs of further analytics on the S2T results as well as
		// specific ways of delivering the S2T results to other 
		// downstream systems rather than only writing to files as this example does below.
		(stream<BinarySpeech_t> BinarySpeechData as BSD;
		 stream<EndOfCallSignal_t> EndOfCallSignal as EOCS) as VoiceGatewayInferface = 
			IBMVoiceGatewaySource() {
			logic
				state: {
					// Initialize the default TLS certificate file name if the 
					// user didn't provide his or her own.
					rstring _certificateFileName = 
						($certificateFileName != "") ?
						$certificateFileName : getThisToolkitDir() + "/etc/ws-server.pem";
				}
				
			param
				tlsPort: $tlsPort;
				certificateFileName: _certificateFileName;
				nonTlsEndpointNeeded: $nonTlsEndpointNeeded;
				nonTlsPort: $nonTlsPort;
				// Initial delay before generating the very first tuple.
				// This is a one time delay when this operator starts up.
				// This delay should give sufficient time for the
				// WatsonS2T operator(s) to come up and be ready to
				// receive the speech data tuples sent by this operator.
				initDelay: $initDelayBeforeSendingDataToS2TEngines;
				vgwLiveMetricsUpdateNeeded: $vgwLiveMetricsUpdateNeeded;
				websocketLoggingNeeded: $vgwWebsocketLoggingNeeded;
				vgwSessionLoggingNeeded: $vgwSessionLoggingNeeded;
				vgwStaleSessionPurgeInterval: $vgwStaleSessionPurgeInterval;
				ipv6Available: $ipv6Available;
			
			// Get these values via custom output functions	provided by this operator.
			output
				BSD: vgwSessionId = getIBMVoiceGatewaySessionId(),
					isCustomerSpeechData = isCustomerSpeechData(),
					vgwVoiceChannelNumber = getVoiceChannelNumber(),
					callerPhoneNumber = getCallerPhoneNumber(),
					agentPhoneNumber = getAgentPhoneNumber(),
					speechDataFragmentCnt = getTupleCnt(),
					totalSpeechDataBytesReceived = getTotalSpeechDataBytesReceived();
		}

		// We have to always route the speech data bytes (fragments) coming from  
		// a given vgwSessionId_vgwVoiceChannelNumber to a particular 
		// WatsonS2T operator instance available within a parallel region. 
		// We already explained in detail in the previous operator's
		// commentary section about why it must be done this way.
		// This idea of pairing up a vgwSessionId_vgwVoiceChannelNumber combo
		// to a particular parallel region channel is a must for the 
		// speech data bytes of a given speaker in a voice call to always land in 
		// the same WatsonS2T engine. This stickiness (a.k.a channel affinity) is
		// important to continuously transcribe the speech data arriving on both the
		// voice channels at all the time including the silence time of a speaker.
		// This is needed because the IBM Voice Gateway keeps sending the 
		// speech data bytes of both the speakers (whether active or silent) at 
		// all the time on two voice channels by dedicating one channel to an
		// agent and the other channel to the caller. So, this requires 
		// extra logic to locate an unused parallel channel 
		// i.e. an idle S2T engine to be assigned for a 
		// given vgwSessionId_vgwVoiceChannelNumber.
		// That special logic happens inside this operator.
		(stream<BinarySpeech_t> BinarySpeechDataFragment as BSDF) as
			BinarySpeechDataRouter = Custom(BinarySpeechData as BSD;
			EndOfCallSignal as EOCS) {
			logic
				state: {
					// This map tells us which UDP channel is processing a 
					// given vgwSessionId_vgwVoiceChannelNumber combo.
					mutable map<rstring, int32> _vgwSessionIdToUdpChannelMap = {};
					// This list tells us which UDP channels are 
					// idle at any given time.
					mutable list<int32> _idleUdpChannelsList = 
						prepareIdleUdpChannelsList($numberOfS2TEngines);
					// This map tells us which UDP channel is going to process
					// the given voice call's (i.e. vgwSessionId) transcription
					// results in the S2TResultProcessor composite that appears
					// below in this SPL source file.
					mutable map<rstring, int32> _vgwSessionToResultProcessorChannelMap = {};
					mutable BinarySpeech_t _oTuple = {};
					mutable rstring _key = "";
				}
			
				// Process the Binary Speech Data.
				onTuple BSD: {
					// Get the sessionId + channelNumber combo string.
					_key = BSD.vgwSessionId + "_" + (rstring)BSD.vgwVoiceChannelNumber;
					
					// Check if this vgwSessionId_vgwVoiceChannelNumber combo already 
					// has an S2T engine allocated for it via an UDP channel.					
					if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
						// This is a speaker of an ongoing voice call who has 
						// already been assigned to an S2T engine.
						// Always send this speaker's speech data fragment to 
						// that same S2T engine.
						BSD.s2tEngineId = _vgwSessionIdToUdpChannelMap[_key];
						// We can always assume that there is a preselected 
						// S2T result processor UDP channel available for this 
						// voice call (i.e. vgwSessionId). Because, it is already 
						// done in the else block below when this voice call's 
						// first speaker's speech data arrives here.
						// Let us fetch and assign it here.
						if (has(_vgwSessionToResultProcessorChannelMap, 
							BSD.vgwSessionId) == true) {
							BSD.s2tResultProcessorId = 
								_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
						} else {
							// This should never happen since the call will end
							// for both the speakers almost at the same time after 
							// which there will be no speech data from any of the
							// speakers participating in a given voice call.
							// This else block is just part of defensive coding.
							appTrc(Trace.error, 
								"_XXXXX No S2T result processor engine available at this time for the " +
								"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
								". This should be a rare occurrence towards the very end of the call." + 
								" We are not going to process the speech data bytes" +
								" of this speaker in this voice call.");
							return;
						}
					} else {
						// If we are here, that means this is a brand new speaker of a
						// voice call for whom we must find an idle UDP channel a.k.a
						// an idle S2T engine that can process this speaker's speech data.
						int32 myS2TEngineId = getAnIdleUdpChannel(_idleUdpChannelsList);
						
						if (myS2TEngineId == -1) {
							// This is not good and we should never end up in this situation.
							// This means we have not provisioned sufficient number of S2T engines to
							// handle the maximum planned concurrent calls. We have to ignore this
							// speech data fragment and hope that an idle UDP channel number will
							// become available by the time the next speech data fragment for this
							// particular vgwSessionId_vgwVoiceChannelNumber combo arrives here. 
							if (BSD.speechDataFragmentCnt == 1) {
								// Display this alert only for the very first data fragment of a 
								// given speaker of a given voice call.
								appTrc(Trace.error, "No idle S2T engine available at this time for the " +
									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
									". There are " + (rstring)$numberOfS2TEngines +
									" S2T engines configured and they are all processing other" +
									" voice calls at this time. Please start sufficient number of S2T engines" +
									" next time to handle your maximum expected concurrent calls." +
									" A rule of thumb is to have two S2T engines to process" +
									" two speakers in every given concurrent voice call.");
							}

							return;	
						} else {
							// We got an idle S2T engine.
							BSD.s2tEngineId = myS2TEngineId;
							// Insert into the state map for future reference.
							insertM(_vgwSessionIdToUdpChannelMap, 
								_key, myS2TEngineId);
								
							// For this voice call (i.e. vgwSessionId), select a 
							// single result processor UDP channel. Both speakers in this 
							// same voice call will use that same result processor instance.
							// This will ensure that the S2T results for both the speakers 
							// will reach the same result processor.
							if (has(_vgwSessionToResultProcessorChannelMap, 
								BSD.vgwSessionId) == false) {
								insertM(_vgwSessionToResultProcessorChannelMap,
									BSD.vgwSessionId, myS2TEngineId);
							} 
							
							// Set the S2T result processor id.
							BSD.s2tResultProcessorId = 
								_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
						} // End of if (myS2TEngineId == -1)
					} // End of if (has(_vgwSessionIdToUdpChannelMap, _key)

					appTrc(Trace.debug, "vgwSessionId=" + BSD.vgwSessionId +
						", isCustomerSpeechData=" + (rstring)BSD.isCustomerSpeechData +
						", vgwVoiceChannelNumber=" + (rstring)BSD.vgwVoiceChannelNumber +
						", speechDataFragmentCnt=" + (rstring)BSD.speechDataFragmentCnt +
						", totalSpeechDataBytesReceived=" + 
						(rstring)BSD.totalSpeechDataBytesReceived +
						", s2tEngineId=" + (rstring)BSD.s2tEngineId +
						", s2tResultProcessorId=" + (rstring)BSD.s2tResultProcessorId); 
					// Submit this tuple.
					submit(BSD, BSDF);
				} // End of onTuple BSD
				
				// Process the end of voice call signal.
				// Since there are two channels in every voice call,
				// those two channels will carry their own "End S2T session"
				// message from the Voice Gateway. The logic below takes care of
				// handling two End of Call Signals for every voice call.
				onTuple EOCS: {
					// Get the allocated S2T engine id for a given 
					// vgwSessionId_vgwVoiceChannelNumber combo.
					// We should always have an S2T engine id. If not, that is a 
					// case where the user didn't provision sufficient number of 
					// S2T engines and there was no idle S2T engine available for that 
					// given vgwSessionId_vgwVoiceChannelNumber combo. 
					// This situation can be avoided by starting the application with a 
					// sufficient number of S2T engines needed for the anticipated 
					// maximum concurrent voice calls. A rule of thumb is to have 
					// two S2T engines to process two speakers in every given 
					// concurrent voice call.
					//
					// Get the sessionId + channelNumber combo string.
					_key = EOCS.vgwSessionId + "_" + (rstring)EOCS.vgwVoiceChannelNumber;
					
					if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
						// Let us send an empty blob to the WatsonS2T operator to indicate that
						// this speaker of a given voice call is done.
						_oTuple = (BinarySpeech_t){};
						// Copy the three input tuple attributes that must
						// match with that of the outgoing tuple.
						assignFrom(_oTuple, EOCS);
						// Assign the S2T engine id where this voice channel was
						// getting processed until now.
						_oTuple.s2tEngineId = _vgwSessionIdToUdpChannelMap[_key];
						submit(_oTuple, BSDF);
						// We are now done with this vgwSessionId_vgwVoiceChannelNumber combo.
						// So, we can release the S2T engine and add it to the idle UDP channels list.
						removeM(_vgwSessionIdToUdpChannelMap, _key);
						appendM(_idleUdpChannelsList, _oTuple.s2tEngineId);
					}
					
					// Since this voice call is ending, let us release the S2T result processor 
					// instance that was allocated above for this voice call.
					if (has(_vgwSessionToResultProcessorChannelMap, 
						EOCS.vgwSessionId) == true) {
						removeM(_vgwSessionToResultProcessorChannelMap, EOCS.vgwSessionId);
					}
				}
				
			config
				threadedPort: queue(BSD, Sys.Wait), queue(EOCS, Sys.Wait);
		} // End of Custom operator.

		// Invoke one or more instances of the IBMWatsonSpeech2Text composite operator.
		// You can send the audio data to this operator all at once or 
		// you can send the audio data for the live-use case as it becomes
		// available from your telephony network switch.
		// Avoid feeding audio data coming from more than one data source into this 
		// parallel region which may cause erroneous transcription results.
		//
		// A single input stream into this operator is the audio blob content.
		// Please note that we are using an int32 value (exactly in the
		// range of the available number of S2T engines) as our
		// UDP partition key. That will help us to always use that
		// preselected partition for a given voice call thereby 
		// avoiding cross talk and mix up of data from multiple 
		// voice calls landing in a given parallel channel etc.
		@parallel(width = $numberOfS2TEngines, 
		partitionBy=[{port=BSDF, attributes=[s2tEngineId]}])
		(stream<S2TResult_t> MyS2TResult; 
		 stream<S2TDiagnosticsResult_t> Diagnostics) as Speech2Text = 
			IBMWatsonSpeech2Text(BinarySpeechDataFragment as BSDF) {
			// You must not fuse the WatsonS2T operator instances since the
			// WatsonS2T engine's library code needs to run on its own Linux process.
			// So, you must keep each instance of this operator on its own PE (a.k.a Linux process).
			config
				placement : partitionExlocation("s2tpartition");
		}

		// Let us invoke the same number of S2T result processors as 
		// there are S2T engines.
		// Please note that we are using an int32 value (exactly in the
		// range of the available number of S2T engines) as our
		// UDP partition key. That will help us to always use that
		// preselected partition for a given voice call thereby 
		// avoiding cross talk and mix up of data from multiple 
		// voice calls landing in a given parallel channel etc.
		@parallel(width = $numberOfS2TEngines, 
		partitionBy=[{port=MSR, attributes=[s2tResultProcessorId]}])
		() as S2TResultProcessorSink = S2TResultProcessor(MyS2TResult as MSR) {
		}

		// =================================================================
		// This code block is here purely for debugging purposes to 
		// capture the raw audio data received in a given voice channel 
		// for a given VGW session id. You have to remove the block comment
		// below to activate this section of the code when you need it.
		// The following block of code is good only for testing it with
		// a single voice call. If you use the following section of code
		// for multiple voice calls, then the speech data bytes from
		// multiple calls will get mixed up. So, use it to debug the
		// speech data contents for just a single voice call.
		// In a real-life application, you will not have a need for
		// this section of the code.
		/*
		(stream<BinarySpeech_t> BinarySpeechDataFragmentInVoiceChannel1;
		 stream<BinarySpeech_t> BinarySpeechDataFragmentInVoiceChannel2)
			as BinarySpeechDataFilterByVoiceChannel = 
			Split(BinarySpeechDataFragment as BSDF) {
			param
				// We will only have either channel number 1 or 2.
				// So, send the speech data received via channel number 1 to 
				// output port index 0 (i.e. first port).
				// Send the speech data received via channel number 2 to 
				// output port index 1 (i.e. second port).
				index: (BSDF.vgwVoiceChannelNumber == 1) ? 0 : 1;
		}
		
		// Send only the blob part of the incoming tuple from voice channel 1.
		(stream<blob speech> SpeechDataInVoiceChannel1 as SD1)
			as SpeechDataFromVoiceChannel1 = 
			Functor(BinarySpeechDataFragmentInVoiceChannel1) {	
		}

		// Send only the blob part of the incoming tuple from voice channel 2.
		(stream<blob speech> SpeechDataInVoiceChannel2 as SD2)
			as SpeechDataFromVoiceChannel2 = 
			Functor(BinarySpeechDataFragmentInVoiceChannel2) {	
		}
		
		// Write the speech data bytes received on voice channel 1 to its own binary file.
		() as VoiceChannelSink1 = FileSink(SpeechDataInVoiceChannel1 as SD) {
			param
				// You can use this command to convert this 
				// mulaw formatted audio file into a WAV file in order to
				// play it using Audacity, QuickTime Player etc.:
				file: "voice-channel1-speech-data.bin";
				format: block;
				flush: 1u;

			config
				threadedPort: queue(SD, Sys.Wait);
		}

		// Write the speech data bytes received on voice channel 2 to its own binary file.
		() as VoiceChannelSink2 = FileSink(SpeechDataInVoiceChannel2 as SD) {
			param
				// You can use this command to convert this 
				// mulaw formatted audio file into a WAV file in order to
				// play it using Audacity, QuickTime Player etc.:
				// ffmpeg -f mulaw -ar 8000 -i <raw data> -codec:a pcm_mulaw <wav-filename>
				file: "voice-channel2-speech-data.bin";
				format: block;
				flush: 1u;

			config
				threadedPort: queue(SD, Sys.Wait);
		}
		*/
		// =================================================================
	config restartable : false;
} // End of the main composite.

// Following is a composite where we are going to perform the
// logic to invoke the WatsonS2T operator for doing the
// Speech 2 Text transcription.
public composite IBMWatsonSpeech2Text(input AudioBlobContent; output S2TResult, S2TDiagnostics) {
	param
		expression<float64> $samplingRate : (float64)getSubmissionTimeValue("samplingRate", "8000.0");
		expression<rstring> $watsonS2TConfigFile :
			getSubmissionTimeValue("WatsonS2TConfigFile",
			"/homes/hny5/sen/s2t_standalone/toolkit.speech2text-v2.12.0/model/en_US.8kHz.general.diarization.low_latency.pset") ;
		expression<rstring> $watsonS2TModelFile :
			getSubmissionTimeValue("WatsonS2TModelFile",
			"/homes/hny5/sen/s2t_standalone/toolkit.speech2text-v2.12.0/model/en_US.8kHz.general.pkg") ;
		expression<rstring> $watsonGrammarPatchFile1 :
			getSubmissionTimeValue("watsonGrammarPatchFile1", "") ;
		expression<float32> $watsonGrammarPatchWeights1 :(float32)
			getSubmissionTimeValue("watsonGrammarPatchWeights1", "0.7") ;
		expression<rstring> $watsonAMPatchFile :
			getSubmissionTimeValue("watsonAMPatchFile", "");
		expression<boolean> $outputNBestUtterances :(boolean)
			getSubmissionTimeValue("OutputNBestUtterances", "false") ;

	graph
		// This Custom operator receives the tuple sent by the Voice Gateway Source operator and
		// takes a copy of its blob speech attribute into the blob payload attribute.
		// This is needed because this application uses the RtpDecode operator below for
		// transforming the single byte mulaw formatted speech data into the 
		// double byte i.e. 16 bit mulaw formatted speech data. Only then, the WatsonS2T operator
		// can successfully transcribe that 16 bit raw PCM data. RtpDecode expects the 
		// single byte mulaw formatted data to be inside an attribute called "blob payload".
		// Hence, we must do this unavoidable memory copy that will result in some performance overhead.
		// RtpDecode also expects the length of the payload to be sent via another "int32 payloadLength" attribute.
		// We will do that memory copy based data transformation here now.
		//
		// Please note below in this operator and in the RtpDecode operator that we are using the
		// incoming stream name in place of the output schema name. It is perfectly fine and it is
		// accepted by the SPL compiler.
		(stream<AudioBlobContent> AudioBlobWithPayloadAttribute as ABWPA) = Custom(AudioBlobContent as ABC) {
			logic
				onTuple ABC: {
					// Since the incoming and outgoing stream schema are exactly the same,
					// we can directly modify the incoming tuple and send it out.
					// Take a copy of the "blob speech" into the "blob payload" as needed by the 
					// downstream RtpDecode operator.
					ABC.payload = ABC.speech;
					ABC.payloadLength = (int32)blobSize(ABC.speech);
					
					// If the payload size is zero, that means the voice call ended on a
					// vgwSessionId_voiceChannelNumber combo. So, this particular condition
					// must be informed to the WatsonS2T operator to reset its internal
					// buffers and complete the final utterance. Please read the comment
					// below in the WatsonS2T operator for its resetOnIdChange parameter.
					if (ABC.payloadLength <= 0) {
						// The logic in this statement is something that is needed only
						// for this application because of the way WatsonS2T handles its
						// end of speech cleanup and flushing of its final utterances.
						ABC.id = "MyDummyVoiceCall";
					} else {
						// This is an ongoing active call. So, we will set the
						// id attribute to the unique identifier of this blob fragment's origin.
						ABC.id = ABC.vgwSessionId + "_" + (rstring)ABC.vgwVoiceChannelNumber;
					}
					
					// Send this tuple away.
					submit(ABC, ABWPA);
				}				
		}
		
		// This operator converts the blob payload in 8 bit Mulaw format to 
		// raw 16 bit Mulaw/PCM format which is acceptable by the WatsonS2T operator.
		(stream<AudioBlobWithPayloadAttribute> AudioBlob as AB) = 
		 RtpDecode(AudioBlobWithPayloadAttribute) {
			output
				AB : speech = getPcmPayload(); 
		}
		
		// This is the embedded Watson S2T engine present in the 
		// IBM Streams speech2text toolkit C++ operator. Let us invoke it now.
		(stream<S2TResult_t> S2TResult; 
		 stream<S2TDiagnosticsResult_t> S2TDiagnostics) = WatsonS2T(AudioBlob as AB) {
			logic
				state: {
					mutable int32 _conversationCnt = 0;
					mutable rstring _conversationId = "";
				}
				
				onTuple AB: {
					if (_conversationId != AB.id) {
						// There may be many blob fragments arriving for a given audio conversation.
						// So, display only when the very first blob fragment for a given audio arrives.
						_conversationId = AB.id;
					
						// Ignore the MyDummyVoiceCall id which is to be ignored.
						// See the commentary at the very end of the
						// Custom operator above that copies and sends the speech blob fragment.
						if (_conversationId != "MyDummyVoiceCall") {
							appTrc(Trace.error, "Channel " + (rstring)getChannel() + 
								", Speech input " + (rstring)++_conversationCnt +
								": " + (rstring)_conversationId);
						}
					}
				}

			param
				samplingRate : $samplingRate;
				watsonConfigFile : $watsonS2TConfigFile ;
				watsonModelFile : $watsonS2TModelFile ;
				// If you have no LM and/or AM patch files. it is okay to 
				// set the following patch file parameters to an empty string.
				watsonGrammarPatchFiles : $watsonGrammarPatchFile1;
				watsonGrammarPatchWeights: $watsonGrammarPatchWeights1;
				watsonAMPatchFile: $watsonAMPatchFile;
				
				//
				// If we set this to false, then it will process every incoming 
				// partial blob fragment which may not have enough data to 
				// result in a meaningful text. You are better off setting it to 
				// false when you are feeding the actual file name as the 
				// input to WatsonS2T via 'rstring speech' attribute instead of the
				// speech blob fragments via 'blob speech' attribute.
				// If we set this to true, then it will set the transcription completed flag to
				// true only on the next incoming unique file i.e. after the very last
				// blob fragment for the current unique id (file name or call id)  at 
				// which point the id attribute will change to a new file name or call id.
				// See the commentary at the very end of the
				// Custom operator above that reads the binary file and
				// sends out the blob fragments.
				resetOnIdChange : true;
				punctuationOnReset : true ;
				
			output
				S2TResult : utteranceText = getUtteranceText(),
					utteranceStartTime = getUtteranceStartTime(),
					utteranceEndTime = getUtteranceEndTime(),
					utteranceNumber = getUtteranceNumber(),
					utteranceConfidence = getUtteranceConfidence(),
					utteranceWords = getUtteranceWords(),
					utteranceSpeakers = getUtteranceSpeakers(),
					utteranceWordConfidences = getUtteranceWordConfidences(),
					nBestHypotheses = getNBestHypotheses(),
					wordAlternatives = getConsensus(),
					wordAlternativesConfidences = getConsensusConfidences(),
					wordAlternativesStartTimes = getConsensusStartTimes();
					
				S2TDiagnostics : log = getDiagnosticsMessage() ;
		}
} // End of the composite IBMWatsonSpeech2Text

// Following is a sink composite where we are going to process the
// S2T result of a given voice call in specific ways such as
// storing in files, message queues, databases or make it 
// accessible from a web application.
public composite S2TResultProcessor(input MyTranscriptionResult) {
	param
		// This submission time value decides whether to write the
		// transcription results to files or not.
		expression<boolean> $writeTranscriptionResultsToFiles : (boolean)
			getSubmissionTimeValue("writeTranscriptionResultsToFiles", "true");

		// This submission time value decides whether to send the full 
		// transcription results to an HTTP endpoint or not.
		// CAUTION: Don't enable this option if you have a large number of
		// maximum concurrent voice calls. In such scenarios, sending the
		// live transcription results via HTTP may not scale well.
		expression<boolean> $sendTranscriptionResultsToHttpEndpoint : (boolean)
			getSubmissionTimeValue("sendTranscriptionResultsToHttpEndpoint", "false");
			
		// This submission time value allows the user to specify the
		// HTTP endpoint to where the transcription results must be sent.
		expression<rstring> $httpEndpointForSendingTranscriptionResults : 
			getSubmissionTimeValue("httpEndpointForSendingTranscriptionResults", 
				"http://www.MyTranscriptionResults.com");
		
		// This submission time value indicates a file name where the HTTP responses will be logged.	
		expression<rstring> $httpResponseFile : 
			getSubmissionTimeValue("httpResponseFile", "/dev/null");
			
	graph
		// In a real-life application, there will be additional operators here with the 
		// necessary logic to look inside the tuples arriving on the S2TResult stream and
		// analyze different kinds of speech to text result attributes returned from the S2T engine.
		// 
		// But, in this simple example we will only collect the results 
		// arriving from the WatsonS2T operator and write along with
		// all the S2T related attributes to individual files.
		// As mentioned in several code blocks above, you can simply reuse all
		// the code provided in this example file as it is except for this composite/
		// You can feel free to make any code changes to perform 
		// specific analytics on the S2T results as well as make changes
		// to store the S2T results elsewhere instead of files or 
		// send the S2T results to other downstream systems such as your
		// web dashboarding applications.
		(stream<MyTranscriptionResult> TranscriptionResultForWritingToFile as TRFWTF;
		 stream<MyTranscriptionResult> TranscriptionResultForSendingToHttp as TRFSTH) = 
			Custom(MyTranscriptionResult as MTR) {
			logic
				onTuple MTR: {
					// We will write the transcription results to 
					// files if the user wanted it that way.
					if ($writeTranscriptionResultsToFiles == true) {
						submit(MTR, TRFWTF);
					} 
					
					// We will send the transcription results to an
					// HTTP endpoint if the user wanted it that way.
					if ($sendTranscriptionResultsToHttpEndpoint == true &&
						$httpEndpointForSendingTranscriptionResults != "") {
						// We need a valid URL in order to send the results there.
						submit(MTR, TRFSTH);
					}
				}
				onPunct MTR: {
					if ($writeTranscriptionResultsToFiles == true) {
						submit(currentPunct(), TRFWTF);
					}
				}
		}
		
		() as MySink1 = FileSink(TranscriptionResultForWritingToFile as TRFWTF) {
			param
				// This file will contain a comprehensive set of full S2T results.
				file: TRFWTF.vgwSessionId + "-full-result.txt";
				closeMode: dynamic;
				quoteStrings: false;
				append: true;
				flush: 1u;
				writePunctuations: true;

			config
				threadedPort: queue(TRFWTF, Sys.Wait);
		}
		
		// In this operator, we will filter only the utterances and send it 
		// to the downstream operator to be written to the individual files.
		(stream<rstring vgwSessionId, 
		 boolean isCustomerSpeechData,
		 int32 vgwVoiceChannelNumber, 
		 rstring utteranceText> Utterance as U) 
			as UtteranceFilter = Custom(TranscriptionResultForWritingToFile as TRFWTF) {
			logic
				state: {
					mutable Utterance _oTuple = {};
				}
					
				onTuple TRFWTF: {
						assignFrom(_oTuple, TRFWTF);
						submit(_oTuple, U);
				}
				
			config
				threadedPort: queue(TRFWTF, Sys.Wait);
		}

		// Write only the utterances to a file.
		() as MySink2 = FileSink(Utterance as U) {
			param
				// This file will contain only a small subset of the
				// S2T results (Unique call id, channel number, caller or agent, utterance).
				file: U.vgwSessionId + "-utterance-result.txt";
				closeMode: dynamic;
				quoteStrings: false;
				append: true;
				flush: 1u;

			config
				threadedPort: queue(U, Sys.Wait);
		}
		
		// CAUTION: This logic of converting to JSON and then 
		// pushing/streaming it via HTTP REST call to an http endpoint is 
		// fine for a low maximum number of concurrent voice calls (e-g: 100 calls).
		// For a large maximum number of concurrent calls, this approach
		// of sending the transcription results to an HTTP endpoint may not
		// scale very well.
		//
		// Let us now convert the transcription result tuple into JSON.
		(stream<Json> TranscriptionResultInJson) as ResultToJson = 
			TupleToJSON(TranscriptionResultForSendingToHttp as TRFSTH) {
			config
				threadedPort: queue(TRFSTH, Sys.Wait);
		}
		
		// Send the transcription result in JSON to the HTTP endpoint.
		(stream<HTTPResponse> HttpResponse) as TranscriptionResultHttpSender = 
			HTTPPost(TranscriptionResultInJson as TRIJ) {
			param
				url: $httpEndpointForSendingTranscriptionResults;
				headerContentType: "application/json";
				maxRetries: 3;
				
			config
				threadedPort: queue(TRIJ, Sys.Wait);
		}
		
		() as MySink3 = FileSink(HttpResponse as HR) {
			param
				// This file will contain the HTTP response from 
				// sending the transcription results to an HTTP endpoint.
				file: $httpResponseFile;
				flush: 1u;

			config
				threadedPort: queue(HR, Sys.Wait);
		}
} // End of the composite S2TResultProcessor

// Following are the common utility functions for 
// use within the composites declared above.
//
// This function creates a new list with all the UDP channel numbers in it. 
// That means, all such channels are idle at this time. 
public list<int32> prepareIdleUdpChannelsList(int32 s2tEngineCount) {
	mutable list<int32> myList = [];
	
	for(int32 idx in range(s2tEngineCount)) {
		appendM(myList, idx);
	}
	
	return(myList);
}

// This function takes the idle UDP channels list as an input and
// returns an idle UDP channel number from the top of the list.
// If all are busy at this time, it will return -1.
public int32 getAnIdleUdpChannel(mutable list<int32> myList) {
	if (size(myList) > 0) {
		// Get the channel number available at the very top of the list.
		int32 channelNumber = myList[0];
		// Remove the topmost channel number.
		removeM(myList, 0);
		return(channelNumber);
	} else {
		// There is no idle UDP channel number available at this time.
		return(-1);
	}
}