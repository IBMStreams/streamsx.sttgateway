/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2019
==============================================
*/

/*
==============================================
First created on: Jul/01/2018

This example demonstrates transcription of audio files into text using the WatsonSTT operator. 
This example code is suitable for processing audio data that is already saved into 
a set of audio files (WAV, MP3 etc.) and made available within a specific directory.

This example shows how to pick up the audio files available in a given directory and
simply send the name of the file to the WatsonSTT operator to get the transcription results.
If you are looking to receive the audio data directly from the voice network switches via TCP/UDP (OR)
read the binary audio content from the WAV files on your own and send the
audio blob data (either in full or in partial blob fragments) to the WatsonSTT operator,
then please refer to a different example project named AudioRawWatsonSTT.

WAV files can be in uncompressed PCM, 16-bit little endian, 
8 kHz (narrowband) or 16 KHz (broadband) sampling rate, mono format. 

To either downsample or upsample a WAV file, you can use the following ffmpeg command: 
    ffmpeg -i MyFile.wav -ac 1 -ar 8000 MyNewFile.wav

You can build this example from command line via the make command by using the
Makefile available in the top-level directory of this example. It will be
necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by
pointing it to the full path of your 
streamsx.sttgateway/com.ibm.streamsx.sttgateway directory.

If you want to build this example inside the Streams Studio, there are certain
build configuration settings needed. Please refer to the streamsx.sttgateway
toolkit documentation to learn more about those Streams Studio configuration settings.
==============================================
*/
namespace com.ibm.streamsx.sttgateway.sample.watsonstt;

use com.ibm.streamsx.sttgateway.watson::*;

public composite AudioFileWatsonSTT {
	param
		// IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the 
		// Watson STT cloud service. For the STT service on IBM Public Cloud, 
		// one must use the unexpired IAM access token (generated by using your 
		// IBM Public cloud STT service instance's API key). 
		// So, user must provide here his/her API key. We have some logic below that 
		// will use the user provided API key to generate the IAM access token and 
		// send that to the WatsonSTT operator.
		// There is additional logic available below to keep refreshing that
		// IAM access token periodically in order for it to stay unexpired.
		// You should leave this submission time value empty when not using STT on IBM public cloud.
		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
		expression<rstring> $sttApiKey : getSubmissionTimeValue("sttApiKey", "");
		// Specify either the public cloud IAM Token fetch/refresh URL.
		expression<rstring> $sttIAMTokenURL : 
			getSubmissionTimeValue("sttIAMTokenURL", "https://iam.cloud.ibm.com/identity/token");
		// Specify the IBM STT on Cloud Pak for Data (CP4D i.e. private cloud) access token.
		// You should leave this submission time value empty when not using STT on CP4D.
		expression<rstring> $sttOnCP4DAccessToken : getSubmissionTimeValue("sttOnCP4DAccessToken", "");
		expression<rstring> $audioDir : getSubmissionTimeValue("audioDir", "../../audio-files"); 
		expression<int32> $numberOfSTTEngines :(int32)
			getSubmissionTimeValue("numberOfSTTEngines", "5") ;
		expression<rstring> $sttUri : getSubmissionTimeValue("sttUri",
			"wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize");
		expression<rstring> $sttBaseLanguageModel : 
			getSubmissionTimeValue("sttBaseLanguageModel", "en-US_NarrowbandModel");
		expression<rstring> $contentType : 
			getSubmissionTimeValue("contentType", "audio/wav");
		expression<rstring> $baseModelVersion : 
			getSubmissionTimeValue("baseModelVersion", "");
		expression<rstring> $customizationId : 
			getSubmissionTimeValue("customizationId", "");
		expression<rstring> $acousticCustomizationId : 
			getSubmissionTimeValue("acousticCustomizationId", "");
		expression<float64> $customizationWeight : 
			(float64)getSubmissionTimeValue("customizationWeight", "0.30");
		expression<int32> $sttBatchSize : (int32)getSubmissionTimeValue("sttBatchSize", "0");
		expression<int32> $sttResultMode : (int32)getSubmissionTimeValue("sttResultMode", "3");
		expression<int32> $maxUtteranceAlternatives : 
			(int32)getSubmissionTimeValue("maxUtteranceAlternatives", "1");	
		expression<boolean> $sttRequestLogging : 
			(boolean)getSubmissionTimeValue("sttRequestLogging", "false");
		expression<boolean> $filterProfanity : 
			(boolean)getSubmissionTimeValue("filterProfanity", "false");
		expression<boolean> $sttJsonResponseDebugging : 
			(boolean)getSubmissionTimeValue("sttJsonResponseDebugging", "false");
		expression<float64> $wordAlternativesThreshold : 
			(float64)getSubmissionTimeValue("wordAlternativesThreshold", "0.0");
		expression<boolean> $smartFormattingNeeded : 
			(boolean)getSubmissionTimeValue("smartFormattingNeeded", "false");
		expression<float64> $keywordsSpottingThreshold : 
			(float64)getSubmissionTimeValue("keywordsSpottingThreshold", "0.0");
		expression<list<rstring>> $keywordsToBeSpotted : 
			(list<rstring>)getSubmissionTimeValue("keywordsToBeSpotted", "[]");	
		expression<boolean> $websocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("websocketLoggingNeeded", "false");
		expression<float64> $cpuYieldTimeInAudioSenderThread : 
			(float64)getSubmissionTimeValue("cpuYieldTimeInAudioSenderThread", "0.001");
		expression<boolean> $sttLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("sttLiveMetricsUpdateNeeded", "true");

	type
		// This STT result type contains many attributes to
		// demonstrate all the basic and very advanced features of 
		// the Watson STT service. Not all real life applications will need 
		// all these attributes. You can decide to include or omit these
		// attributes based on the specific STT features your application will need. 
		// Trimming the unused attributes will also help in 
		// reducing the STT processing overhead and in turn 
		// help in receiving the STT results faster.
		// Read the streamsx.sttgateway toolkit documentation to learn about
		// what features are available, how they work and how different attributes are 
		// related to those features.
		STTResult_t = rstring conversationId, int32 utteranceNumber,
			rstring utteranceText, boolean finalizedUtterance,
			float32 confidence, rstring fullTranscriptionText,
			rstring sttErrorMessage,
			list<rstring> utteranceAlternatives, 
			list<list<rstring>> wordAlternatives,
			list<list<float64>> wordAlternativesConfidences,
			list<float64> wordAlternativesStartTimes,
			list<float64> wordAlternativesEndTimes,
			list<rstring> utteranceWords,
			list<float64> utteranceWordsConfidences,
			list<float64> utteranceWordsStartTimes,
			list<float64> utteranceWordsEndTimes,
			float64 utteranceStartTime,
			float64 utteranceEndTime,
			list<int32> utteranceWordsSpeakers,
			list<float64> utteranceWordsSpeakersConfidences,
			map<rstring, list<map<rstring, float64>>> keywordsSpottingResults,
			boolean transcriptionCompleted,
			int32 sequence;

	graph
		// IMPORTANT: IBM STT service on public cloud requires
		// an unexpired valid IAM access token to perform the 
		// speech to text task in a secure manner. You may either 
		// provide the token in parameter sttOnCP4DAccessToken, or provide 
		// the parameter sttIAMTokenURL and sttApiKey and the operator IAMAccessTokenGenerator
		// generates a new access token and then periodically refresh it. 
		// Output stream of this composite operator is connected to the
		// second input stream of the WatsonSTT operator that is used below.
		// For a correct STT operation, user must set only one of these two
		// submission time parameters to a non-empty value: sttAPIKey or sttOnCP4DAccessToken.
		(stream<IAMAccessToken> IamAccessToken as IAT)
			as IamAccessTokenGenerator = IAMAccessTokenGenerator() {
			param
				// This operator takes these four parameters.
				apiKey: $sttApiKey;
				iamTokenURL: $sttIAMTokenURL;
				accessToken: $sttOnCP4DAccessToken;
				// All connection parameter are taken from params.
				// So if we clean the app config name, we avoid error logs
				appConfigName: "";
		}

		// Scan a directory periodically to pick up the audio files and 
		// send the name of that audio file to the WatsonSTT operator.
		//
		// The output stream from this operator will be connected to the
		// WatsonSTT operator's input stream. That operator expects one of its
		// input stream attributes to be named as 'speech' with an SPL type of
		// either rstring or blob. That operator can take as input an 
		// audio filename (rstring) or raw binary audio data (blob).
		// In this example, we have to simply send the name of the audio file to the
		// WatsonSTT operator which will read the binary audio data on its own from that file.
		// In addition to that mandatory input stream attribute, additional
		// attributes can be sent in the input stream to be auto assigned to
		// the output stream of the WatsonSTT operator.
		stream<rstring speech, int32 sequence> AudioFileName as O = DirectoryScan() {
			logic state:
				mutable int32 counter = 0;
			param
				directory : $audioDir;
				pattern : "\\.wav$";
				// Give sufficient delay here so that the
				// previous operator can complete generating the
				// IAM access token and send it to the WatsonSTT operator.
				initDelay : 15.0;
				sortBy: name;
			output O:
				sequence = counter++;
			config
				placement: partitionIsolation;
		}
		
		// The WatsonSTT Operator required that each conversation end is flagged with
		// Window Punctuation.
		stream<I> AudioFileNameFlagged as O = Punctor(AudioFileName as I) {
			param
				punctuate: true;
				position: after;
		}
		
		// Invoke one or more instances of the WatsonSTT operator.
		// Avoid feeding audio data coming from more than one data source into this 
		// parallel region which may cause erroneous transcription results.
		// NOTE: The WatsonSTT operator allows fusing multiple instances of
		// this operator into a single PE. This will help in reducing the 
		// total number of CPU cores used in running the application.
		// First input stream into this operator is the fully qualified audio file name.
		// Second input stream into this operator is your STT service instance's IAM access token.
		@parallel(width = $numberOfSTTEngines, broadcast=[AT])
		stream<STTResult_t> STTResult = 
			WatsonSTT(AudioFileNameFlagged as AFN; IamAccessToken as AT) {
			logic
				state: {
					mutable int32 _conversationCnt = 0;
				}
				
				onTuple AFN: {
					printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " Channel " + (rstring)getChannel() + 
						", Speech input " + (rstring)++_conversationCnt +
						": " + (rstring)AFN.speech);
				}
			
			// Just to demonstrate, we are using all the operator parameters below.
			// Except for the first three parameters, every other parameter is an
			// optional one. In real-life applications, such optional parameters
			// can be omitted unless you want to change the default behavior of them.
			param
				uri: $sttUri;
				baseLanguageModel: $sttBaseLanguageModel;
				contentType: $contentType;
				sttResultMode: $sttResultMode;
				sttRequestLogging: $sttRequestLogging;
				filterProfanity: $filterProfanity;
				sttJsonResponseDebugging: $sttJsonResponseDebugging;
				maxUtteranceAlternatives: $maxUtteranceAlternatives;
				wordAlternativesThreshold: $wordAlternativesThreshold;
				smartFormattingNeeded: $smartFormattingNeeded;
				keywordsSpottingThreshold: $keywordsSpottingThreshold;
				keywordsToBeSpotted: $keywordsToBeSpotted;
				websocketLoggingNeeded: $websocketLoggingNeeded;
				cpuYieldTimeInAudioSenderThread: $cpuYieldTimeInAudioSenderThread;
				sttLiveMetricsUpdateNeeded : $sttLiveMetricsUpdateNeeded;
								
				// Use the following operator parameters as needed.
				// Point to a specific version of the base model if needed.
				//
				// e-g: "en-US_NarrowbandModel.v07-06082016.06202016"
				baseModelVersion: $baseModelVersion;
				// Language model customization id to be used for the transcription.
				// e-g: "74f4807e-b5ff-4866-824e-6bba1a84fe96"
				customizationId: $customizationId;
				// Acoustic model customization id to be used for the transcription.
				// e-g: "259c622d-82a4-8142-79ca-9cab3771ef31"
				acousticCustomizationId: $acousticCustomizationId;
				// Relative weight to be given to the words in the custom Language model.
				customizationWeight: $customizationWeight;
			
			// Just for demonstrative purposes, we are showing below the output attribute
			// assignments using all the available custom output functions. In your
			// real-life applications, it is sufficient to do the assignments via
			// custom output functions only as needed.
			//
			// Some of the important output functions that must be used to check
			// the result of the transcription are:
			// getSTTErrorMessage --> It tells whether the transcription succeeded or not.
			// isFinalizedUtterance --> In sttResultMode 1, it tells whether this is a 
			//                          partial utterance or a finalized utterance.
			// isTranscriptionCompleted --> It tells whether the transcription is 
			//                              completed for the current audio conversation or not.
			//
			output
				STTResult: conversationId = speech, 
					utteranceNumber = getUtteranceNumber(),
					utteranceText = getUtteranceText(),
					finalizedUtterance = isFinalizedUtterance(),
					confidence = getConfidence(),
					fullTranscriptionText = getFullTranscriptionText(),
					sttErrorMessage = getSTTErrorMessage(),
					transcriptionCompleted = isTranscriptionCompleted(),
					// n-best utterance alternative hypotheses.
					utteranceAlternatives = getUtteranceAlternatives(),
					// Confusion networks (a.k.a. Consensus)
					wordAlternatives = getWordAlternatives(),
					wordAlternativesConfidences = getWordAlternativesConfidences(),
					wordAlternativesStartTimes = getWordAlternativesStartTimes(),
					wordAlternativesEndTimes = getWordAlternativesEndTimes(),
					utteranceWords = getUtteranceWords(),
					utteranceWordsConfidences = getUtteranceWordsConfidences(),
					utteranceWordsStartTimes = getUtteranceWordsStartTimes(),
					utteranceWordsEndTimes = getUtteranceWordsEndTimes(),
					utteranceStartTime = getUtteranceStartTime(),
					utteranceEndTime = getUtteranceEndTime(),
					// Speaker label a.k.a. Speaker id
					utteranceWordsSpeakers = getUtteranceWordsSpeakers(),
					utteranceWordsSpeakersConfidences = getUtteranceWordsSpeakersConfidences(),
					// Results from keywords spotting (matching) in an utterance.
					keywordsSpottingResults = getKeywordsSpottingResults();

			// If needed, you can decide not to fuse the WatsonSTT operator instances and
			// keep each instance of this operator on its own PE (a.k.a Linux process) by
			// removing the block comment around this config clause.
			/*
			config
				placement : partitionExlocation("sttpartition");
			*/
		}
		
		// In a real-life application, there will be additional operators here with the 
		// necessary logic to look inside the tuples arriving on the STTResult stream and
		// analyze different kinds of speech to text result attributes returned from the STT service.
		// 
		// But, in this simple example we will only collect the results arriving from the 
		// WatsonSTT operator and display them on stdout.
		() as MySink1 = Custom(STTResult as SR) {
			logic
				state: {
					mutable int32 _conversationCnt = 0;
				}
				
				onTuple SR: {
					// If the user gave us a non-zero batch size, we will print a single line
					// about the batch completion at the very end. In that case, we will 
					// avoid printing every individual transcription result. This will not
					// make the file system very busy in the Distributed Mode execution.
					// This is only useful if someone is interested in doing a 
					// timing measurement to see how long it takes to transcribe a 
					// known batch of audio conversations with the sttResultMode operator
					// parameter to set to a value of 3 (get the full text after the
					// entire conversation is transcribed).
					if ($sttBatchSize > 0) {
						++_conversationCnt;
						
						if (_conversationCnt == 1) {
							printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " STT result for conversation " + (rstring) _conversationCnt
								+ " " + conversationId + " arrived.");
						} else if (_conversationCnt == $sttBatchSize) {
							printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " STT result for conversation " + (rstring) _conversationCnt
								+ " " + conversationId + " arrived.");
						}
					} else {
						// User didn't give a non-zero batch size. In that case,
						// we will print every transcription result, which will make the
						// file system busy in the Distributed Mode execution.
						printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " " + (rstring)++_conversationCnt + 
							") STT result: " + (rstring)SR);
					}
				}
				
			config
				//Place this operator into an own PE to separate the final result printouts from all other printouts
				placement: partitionIsolation;
				threadedPort: queue(SR, Sys.Wait);
		}

	config restartable: false;
} // End of composite AudioFileWatsonSTT (Main composite)
