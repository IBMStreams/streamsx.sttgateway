/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2019
==============================================
First created on: Jul/01/2018
==============================================
*/
namespace com.ibm.streamsx.sttgateway.sample.watsonstt;

use com.ibm.streamsx.sttgateway.watson::*;
/**
 * This example demonstrates transcription of audio files into text using the WatsonSTT operator. 
 * This example code is suitable for processing audio data that is already saved into 
 * a set of audio files (WAV, MP3 etc.) and made available within a specific directory.
 * 
 * This example shows how to pick up the audio files available in a given directory and
 * simply send the name of the file to the WatsonSTT operator to get the transcription results.
 * If you are looking to receive the audio data directly from the voice network switches via TCP/UDP (OR)
 * read the binary audio content from the WAV files on your own and send the
 * audio blob data (either in full or in partial blob fragments) to the WatsonSTT operator,
 * then please refer to a different example project named AudioRawWatsonSTT.
 * 
 * WAV files can be in uncompressed PCM, 16-bit little endian, 
 * 8 kHz (narrowband) or 16 KHz (broadband) sampling rate, mono format. 
 * 
 * To either downsample or upsample a WAV file, you can use the following ffmpeg command: 
 *     ffmpeg -i MyFile.wav -ac 1 -ar 8000 MyNewFile.wav
 * 
 * You can build this example from command line via the make command by using the
 * Makefile available in the top-level directory of this example. It will be
 * necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by
 * pointing it to the full path of your 
 * streamsx.sttgateway/com.ibm.streamsx.sttgateway directory.
 * 
 * If you want to build this example inside the Streams Studio, there are certain
 * build configuration settings needed. Please refer to the streamsx.sttgateway
 * toolkit documentation to learn more about those Streams Studio configuration settings.
 * 
 * 
 * IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the 
 * Watson STT cloud service. For the STT service on IBM Public Cloud, 
 * one must use the unexpired IAM access token (generated by using your 
 * IBM Public cloud STT service instance's API key). 
 * So, user must provide here his/her API key. 
 * The operator IAMAccessTokenGenerator will use the user provided API key to generate the IAM access token and 
 * send that to the WatsonSTT operator. The operator IAMAccessTokenGenerator will keep refreshing that 
 * IAM access token periodically in order for it to stay unexpired. 
 * Alternatively you can provide the access token directly. In this case the access token is send directly to Watson STT 
 * cloud service and no refreshing logic is invoked.
 * See:
 * https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
 * 
 * The sample is designed to receive all connection related parameters from *application configuration* with name 
 * *sttConnection*. This application configuration must contain the following properties:
 * 
 * 	* apiKey: IBM Public cloud STT service instance's API key Specify either the apiKey or
 * 	* accessToken: access token (e. g. Specify the IBM STT on Cloud Pak for Data access token) This property is required if apiKey is used.
 * 	* iamTokenURL: The service url of the IAM Token authorization service to fetch and refresh the IAM access
 * 	* url:         The websocket url of your stt service instance
 * 
 * A sample for an stt url:
 * wss://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/4833f0f6-d2b3-4ee0-bc35-ecaf21eabcdef/v1/recognize
 * 
 * To create the application configuration enter the following commands:
 * 		streamtool mkappconfig --description 'connection configuration for IBM Cloud Watson stt service' \
 * 			--property "apiKey=<your api key>" \
 * 			--property "iamTokenURL=https://iam.cloud.ibm.com/identity/token" \
 * 			--property "url=<your stt instance uri> \
 * 			"sttConnection"
 * 
 * NOTE: You may use the scripts start and stop to stop the sample
 * 
 * 
 * @param   appConfigName          The name of the application configuration to look for connection parameters. Default is *sttConnection*
 * 
 * @param   audioDir               directory with the audio files
 * 
 * @param   baseLanguageModel      base language model; Default: en-US_NarrowbandModel
 * 
 * @param   contentType            content type; Default audio/wav
 * 
 * @param   nonFinalUtterancesNeeded  nonFinalUtterancesNeeded; Default false
 */
public composite AudioFileWatsonSTT {

	param
		expression<rstring> $appConfigName:                  getSubmissionTimeValue("appConfigName", "sttConnection");
		expression<rstring> $audioDir:                       getSubmissionTimeValue("audioDir", "../../audio-files"); 
		expression<rstring> $baseLanguageModel:              getSubmissionTimeValue("baseLanguageModel", "en-US_NarrowbandModel");
		expression<rstring> $contentType:                    getSubmissionTimeValue("contentType", "audio/wav");
		expression<boolean> $nonFinalUtterancesNeeded: (boolean)getSubmissionTimeValue("nonFinalUtterancesNeeded", "false");

	type
		// This STT result type
		STTResult = 
			uint64 myseq,
			int32 utteranceNumber,
			boolean transcriptionCompleted, boolean finalizedUtterance,
			float64 confidence,
			float64 utteranceStartTime,
			float64 utteranceEndTime,
			rstring utteranceText,
			rstring sttErrorMessage,
			
			/*list<rstring> utteranceWords,
			list<float64> utteranceWordsConfidences,
			list<float64> utteranceWordsStartTimes,
			
			list<rstring> utteranceAlternatives, 
			list<list<rstring>> wordAlternatives,
			list<list<float64>> wordAlternativesConfidences,
			list<float64> wordAlternativesStartTimes,
			list<float64> wordAlternativesEndTimes,
	
			list<float64> utteranceWordsEndTimes,
			list<int32> utteranceWordsSpeakers,
			map<rstring, list<map<rstring, float64>>> keywordsSpottingResults,*/
			
			rstring conversationId;

	graph

		// Scan a directory periodically to pick up the audio files and 
		// send the name of that audio file to the WatsonSTT operator.
		//
		// The output stream from this operator will be connected to the
		// WatsonSTT operator's input stream. That operator expects one of its
		// input stream attributes to be named as 'speech' with an SPL type of
		// either rstring or blob. That operator can take as input an 
		// audio filename (rstring) or raw binary audio data (blob).
		// In this example, we have to simply send the name of the audio file to the
		// WatsonSTT operator which will read the binary audio data on its own from that file.
		// In addition to that mandatory input stream attribute, additional
		// attributes can be sent in the input stream to be auto assigned to
		// the output stream of the WatsonSTT operator.
		stream<rstring speech, uint64 myseq> FileNameStream as O = DirectoryScan() {
			logic
				state:
					mutable uint64 sequence_ = 0;
			param
				directory : $audioDir;
				pattern : "\\.wav$";
				sortBy: name;
				// Give sufficient delay here so that the previous operator can complete generating the
				// IAM access token and send it to the WatsonSTT operator.
				// This is not a requirement but avoids error logs in WatsonSTT operator
				initDelay: 5.0;
			output O:
				myseq = sequence_++;
			config
				placement : partitionColocation("somePartitionColocationId");
		}
		
		// The WatsonSTT Operator required that each conversation end is flagged with
		// Window Punctuation.
		stream<I> FlaggedFileNameStream = Punctor(FileNameStream as I) {
			param
				punctuate: true;
				position: after;
			config
				placement : partitionColocation("somePartitionColocationId");
		}
		
		// Provide the second input stream with the access token and refresh the token
		// periodically if required
		stream<IAMAccessToken> IAMAccessTokenStream = IAMAccessTokenGenerator() {
			config
				placement : partitionColocation("somePartitionColocationId");
		}

		// Invoke one instance of the WatsonSTT operator.
		// First input stream into this operator is the fully qualified audio file name.
		// Second input stream into this operator is your STT service instance's IAM access token.
		stream<STTResult> STTResultStream as O = WatsonSTT(FlaggedFileNameStream; IAMAccessTokenStream) {
			param
				uri: getApplicationConfigurationProperty($appConfigName, "url", "");
				baseLanguageModel: $baseLanguageModel;
				contentType: $contentType;
				sttResultMode: partial;
				nonFinalUtterancesNeeded: $nonFinalUtterancesNeeded;
				/*maxUtteranceAlternatives: 4;
				wordAlternativesThreshold: 0.7;
				keywordsSpottingThreshold: 0.2;
				keywordsToBeSpotted: ["Abraham", "Lincoln", "November", "nation", "freedom", "bill", "pay"];*/
			output O:
				conversationId = speech,
				utteranceNumber = getUtteranceNumber(),
				utteranceText = getUtteranceText(),
				finalizedUtterance = isFinalizedUtterance(),
				confidence = getConfidence(),
				transcriptionCompleted = isTranscriptionCompleted(),
				utteranceStartTime = getUtteranceStartTime(),
				utteranceEndTime = getUtteranceEndTime(),
				/*utteranceWords = getUtteranceWords(),
				utteranceWordsConfidences = getUtteranceWordsConfidences(),
				utteranceWordsStartTimes = getUtteranceWordsStartTimes(),
				
				utteranceAlternatives = getUtteranceAlternatives(),
				wordAlternatives = getWordAlternatives(),
				wordAlternativesConfidences = getWordAlternativesConfidences(),
				wordAlternativesStartTimes = getWordAlternativesStartTimes(),
				wordAlternativesEndTimes = getWordAlternativesEndTimes(),
				
				utteranceWordsSpeakers = getUtteranceWordsSpeakers(),
				keywordsSpottingResults = getKeywordsSpottingResults(),*/
				sttErrorMessage = getSTTErrorMessage();
			config
				placement : partitionColocation("somePartitionColocationId");
		}

		// Make some formatting of the output
		stream<rstring line> PrintStream as O = Custom(STTResultStream as I) {
			logic
				onTuple I: {
					mutable O otuple = {};
					if (sttErrorMessage == "") {
						otuple.line = "conversationId=" + conversationId + " myseq=" + (rstring)myseq + "\n" +
							"transcriptionCompleted=" + (rstring)transcriptionCompleted +
							" finalizedUtterance=" + (rstring)finalizedUtterance +
							" confidence=" + (rstring)confidence + " utteranceStartTime=" + (rstring)utteranceStartTime +
							" utteranceEndTime=" + (rstring)utteranceEndTime + "\n" +
							utteranceText + "\n";
					} else {
						otuple.line = "conversationId=" + conversationId + " myseq=" + (rstring)myseq + "\n" +
							"transcriptionCompleted=" + (rstring)transcriptionCompleted +
							" finalizedUtterance=" + (rstring)finalizedUtterance + "\n" +
							"    *** ERROR *** " + sttErrorMessage + "\n";
					}
					submit(otuple, O);
				}
			config
				placement : partitionColocation("somePartitionColocationId");
		}
		
		() as Sink = FileSink(PrintStream) {
			param
				file: "Tuples";
				append: true;
				flush: 1u;
				format: FileSink.line;
			config
				placement : partitionColocation("somePartitionColocationId");
		}

	config restartable: false;
} // End of composite AudioFileWatsonSTT (Main composite)
