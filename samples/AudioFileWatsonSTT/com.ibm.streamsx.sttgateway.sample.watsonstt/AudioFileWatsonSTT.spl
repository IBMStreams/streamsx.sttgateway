/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2019
==============================================
First created on: Jul/01/2018
==============================================
*/
namespace com.ibm.streamsx.sttgateway.sample.watsonstt;

use com.ibm.streamsx.sttgateway.watson::*;
/**
 * This example demonstrates transcription of audio files into text using the WatsonSTT operator. 
 * This example code is suitable for processing audio data that is already saved into 
 * a set of audio files (WAV, MP3 etc.) and made available within a specific directory.
 * 
 * This example shows how to pick up the audio files available in a given directory and
 * simply send the name of the file to the WatsonSTT operator to get the transcription results.
 * If you are looking to receive the audio data directly from the voice network switches via TCP/UDP (OR)
 * read the binary audio content from the WAV files on your own and send the
 * audio blob data (either in full or in partial blob fragments) to the WatsonSTT operator,
 * then please refer to a different example project named AudioRawWatsonSTT.
 * 
 * WAV files can be in uncompressed PCM, 16-bit little endian, 
 * 8 kHz (narrowband) or 16 KHz (broadband) sampling rate, mono format. 
 * 
 * To either downsample or upsample a WAV file, you can use the following ffmpeg command: 
 *     ffmpeg -i MyFile.wav -ac 1 -ar 8000 MyNewFile.wav
 * 
 * You can build this example from command line via the make command by using the
 * Makefile available in the top-level directory of this example. It will be
 * necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by
 * pointing it to the full path of your 
 * streamsx.sttgateway/com.ibm.streamsx.sttgateway directory.
 * 
 * If you want to build this example inside the Streams Studio, there are certain
 * build configuration settings needed. Please refer to the streamsx.sttgateway
 * toolkit documentation to learn more about those Streams Studio configuration settings.
 * 
 * 
 * IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the 
 *  Watson STT cloud service. For the STT service on IBM Public Cloud, 
 *  one must use the unexpired IAM access token (generated by using your 
 *  IBM Public cloud STT service instance's API key). 
 *  So, user must provide here his/her API key. We have some logic below that 
 *  will use the user provided API key to generate the IAM access token and 
 *  send that to the WatsonSTT operator.
 *  There is additional logic available below to keep refreshing that
 *  IAM access token periodically in order for it to stay unexpired.
 *  You should leave this submission time value empty when not using STT on IBM public cloud.
 *  https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
 * 
 * @param	sttApiKey	Specify either the public cloud IAM Token fetch/refresh URL.
 * 
 * @param	sttIAMTokenURL	Specify either the public cloud IAM Token fetch/refresh URL.
			Default https://iam.cloud.ibm.com/identity/token
			* 
 * @param	sttOnCP4DAccessToken	 Specify the IBM STT on Cloud Pak for Data (CP4D i.e. private cloud) access token.
 *  You should leave this submission time value empty when not using STT on CP4D.
 * 
 * @param	audioDir	directory with the audio files
 * @param	numberOfSTTEngines	the number of stt engines
 * @param	sttUri	the stt service uri
 * @param	sttBaseLanguageModel	base language model; Default: en-US_NarrowbandModel
 * @param	contentType	content type; Default audio/wav
 * @param	baseModelVersion	base model version
 * @param	customizationId	"customizationId"
 * @param	acousticCustomizationId	acousticCustomizationId
 * @param	customizationWeight	customizationWeight; Default 0.30
 * @param	sttJsonResponseDebugging	sttJsonResponseDebugging; Default false
 * @param	smartFormattingNeeded	smartFormattingNeeded; Default false
 * @param	keywordsSpottingThreshold	"keywordsSpottingThreshold; Default 0.0
 * @param	keywordsToBeSpotted	keywordsToBeSpotted
 * @param	websocketLoggingNeeded	websocketLoggingNeeded; Default false
 */

public composite AudioFileWatsonSTT {

	param
		expression<rstring> $sttApiKey:                getSubmissionTimeValue("sttApiKey", "");
		expression<rstring> $sttIAMTokenURL:           getSubmissionTimeValue("sttIAMTokenURL", "https://iam.cloud.ibm.com/identity/token");
		expression<rstring> $sttOnCP4DAccessToken:     getSubmissionTimeValue("sttOnCP4DAccessToken", "");
		expression<rstring> $audioDir:                 getSubmissionTimeValue("audioDir", "../../audio-files"); 
		expression<int32> $numberOfSTTEngines:  (int32)getSubmissionTimeValue("numberOfSTTEngines", "5") ;
		expression<rstring> $sttUri:                   getSubmissionTimeValue("sttUri");
		expression<rstring> $sttBaseLanguageModel:     getSubmissionTimeValue("sttBaseLanguageModel", "en-US_NarrowbandModel");
		expression<rstring> $contentType:              getSubmissionTimeValue("contentType", "audio/wav");
		expression<rstring> $baseModelVersion:         getSubmissionTimeValue("baseModelVersion", "");
		expression<rstring> $customizationId:          getSubmissionTimeValue("customizationId", "");
		expression<rstring> $acousticCustomizationId:  getSubmissionTimeValue("acousticCustomizationId", "");
		expression<float64> $customizationWeight: (float64)getSubmissionTimeValue("customizationWeight", "0.30");
		expression<boolean> $sttJsonResponseDebugging:(boolean)getSubmissionTimeValue("sttJsonResponseDebugging", "false");
		expression<boolean> $smartFormattingNeeded:(boolean)getSubmissionTimeValue("smartFormattingNeeded", "false");

	type
		// This STT result type
		STTResult_t = int32 sequence, rstring conversationId, rstring sttErrorMessage, rstring utterance;

	graph
		// IMPORTANT: IBM STT service on public cloud requires
		// an unexpired valid IAM access token to perform the 
		// speech to text task in a secure manner. You may either 
		// provide the token in parameter sttOnCP4DAccessToken, or provide 
		// the parameter sttIAMTokenURL and sttApiKey and the operator IAMAccessTokenGenerator
		// generates a new access token and then periodically refresh it. 
		// Output stream of this composite operator is connected to the
		// second input stream of the WatsonSTT operator that is used below.
		// For a correct STT operation, user must set only one of these two
		// submission time parameters to a non-empty value: sttAPIKey or sttOnCP4DAccessToken.
		stream<IAMAccessToken> IamAccessToken = IAMAccessTokenGenerator() {
			param
				apiKey: $sttApiKey;
				iamTokenURL: $sttIAMTokenURL;
				accessToken: $sttOnCP4DAccessToken;
				// All connection parameter are taken from params.
				// So if we clean the app config name, we avoid error logs
				appConfigName: "";
			config
				placement : partitionColocation("somePartitionColocationId");
		}

		// Scan a directory periodically to pick up the audio files and 
		// send the name of that audio file to the WatsonSTT operator.
		//
		// The output stream from this operator will be connected to the
		// WatsonSTT operator's input stream. That operator expects one of its
		// input stream attributes to be named as 'speech' with an SPL type of
		// either rstring or blob. That operator can take as input an 
		// audio filename (rstring) or raw binary audio data (blob).
		// In this example, we have to simply send the name of the audio file to the
		// WatsonSTT operator which will read the binary audio data on its own from that file.
		// In addition to that mandatory input stream attribute, additional
		// attributes can be sent in the input stream to be auto assigned to
		// the output stream of the WatsonSTT operator.
		stream<rstring speech, int32 sequence> AudioFileName as O = DirectoryScan() {
			logic
				state:
					mutable int32 sequence_ = 0;
			param
				directory : $audioDir;
				pattern : "\\.wav$";
				sortBy: name;
				// Give sufficient delay here so that the previous operator can complete generating the
				// IAM access token and send it to the WatsonSTT operator.
				// This is not a requirement but avoids error logs in WatsonSTT operator
				initDelay: 5.0;
			output O: sequence = sequence_++;
			config
				placement : partitionColocation("somePartitionColocationId");
		}
		
		// The WatsonSTT Operator required that each conversation end is flagged with
		// Window Punctuation.
		stream<I> AudioFileNameFlagged as O = Punctor(AudioFileName as I) {
			param
				punctuate: true;
				position: after;
			config
				placement : partitionColocation("somePartitionColocationId");
		}
		
		// Invoke one or more instances of the WatsonSTT operator.
		// Avoid feeding audio data coming from more than one data source into this 
		// parallel region which may cause erroneous transcription results.
		// NOTE: The WatsonSTT operator allows fusing multiple instances of
		// this operator into a single PE. This will help in reducing the 
		// total number of CPU cores used in running the application.
		// First input stream into this operator is the fully qualified audio file name.
		// Second input stream into this operator is your STT service instance's IAM access token.
		@parallel(width = $numberOfSTTEngines, broadcast=[AT])
		stream<STTResult_t> STTResult = 
			WatsonSTT(AudioFileNameFlagged as AFN; IamAccessToken as AT) {
			logic
				state:
					mutable int32 _conversationCnt = 0;
				
				onTuple AFN: {
					// Print the input
					printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " Channel " + (rstring)getChannel() + 
						", Speech input " + (rstring)++_conversationCnt +
						": " + (rstring)AFN.speech);
				}
			param
				sttResultMode: complete;
				uri: $sttUri;
				baseLanguageModel: $sttBaseLanguageModel;
				contentType: $contentType;
				sttJsonResponseDebugging: $sttJsonResponseDebugging;
				// Use the following operator parameters as needed.
				// Point to a specific version of the base model if needed.
				//
				// e-g: "en-US_NarrowbandModel.v07-06082016.06202016"
				baseModelVersion: $baseModelVersion;
				// Language model customization id to be used for the transcription.
				// e-g: "74f4807e-b5ff-4866-824e-6bba1a84fe96"
				customizationId: $customizationId;
				// Acoustic model customization id to be used for the transcription.
				// e-g: "259c622d-82a4-8142-79ca-9cab3771ef31"
				acousticCustomizationId: $acousticCustomizationId;
				// Relative weight to be given to the words in the custom Language model.
				customizationWeight: $customizationWeight;
				smartFormattingNeeded: $smartFormattingNeeded;
			output
				STTResult: conversationId = speech, 
				utterance = getUtteranceText();
			config
				placement : partitionColocation("somePartitionColocationId");
		}
		
		// Print the results
		() as MySink1 = Custom(STTResult as SR) {
			logic state: mutable int32 _conversationCnt = 0;
				onTuple SR: {
					printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " " + (rstring)++_conversationCnt + 
						") STT result: " + (rstring)SR);
				}
				onPunct SR: printStringLn(hourMinuteSecondMillisec(getTimestamp()) + (rstring)currentPunct());
			config
				placement : partitionColocation("somePartitionColocationId");
		}

	config restartable: false;
} // End of composite AudioFileWatsonSTT (Main composite)
