/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2022
==============================================
*/

/*
==============================================
First created on: Nov/27/2020
Last modified on: May/10/2022

IMPORTANT NOTE
--------------
This application functionally does everything that the other
application named VoiceGatewayToStreamsToWatsonSTT does with a 
major difference of how it receives the speech data for processing.
That other application invokes the IBMVoiceGatewaySource operator 
to receive the speech data directly from the IBM Voice Gateway product.
However, that other application struggles to scale well beyond
80 speech engines when it is executed in certain Docker container
based Linux machines. We have seen this behavior in a few customer 
environments where that application experienced job submission
errors such as failing to extract the SAB file or failing to
establish PE to PE connection. To avoid such errors, this new
speech processor application VgwDataRouterToWatsonSTT will not invoke 
the IBMVoiceGatewaySource operator on its own to receive the speech data.
Instead, it will receive the speech data via an intermediate application 
named VgwDataRouter that is available in a separate project directory.
This VgwDataRouterToWatsonSTT application will contain the rest of the
logic as found in the other VoiceGatewayToStreamsToWatsonSTT application.
The task of receiving the voice calls' speech data from the Voice Gateway
product and distributing it to one or more speech processor jobs is 
solely done by the VgwDataRouter application. With this arrangement, 
users should be able to launch the VgwDataRouter application only once and 
then launch the VgwDataRouterToWatsonSTT application in multiple jobs as needed. 
A single job for the VgwDataRouter application now is responsible to invoke the 
IBMVoiceGatewaySource operator for receiving the voice calls' speech data from 
the IBM Voice Gateway product. Each VgwDataRouterToWatsonSTT job can have a 
smaller number of STT engines configured to avoid any launch errors in a
docker based runtime infrastructure. The single VgwDataRouter job knows 
how to round-robin load balance the voice calls' speech data across the 
many running jobs of the VgwDataRouterToWatsonSTT application. By running 
many jobs configured with a smaller number of STT engines will help in 
avoiding the job submission errors mentioned above that can occur while 
running a massively sized single job configured with hundreds of STT engines 
in a docker container based runtime infrastructure.

Users must plan ahead of time about how many jobs of the 
VgwDataRouterToWatsonSTT application they will launch and how many 
STT engines each of those jobs will invoke. For example, if a total of 
150 concurrent calls need to be handled, that will require a total of
300 STT engines. So, one possibility is to plan 10 jobs for the 
VgwDataRouterToWatsonSTT application with 30 STT engines in each of 
those jobs. It is an important requirement that the same number of the
STT engines get invoked within each of those jobs for the speech data
distribution from the VgwDataRouter to work correctly.

Once the planning described in the previous paragraph is done, it is 
time now to first start only one job for the VgwDataRouter application.
This application's purpose is to receive speech data from the Voice Gateway 
and then route it to an appropriate speech processor job. After starting it,
multiple jobs of the VgwDataRouterToWatsonSTT application can be
started to receive the voice calls' speech data from the
VgwDataRouter application and do the required speech to text processing.

In order for this distributed collection of applications to work, there are
specific submission time parameters needed by both of those applications.

Mandatory submission time parameter(s) for the VgwDataRouter application:
Total number of speech processor jobs. e-g: -P totalNumberOfSpeechProcessorJobs=10
Number of Speech Engines in each speech processor job. e-g:  -P numberOfSpeechEnginesPerSpeechProcessorJob=30
TLS port number for VGW to send speech data to. e-g: -P tlsPortForVgwDataRx=8443
TLS port number for STT jobs to connect to for receiving speech data. e-g: -P tlsPortForVgwDataTx=8444

Mandatory submission time parameter(s) for the VgwDataRouterToWatsonSTT application:
Speech processor unique id. e-g: -P idOfThisSpeechProcessor=5
VGW Data router URL. e-g: -P vgwDataRouterTxUrl=wss://myhost10:8444/5
                     [At the very end of this URL, you must include the unique id of this speech processor.]

A) What does this example application do?
   --------------------------------------
This example demonstrates the integration of the following three products to
achieve Real-Time Speech-To-Text transcription to get the text ready for
any further analytics.

1) IBM Voice Gateway v1.0.3.0 or higher
2) IBM Streams v4.2.1.6 or higher
3) IBM Watson Speech To Text (on Cloud Pak for Data or on IBM public cloud)

These three products will work in the following sequence:

Your Telephony SIPREC-->IBM Voice Gateway-->IBM Streams-->IBM Watson STT

Note
----
This is a miniature version of a similarly named application that can be found
in the samples directory of the streamsx.sttgateway toolkit. In this mini version,
the application logic is trimmed so that it will not have other features such as 
call replay, call recording, additional logging of utterances/full results,
sending utterance results via HTTP etc. If you have a need for such features,
please use the other non-miniature version.

B) Other IBM Streams toolkits you will need for this application
   -------------------------------------------------------------
1) You must first ensure that your Streams machine where you will be compiling
this example application has these toolkits fully built and ready.
This application has a dependency on these toolkits.
   a) streamsx.sttgateway toolkit (v2.2.5 or higher)
      https://github.com/IBMStreams/streamsx.sttgateway
   b) com.ibm.streamsx.json (v1.4.6 or higher)
      [This toolkit is already available in your Streams machine's $STREAMS_INSTALL/toolkits directory.]
   c) com.ibm.streamsx.websocket (v1.0.9 or higher)
      https://github.com/IBMStreams/streamsx.websocket
   d) A utility SPL project STTGatewayUtils available in the samples directory of the sttgateway toolkit.
   e) There is an indirect dependency on the streamsx.inet toolkit (v2.3.6 or higher) that 
      gets used by an utility composite operator present inside the streamsx.sttgateway toolkit.
         
C) Building this example application
   ---------------------------------
You can build this example in a Linux terminal window via the make command by using the
Makefile available in the top-level directory of this example. Please follow
these steps to build this application:

1) streamsx.sttgateway toolkit also needs a specific version of the C++ boost library and 
the C++ websocketpp library. Please follow the instructions available in the documentation of 
that toolkit to download and prepare those libraries via the ant tool before you can 
compile this example application.

2) You have to either set the following environment variables on a terminal window where
you will run the make command or you can edit the Makefile and change these toolkit directories
to point to the correct location on your IBM Streams machine. 

   a) It will be necessary to export the STREAMS_STTGATEWAY_TOOLKIT (v2.2.5 or higher) environment variable by
      pointing it to the full path of your com.ibm.streamsx.sttgateway directory. 

   b) You have to export the STREAMS_JSON_TOOLKIT (v1.4.6 or higher) and point to the correct directory.

   c) You have to export the STREAMS_WEBSOCKET_TOOLKIT (v1.0.6 or higher) and point to the correct directory.

   d) There is an indirect dependency on the streamsx.inet toolkit. So, you have to export the 
      STREAMS_INET_TOOLKIT (v2.3.6 or higher) and point to the correct directory.

   e) If you don't want to set the above-mentioned environment variables every time you want to
      build this application, you can also edit the Makefile and enter these directories there directly.

D) Running this example application
   --------------------------------
1) In order to run this example, you must first configure the IBM Voice Gateway v1.0.3.0 or higher 
to send the binary speech data to another IBM Streams application named VgwDataRouter. 
That involves configuring the IBM Voice Gateway media relay and the IBM Voice Gateway 
SIP Orchestrator components and then deploying them. Please refer to the 
streamsx.sttgateway documentation section titled "Requirements for this toolkit".

2) Once you start the other IBM Streams application named VgwDataRouter and make sure that the 
IBM Voice Gateway is configured to send the speech data to that application, you can then give 
the following command to deploy this application in distributed mode one or more jobs/copies as needed.

NOTE: At the very end of the VgwDataRouterTxURL in the command below, you must include the unique id of each speech processor.
st submitjob -d <YOUR_STREAMS_DOMAIN> -i <YOUR_STREAMS_INSTANCE>  -P idOfThisSpeechProcessor=5 -P vgwDataRouterTxUrl=wss://myhost10:8444/5 -P numberOfSTTEngines=40 -P numberOfResultProcessors=20 -P sttApiKey=<YOUR_WATSON_STT_API_KEY> -P contentType="audio/mulaw;rate=8000" com.ibm.streamsx.sttgateway.sample.watsonstt.VgwDataRouterToWatsonSTT.sab

3) When the live calls are happening, you can watch the data directory in your copy of
this application for specific files per call that will show you just the utterances or 
the full transcription details for every voice call.

4) For large scale tests in our IBN Streams Lab in NY, I used the following job submission command to
deploy this application for processing 100 concurrent calls.
My IBM Streams instance had 10 application machines with a total of 344 virtual cores 
(i.e. 172 physical cores) and each machine had a minimum of 128GB memory.

time st submitjob -d <YOUR_STREAMS_DOMAIN> -i <YOUR_STREAMS_INSTANCE> -P idOfThisSpeechProcessor=5 -P vgwDataRouterTxUrl=wss://myhost10:8444/5 -P numberOfSTTEngines=200 -P numberOfResultProcessors=100 -P sttApiKey=<YOUR_WATSON_STT_API_KEY> -P contentType="audio/mulaw;rate=8000" -P writeTranscriptionResultsToFiles=true -P sendTranscriptionResultsToHttpEndpoint=true -P httpEndpointForSendingTranscriptionResults=<YOUR_HTTP_ENDPOINT_URL> -C fusionScheme=legacy com.ibm.streamsx.sttgateway.sample.watsonstt.VgwDataRouterToWatsonSTT.sab

When deploying a large number of STT engines (more than 14), it is recommended to follow these tips.

--> Before deploying applications with large number of PEs, it is necessary to have the 
    following domain and instance timeout properties set to higher values.
       st setdomainprop -d <YOUR_STREAMS_DOMAIN> controller.requestTimeout=600 controller.resourceHealthTimeout=1200 domain.serviceHealthTimeout=300 jmx.inactivityTimeout=300

       st setprop -d <YOUR_STREAMS_DOMAIN> -i <YOUR_STREAMS_INSTANCE> hc.pecStartTimeout=600

--> Make a note of the fusionScheme set to legacy towards the end of that job submission command.
    That will help in avoiding PE connection timeouts when deploying large application graphs.

--> When deploying large number of PEs, run these two commands to verify that there are no PE start-up problems.
       st lspes -d <YOUR_STREAMS_DOMAIN> -i <YOUR_STREAMS_INSTANCE> | grep -i no | wc -l
       
       [
       It may take upto 6 minutes to deploy the job with the large scale command shown above.
       If it shows a result of 0 after that time, that means all the PEs are fine.
       If not, there are PE startup problems. Then, you have to check the PE log for
       finding out about the PE startup problems.
       ] 
       
       st lspes -d <YOUR_STREAMS_DOMAIN> -i <YOUR_STREAMS_INSTANCE> | grep -i yes | wc -l
       
       [
       This command should show a result in a number that matches with the total number of
       PEs present in the large scale application graph deployed above. In essence,
       the resulting number here should account for all the PEs in the deployed application.
       ]
==============================================
*/
namespace com.ibm.streamsx.sttgateway.sample.watsonstt;

// We will use the WatsonSTT operators from this namepsace. 
use com.ibm.streamsx.sttgateway.watson::*;
// A few  C++ native functions are used from this namespace.
use com.ibm.streamsx.sttgateway.utils::*;
use spl.file::*;
// HttpPost comes from this namespace..
use com.ibm.streamsx.websocket.op::*;

// This is a schema that will get used in two different 
// composites in this file. So, we have to define it
// outside of those composites in order for the compiler to
// resolve this type correctly.
// 
// This STT result type contains many attributes to
// demonstrate all the basic and very advanced features of 
// the Watson STT service. Not all real-life applications will need 
// all these attributes. You can decide to include or omit these
// attributes based on the specific STT features your application will need. 
// Trimming the unused attributes will also help in 
// reducing the STT processing overhead and in turn 
// help in receiving the STT results faster.
// Read the streamsx.sttgateway toolkit documentation to learn about
// what features are available, how they work and how different attributes are 
// related to those features.
type MySTTResult_t = rstring vgwSessionId, boolean isCustomerSpeechData, 
	int32 vgwVoiceChannelNumber, 
	rstring callerPhoneNumber, rstring agentPhoneNumber,
	rstring ciscoGuid, 
	int32 speechDataFragmentCnt, int32 totalSpeechDataBytesReceived, 
	int32 speechProcessorId, int32 speechEngineId, int32 speechResultProcessorId,
	rstring callStartDateTime, 
	int64 callStartTimeInEpochSeconds,
	// This is the utterance result reception time expressed as ctime string.
	rstring utteranceResultReceptionTime,
	// Utterance Rx Time is time in seconds since the start of the call. 
	int64 utteranceRxTime,
	int32 utteranceNumber,
	rstring utteranceText, boolean finalizedUtterance,
	float64 confidence, 
	rstring sttErrorMessage, boolean transcriptionCompleted,
	list<rstring> utteranceAlternatives, 
	list<list<rstring>> wordAlternatives,
	list<list<float64>> wordAlternativesConfidences,
	list<float64> wordAlternativesStartTimes,
	list<float64> wordAlternativesEndTimes,
	list<rstring> utteranceWords,
	list<float64> utteranceWordsConfidences,
	list<float64> utteranceWordsStartTimes,
	list<float64> utteranceWordsEndTimes,
	float64 utteranceStartTime,
	float64 utteranceEndTime,
	list<int32> utteranceWordsSpeakers,
	list<float64> utteranceWordsSpeakersConfidences,
	map<rstring, list<tuple<float64 startTime, float64 endTime, float64 confidence>>> keywordsSpottingResults;

// The following block of commentary is needed for the spldoc creation during the ant build process.
/**
 * This example demonstrates the integration of the following three products to 
 * achieve Real-Time Speech-To-Text transcription to get the text ready for 
 * any further analytics.
 * 
 * 		1) IBM Voice Gateway 
 * 		2) IBM Streams 
 * 		3) IBM Watson Speech To Text (on Cloud Pak for Data or on IBM public cloud) 
 * 
 * These three products will work in the following sequence:
 * 
 * 		Your Telephony SIPREC-->IBM Voice Gateway-->IBM Streams-->IBM Watson STT
 * 
 * You can build this example from command line via the make command by using the 
 * Makefile available in the top-level directory of this example. It will be 
 * necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by 
 * pointing it to the full path of your 
 * streamsx.sttgateway/com.ibm.streamsx.sttgateway directory. 
 * 
 * If you want to build this example inside the Streams Studio, there are certain 
 * build configuration settings needed. Please refer to the streamsx.sttgateway 
 * toolkit documentation to learn more about those Streams Studio configuration settings. 
 * 
 * @param	idOfThisSpeechProcessor	A number to indicate id of this speech processor job.
 * 
 * @param	vgwDataRouterTxUrl	URL of the VGW Data Router application's data transmitter.
 * 
 * @param	certificateFileName		Client side certificate (.pem) file for the WebSocketSendReceive operator. 
 * 
 * @param   certificatePassword This parameter specifies a password needed for decrypting the WebSocket client's private key in the PEM file. Default is an empty string.
 *
 * @param	vgwLiveMetricsUpdateNeeded	Is live metrics needed for the WebSocketSendReceive operator?
 * 
 * @param	vgwWebsocketLoggingNeeded	Is WebSocket library low level logging needed?
 * 
 * @param	sttApiKey	IBM Watson STT related submission time values are defined below.
 *  IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the 
 *  Watson STT cloud service. For the STT service on IBM Public Cloud, 
 *  one must use the unexpired IAM access token (generated by using your 
 *  IBM Public cloud STT service instance's API key). 
 *  So, user must provide here his/her API key. We have some logic below that 
 *  will use the user provided API key to generate the IAM access token and 
 *  send that to the WatsonSTT operator.
 *  There is additional logic available below to keep refreshing that
 *  IAM access token periodically in order for it to stay unexpired.
 *  You should leave this submission time value empty when not using STT on IBM public cloud.
 *  https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
 * 
 * @param	sttIAMTokenURL	Specify either the public cloud IAM Token fetch/refresh URL.
 * 
 * @param	sttOnCP4DAccessToken	Specify the access token refresh interval in minutes.
 * 
 * @param	numberOfSTTEngines	Number of stt engines
 * 
 * @param	numberOfResultProcessors  Number of result processors
 * 
 * @param	initDelayBeforeSendingDataToSttEngines	Time in seconds to wait before sending data to the STT engines.
 * 
 * wss://api.{location}.speech-to-text.watson.cloud.ibm.com/instances/{instance_id}/v1/recognize
 * @param	sttUri	Mandatory parameter with no default.
 * 
 * @param	sttBaseLanguageModel	sttBaseLanguageModel
 * 
 * @param	contentType	contentType
 * 
 * @param	nonFinalUtterancesNeeded	nonFinalUtterancesNeeded
 * 
 * @param	baseModelVersion	baseModelVersion
 * 
 * @param	customizationId	customizationId
 * 
 * @param	acousticCustomizationId	acousticCustomizationId
 * 
 * @param	customizationWeight	customizationWeight
 * 
 * @param	maxUtteranceAlternatives	maxUtteranceAlternatives
 * 
 * @param	sttRequestLogging	sttRequestLogging
 * 
 * @param	filterProfanity	filterProfanity
 * 
 * @param	wordAlternativesThreshold	wordAlternativesThreshold
 * @param	smartFormattingNeeded	smartFormattingNeeded
 * @param	redactionNeeded	redactionNeeded
 * @param	speechDetectorSensitivity	speechDetectorSensitivity
 * @param	backgroundAudioSuppression	backgroundAudioSuppression
 * @param	characterInsertionBias	characterInsertionBias
 * @param	keywordsSpottingThreshold	keywordsSpottingThreshold
 * @param	keywordsToBeSpotted	keywordsToBeSpotted
 * @param	sttWebsocketLoggingNeeded	sttWebsocketLoggingNeeded
 * @param	cpuYieldTimeInAudioSenderThread	cpuYieldTimeInAudioSenderThread
 * @param	sttLiveMetricsUpdateNeeded	sttLiveMetricsUpdateNeeded
 * 
*/
// This is the main composite for this application.
public composite VgwDataRouterToWatsonSTT {
	param
		// Id of this speech processor job.
		// If you are starting 10 different copies/jobs of this application, then
		// you should give a unique number ranging from 1 to 10 for each of those 
		// jobs in the order in which they get started.
		expression<int32> $idOfThisSpeechProcessor : 
			(int32)getSubmissionTimeValue("idOfThisSpeechProcessor");
		// URL of the VgwDataRouterTxUrl must be specified at the time of
		// launching this application.
		expression<rstring> $vgwDataRouterTxUrl : getSubmissionTimeValue("vgwDataRouterTxUrl");
		// WebSocketSendReceive operator related submission time values are defined below.
		// Please refer to the following file in the etc sub-directory of 
		// this application for details about client-side and server-side certificates:
		// etc/creating-a-self-signed-certificate.txt
		expression<rstring> $certificateFileName :
			getSubmissionTimeValue("certificateFileName", "");
		// Is a password needed for the private key in the certificate file?
		expression<rstring> $certificatePassword : 
			getSubmissionTimeValue("certificatePassword", "");
		// Do you want to specify a file name that contains the public certificate of
		// the trusted remote server. If this file name is not empty, then the
		// WebSocketSendReceive operator will perform a server authentication.
		expression<rstring> $trustedServerCertificateFileName :
			getSubmissionTimeValue("trustedServerCertificateFileName", "");	
		// Do you want to specify a list of identifiers present in the 
		// trusted server's X509 certificate's subject line. If that certificate is
		// self signed, then it will help during the server authentication to approve
		// that server's identity as a known one.
		// 
		// Following are some examples of the subject line as it appears in an X509 public certificate.
		// /C=US/ST=NY/L=Yorktown Heights/O=IBM/OU=AI/CN=websocket.streams/emailAddress=websocket.streams@ibm.com
		// /C=US/O=Let's Encrypt/CN=Let's Encrypt Authority X3
		// /C=BE/O=GlobalSign nv-sa/CN=GlobalSign CloudSSL CA - SHA256 - G3
		// /C=US/O=Google Trust Services/CN=GTS CA 1O1
		// /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA
		// /C=US/ST=New York/L=Armonk/O=IBM/CN=www.ibm.com
		//
		// So your value for this submission time parameter can be as shown here.
		// ['emailAddress=websocket.streams@ibm.com', 'CN=www.ibm.com']
		expression<list<rstring>> $trustedServerX509SubjectIdentifiers :
			(list<rstring>)getSubmissionTimeValue("trustedServerX509SubjectIdentifiers", "[]");
		// Is live metrics needed for the WebSocketSendReceive operator?
		expression<boolean> $websocketLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("websocketLiveMetricsUpdateNeeded", "true");
		// Is WebSocket library low level logging needed?
		expression<boolean> $websocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("websocketLoggingNeeded", "false");
		// Is WebSocket server connection logging needed?
		expression<boolean> $wsConnectionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("wsConnectionLoggingNeeded", "false");
		// Is client message exchange logging needed for debugging?
		expression<boolean> $wsClientSessionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("wsClientSessionLoggingNeeded", "false");
		// Under some circumstances, if the IBMVoiceGatewaySource operator sends 
		// only one EOCS (End Of Call Signal) tuple instead of two as required for
		// the two voice channels, that may eventually cause the application logic
		// below not be able to release the speech engines properly at the end of
		// a voice call for a given VGW session id. We have seen it in certain
		// customer environments. To avoid that condition, such customers can
		// configure this application to treat the very first EOCS tuple as 
		// sufficient to treat a voice call as a "completed call". In that case,
		// it will simply ignore if and when a second EOCS tuple arrives.
		// This feature can be activated to compensate for the situation described
		// above if it happens in some customer environments.
		// (Senthil added this on Feb/01/2021).
		expression<int32> $numberOfEocsNeededForVoiceCallCompletion : 
			(int32)getSubmissionTimeValue("numberOfEocsNeededForVoiceCallCompletion", "2");	
		//
		// IBM Watson STT related submission time values are defined below.
		// IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the 
		// Watson STT cloud service. For the STT service on IBM Public Cloud, 
		// one must use the unexpired IAM access token (generated by using your 
		// IBM Public cloud STT service instance's API key). 
		// So, user must provide here his/her API key. We have some logic below that 
		// will use the user provided API key to generate the IAM access token and 
		// send that to the WatsonSTT operator.
		// There is additional logic available below to keep refreshing that
		// IAM access token periodically in order for it to stay unexpired.
		// You should leave this submission time value empty when not using STT on IBM public cloud.
		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
		expression<rstring> $sttApiKey : getSubmissionTimeValue("sttApiKey", "");
		// Specify either the public cloud IAM Token fetch/refresh URL.
		expression<rstring> $sttIAMTokenURL : 
			getSubmissionTimeValue("sttIAMTokenURL", "https://iam.cloud.ibm.com/identity/token");
		// Specify the IBM STT on Cloud Pak for Data (CP4D i.e. private cloud) access token.
		// You should leave this submission time value empty when not using STT on CP4D.
		expression<rstring> $sttOnCP4DAccessToken : getSubmissionTimeValue("sttOnCP4DAccessToken", "");		
		expression<int32> $numberOfSTTEngines :(int32)
			getSubmissionTimeValue("numberOfSTTEngines", "10");
		// Since both the voice channels for a given call will be
		// processed by individual STT engines, the utterance results 
		// coming from those two engines should be fed into a single
		// result processor. That means, we only need half the total size of
		// the provisioned number of STT engines for the result processors.
		//  We will do a validation of this in one of the custom operators below 
		// at the time of this application's startup phase. 
		expression<int32> $numberOfResultProcessors : (int32)
			getSubmissionTimeValue("numberOfResultProcessors", "5");
		// Time in seconds to wait before sending data to the STT engines.
		expression<float64> $initDelayBeforeSendingDataToSttEngines :
			(float64)getSubmissionTimeValue("initDelayBeforeSendingDataToSttEngines", "15.0"); 			
			
	graph
		// Ingest the speech data coming from the IBM Voice Gateway Data Router application.
		// That is another IBM Streams application named VgwDataRouter.
		// Such speech data arrives here in multiple fragments directly from
		// a live voice call. That application is capable of receiving speech data
		// from multiple calls that can all happen at the very same time between
		// different pairs of speakers. Then, it distributes the speech data
		// to one or more jobs of the application's code written in this SPL file.
		// It is very important to note that the IBM Voice Gateway will keep
		// sending the speech data of the caller and the agent on two 
		// voice channels i.e. one for the caller and the other for the agent.
		// Irrespective of those two speakers talk or remain silent during the
		// call, their assigned voice channel will always carry some binary
		// data. That means, there is no way to know who is currently
		// talking. This constraint limits us from sending only one of the
		// channel's data to a single STT engine at any given time.
		// Instead, this constraint forces us to dedicate a single STT engine
		// per voice channel in a given voice call and keep sending the
		// data being received on that channel continuously to that
		// dedicated STT engine irrespective of whether that channel carries
		// silence or active speech data. In summary, we will need two
		// STT engines to do the Speech 2 Text for every ongoing voice call.
		// So, you have to plan ahead of time about the number of STT engines
		// you will start for handling the maximum number of concurrent calls.
		// As an example, for handling a maximum of 100 concurrent voice calls,
		// you will have to start 200 STT engines.
		//
		// In your own real-life applications, you may want to simply 
		// copy and reuse the code from this example and then make the 
		// changes only where it is really needed.
		// This example presents the following application design pattern:
		// IBMVoiceGatewaySource-->Speech Data Router-->STT Engine->STT Result Processor
		// You should be fine to simply use the entire pattern as it is except for
		// making changes in the STT Result Processor composite and beyond to address
		// your own needs of further analytics on the STT results as well as
		// specific ways of delivering the STT results to other 
		// downstream systems rather than only writing to files as this example does below.
		//
		// Let us generate text data (just one tuple) to establish a 
		// persistent bidirectional connection with the VGW data router application's 
		// data transmitter WebSocketSink operator.
		(stream<SendData_t> TextData as TD) 
			as TextDataGenerator = Beacon() {
			param
				iterations: 1u;
				initDelay: 3.0;
			
			output
				TD: strData = "This data item " + 
					(rstring)(IterationCount() + 1ul) + " is sent as a text.";
		}		

		// ========= START OF THE CODE BLOCK WITH DATA RETRANSMISSION LOGIC =========
		// Any data item we send to the remote WebSocket server will have a result to
		// indicate whether that data item was sent successfully or not.
		// Let us release the incoming tuples one by one as fast as possible once 
		// we get a successful data send result. If the data sending failed due to 
		// connection error, then we can simply retransmit the tuple that didn't get sent. 
		// This safety measure and retransmission logic is a must instead of trying to 
		// send blindly when there is no WebSocket server active on the other end. 
		// This type of low impact throttling/gating the tuple flow based on the 
		// previous tuple's send result will require special application logic. 
		// The pattern shown here can be adopted in real-life applications.
		// Without this logic to check the data send result, the WebSocketSendReceive
		// operator will not properly do its full duplex task of simultaneously
		// sending and receiving data to/from the remote WebSocket server.
		//
		// Note: Consuming the SDR stream below in the 2nd input port will make the
		// compiler to give a "Feedback Loop" warning which can be ignored.
		//
		(stream<SendData_t> SendData as SD) 
			as DataThrottler = Custom(TextData as D; SendDataResult as SDR) {
			logic
				state: {
					mutable list<SendData_t> _dataToBeSent = [];
					mutable boolean _sendResultPending = false;
					mutable uint64 _tuplesSentCnt = 0;
					mutable int32 _numberOfWindowPunctuations = 0;
					mutable int32 _numberOfFinalPunctuations = 0;
					mutable boolean _speechEnginesConfiguredCorrectly = 
						validateSpeechEnginesConfiguration($numberOfSTTEngines);
					mutable boolean _resultProcessorsConfiguredCorrectly = 
						validateResultProcessorConfiguration($numberOfSTTEngines,
						$numberOfResultProcessors);
				}
				
				onTuple D: {
					// We will send the data item from here only when there is
					// no pending data send activity i.e. when we are not waiting to get a send result.
					// If there is a pending data send activity, subsequent data will be 
					// sent in the other onTuple block that checks the result of the 
					// previous data send activity.
					//
					// Simply insert into the list state variable.
					appendM(_dataToBeSent, D);
					
					if (_sendResultPending == false) {
						_tuplesSentCnt++;
						
						if (_tuplesSentCnt == 1ul) {
							appTrc(Trace.error, 
								"Sending the first test data item as handshake from speech processor " +
								(rstring)$idOfThisSpeechProcessor + 
								" to the VgwRouterTx endpoint " + $vgwDataRouterTxUrl + ".");
						}
						
						// There is no pending data send activity.
						// So, we can release the oldest tuple waiting to be 
						// sent to the remote server.
						_sendResultPending = true;
						submit(_dataToBeSent[0], SD);
					}
				}
	
				// Process the send data result.
				// This is the feedback stream coming from the WebSocketSendReceive operator used below.
				onTuple SDR: {
					if (SDR.sendResultCode != 0) {
						// If the previous send activity failed, it will simply retransmit the
						// tuple that couldn't be sent.
						appTrc(Trace.error, "Error in sending data item " + 
							(rstring)_tuplesSentCnt + ". " +  
							"Attempting to the send it again now. Send data result=" + (rstring)SDR);
						// If it is a connection error due to the remote WebSocket server's absence,
						// there is no valid reason to keep retrying immediately and too frequently.
						// It may be a good idea to retry at a slower pace by inducing a wait time that is 
						// suitable for the needs of your application. Additional application logic
						// can also be added to limit the number of retransmission attempts and
						// take a proper action when exceeding that threshold.
						//
						// IMPORTANT TIP
						// -------------
						// It is a good practice for the application logic to backoff and wait for a 
						// reasonable amount of time when there is a connection error with the
						// remote WebSocket server before inputting a tuple again into the WebSocketSendReceive operator.
						// Otherwise, that operator will trigger too many connection attempts on every incoming
						// tuple to send it to the remote server. So, the application
						// logic should make an attempt to wait for a while before attempting to send the
						// data after knowing that there is an ongoing connection problem with the remote server.
						block(10.0);
						submit(_dataToBeSent[0], SD);
					} else {
						// Previous tuple was successfully sent.
						// Remove it from the list state variable.
						removeM(_dataToBeSent, 0);
						
						// Let us send if the next one is available in the waiting list.
						if(size(_dataToBeSent) > 0) {						
							_sendResultPending = true;
							submit(_dataToBeSent[0], SD);	
						} else {
							// Nothing pending to be sent at this time.
							_sendResultPending = false;
						}
					}
				} 
	
				onPunct D: {
					// We are going to ignore the punctuation markers sent by the Beacon operator.
					// Because, we don't want any final punctuation marker to stop the normal
					// operation of the downstream WebSocketSendReceive operator.
					// In general, it is a good idea for the WebSocketSendReceive operator below to 
					// keep its persistent WebSocket client connection open until this application is
					// cancelled/stopped/shut down. That will help in receiving the data sent by the
					// remote server based WebSocketSink operator.
					//				
					// Unless the punctuations play a key role in the application behavior,
					// the strategy described above can be followed.
					return;
				}
						
			config
				threadedPort: queue(D, Sys.Wait);
		}
		// ========= END OF THE CODE BLOCK WITH DATA RETRANSMISSION LOGIC =========
		
		// Send a single data item (tuple) to establish a persistent connection with the
		// remote WebSocket server and then start receiving binary data or text data or both 
		// from the remote WebSocketSink operator.
		(stream<ReceivedData_t> ReceivedData as RD;
		 stream<SendDataResult_t> SendDataResult as SDR) 
			as VgwDataRouterInterface = WebSocketSendReceive(SendData) {
			param
				url: $vgwDataRouterTxUrl;
				certificateFileName: $certificateFileName;
				certificatePassword: $certificatePassword;
				trustedServerCertificateFileName: $trustedServerCertificateFileName;
				// Use this only when you have trouble authenticating a server that 
				// has a self signed certificate.
				trustedServerX509SubjectIdentifiers: $trustedServerX509SubjectIdentifiers;
				websocketLiveMetricsUpdateNeeded: $websocketLiveMetricsUpdateNeeded;
				websocketLoggingNeeded: $websocketLoggingNeeded;
				wsConnectionLoggingNeeded: $wsConnectionLoggingNeeded;
				wsClientSessionLoggingNeeded: $wsClientSessionLoggingNeeded;
			
			// Get these values via custom output functions	provided by this operator.
			output
			    // strData and/or blobData attributes will be automatically
			    // assigned with received data values withing the operator logic.
			    // Other attributes can be assigned manually as done below.
				RD: totalDataItemsReceived = getTotalDataItemsReceived(),
					totalDataBytesReceived = getTotalDataBytesReceived(),
					totalDataItemsSent = getTotalDataItemsSent(),
					totalDataBytesSent = getTotalDataBytesSent();
			
			// A MUST THING TO DO
			// ------------------
			// For the feedback loop logic to work as explained in the
			// commentary above, you must have this placement directive to launch 
			// this operator in its own PE (i.e. Linux process) that is away from 
			// the Custom operator above which is at the other end of the feedback loop.
			// This is done for a valid reason. Infinite recursion occurs when operators with 
			// feedback loops are fused; when the operator submits a tuple to its output port, 
			// the subsequent submit() calls lead to a loop of other submit() calls, 
			// effectively overflowing the call stack. By avoiding this operator from getting 
			// fused, we ensure that it will not lead to deadlocks or stack overflow due to 
			// infinite recursion.			
			config
				placement: partitionIsolation;
		}

		// We can now (twice) deserialize the data received from the VGW data router application to
		// get the actual speech data or the EOCS (End Of Call Signal).
		(stream<BinarySpeech_t> BinarySpeechData as BSD) as VgwDataParser = 
		 Custom(ReceivedData as RD) {
		 	logic
				onTuple RD: {
					// We can only accept binary data at this time from the VgwDataRouter.
					if (blobSize(RD.blobData) > 0ul) {
						// We have to deserialize it twice to get the tuple formetted data.
						// First deserialization will give us the message type and the serialized binary payload.
						mutable DataFromVgwRouter_t outerTuple = (DataFromVgwRouter_t){};
						// Call a native function to do the Blob-->Tuple conversion.
						deserializeTuple(outerTuple, RD.blobData);	
						// Second deserialization of the received payload will 
						// give us the actual tuple that we want.
						if(outerTuple.msgType == 1) {
							// This is speech data sent by the Voice Gateway product.
							mutable BinarySpeech_t speechData = (BinarySpeech_t){};
							deserializeTuple(speechData, outerTuple.payload);
							// Set the current speech processor id.
							speechData.speechProcessorId = $idOfThisSpeechProcessor;
							submit(speechData, BSD);
						} else if(outerTuple.msgType == 2) {
							// This is End Of Call Signal (EOCS) sent by the Voice Gateway product.
							mutable BinarySpeech_t eocs = (BinarySpeech_t){};
							deserializeTuple(eocs, outerTuple.payload);
							submit(eocs, BSD);
						} else {
							// Unsupported message type received.
							appTrc(Trace.error, "Unsupported message type " + 
								(rstring)outerTuple.msgType + 
								" sent by the VgwDataRouter to speech processor " +
								(rstring)$idOfThisSpeechProcessor + ".");
						}					 				
					} // End of if.
				} // End of onTuple.

			config
				threadedPort: queue(RD, Sys.Wait);
		}

		// We have to always route the speech data bytes (fragments) coming from  
		// a given vgwSessionId_vgwVoiceChannelNumber to a particular 
		// WatsonSTT operator instance available within a parallel region. 
		// We already explained in detail in the previous operator's
		// commentary section about why it must be done this way.
		// This idea of pairing up a vgwSessionId_vgwVoiceChannelNumber combo
		// to a particular parallel region channel is a must for the 
		// speech data bytes of a given speaker in a voice call to always land in 
		// the same WatsonSTT engine. This stickiness (a.k.a channel affinity) is
		// important to continuously transcribe the speech data arriving on both the
		// voice channels at all the time including the silence time of a speaker.
		// This is needed because the IBM Voice Gateway keeps sending the 
		// speech data bytes of both the speakers (whether active or silent) at 
		// all the time on two voice channels by dedicating one channel to an
		// agent and the other channel to the caller. So, this requires 
		// extra logic to locate an unused parallel channel 
		// i.e. an idle STT engine to be assigned for a 
		// given vgwSessionId_vgwVoiceChannelNumber.
		// That special logic happens inside this operator.
		(stream<BinarySpeech_t> BinarySpeechDataFragment as BSDF) as
			BinarySpeechDataRouter = Custom(BinarySpeechData as BSD) {
			logic
				state: {
					// This map tells us which UDP channel is processing a 
					// given vgwSessionId_vgwVoiceChannelNumber combo.
					mutable map<rstring, int32> _vgwSessionIdToUdpChannelMap = {};
					// This list tells us which UDP channels for STT engines are 
					// idle at any given time.
					mutable list<int32> _idleUdpChannelsList = 
						prepareIdleUdpChannelsList($numberOfSTTEngines);
					// This list tells us which UDP channels for result processors are 
					// idle at any given time.
					mutable list<int32> _idleResultProcessorUdpChannelsList = 
						prepareIdleResultProcessorUdpChannelsList($numberOfResultProcessors);
					// This map tells us which UDP channel is going to process
					// the given voice call's (i.e. vgwSessionId) transcription
					// results in the STTResultProcessor composite that appears
					// below in this SPL source file.
					mutable map<rstring, int32> _vgwSessionToResultProcessorChannelMap = {};
					// This map tells us which UDP channel for the STT engine that 
					// can be released after completing the speech to text work for a given 
					// vgwSessionId_vgwVoiceChannelNumber.
					// After getting released, such UDP channels will become available for 
					// doing speech to text work for any new voice calls.
					mutable map<rstring, int32> _vgwSessionToCompletedUdpChannelMap = {};
					// This map tells us the call start date time string for a given vgwSessionId.
					mutable map<rstring, rstring> _vgwSessionToCallStartDateTime = {};
					// This map tells us the call start time in epoch seconds for a given vgwSessionId.
					mutable map<rstring, int64> _vgwSessionToCallStartTimeInEpochSeconds = {};
					mutable BinarySpeech_t _oTuple = {};
					mutable rstring _key = "";
				}
			
				// Process the Binary Speech Data.
				onTuple BSD: {
					// Get the sessionId + channelNumber combo string.
					_key = BSD.vgwSessionId + "_" + (rstring)BSD.vgwVoiceChannelNumber;

					// We will get the regular binary speech data and the End Of Call Signal (EOCS) in
					// the same input stream. This design change was done on Feb/09/2021 to avoid any
					// any port locks and/or tuple ordering issues that may happen if we choose to 
        			// do it using two different output ports. The incoming tuple has an attribute
        			// that is set to true or false by the IBMVoiceGatewaySource operator to indicate
        			// whether it is sending binary speech data or an EOCS.
        			if(BSD.endOfCallSignal == false) {
        				// The incoming tuple contains binary speech data.
        				// If we are sent an empty speech blob, let us ignore that as
        				// it will cause unwanted side effects in the downstream logic.
        				if(blobSize(BSD.speech) <= 0ul) {
							appTrc(Trace.error, 
								"_ZZZZZ Empty speech packet received for " +
								"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
								" We are going to ignore this packet by not sending it to" +
								" an STT engine.");
        					return;
        				}
										
						// Check if this vgwSessionId_vgwVoiceChannelNumber combo already 
						// has an STT engine allocated for it via an UDP channel.					
						if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
							// This is a speaker of an ongoing voice call who has 
							// already been assigned to an STT engine.
							// Always send this speaker's speech data fragment to 
							// that same STT engine.
							BSD.speechEngineId = _vgwSessionIdToUdpChannelMap[_key];
							// We can always assume that there is a preselected 
							// STT result processor UDP channel available for this 
							// voice call (i.e. vgwSessionId). Because, it is already 
							// done in the else block below when this voice call's 
							// first speaker's speech data arrives here.
							// Let us fetch and assign it here.
							if (has(_vgwSessionToResultProcessorChannelMap, 
								BSD.vgwSessionId) == true) {
								BSD.speechResultProcessorId = 
									_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
							} else {
								// This should never happen since the call will end
								// for both the speakers almost at the same time after 
								// which there will be no speech data from any of the
								// speakers participating in a given voice call.
								// This else block is just part of defensive coding.
								appTrc(Trace.error, 
									"_XXXXX No STT result processor available at this time for the " +
									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
									". This should be a rare occurrence towards the very end of the call." + 
									" We are not going to process the speech data bytes" +
									" of this speaker in this voice call.");
								return;
							}
						} else {
							// If we are here, that means this is a brand new speaker of a
							// voice call for whom we must find an idle UDP channel a.k.a
							// an idle STT engine that can process this speaker's speech data.
							int32 mySpeechEngineId = getAnIdleUdpChannel(_idleUdpChannelsList);
							
							if (mySpeechEngineId == -1) {
								// This is not good and we should never end up in this situation.
								// This means we have not provisioned sufficient number of STT engines to
								// handle the maximum planned concurrent calls. We have to ignore this
								// speech data fragment and hope that an idle UDP channel number will
								// become available by the time the next speech data fragment for this
								// particular vgwSessionId_vgwVoiceChannelNumber combo arrives here. 
								if (BSD.speechDataFragmentCnt == 1) {
									// Display this alert only for the very first data fragment of a 
									// given speaker of a given voice call.
									appTrc(Trace.error, "No idle STT engine available at this time for the " +
										"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
										". There are " + (rstring)$numberOfSTTEngines +
										" STT engines configured and they are all processing other" +
										" voice calls at this time. Please start sufficient number of STT engines" +
										" next time to handle your maximum expected concurrent calls." +
										" A rule of thumb is to have two STT engines to process" +
										" two speakers in every given concurrent voice call.");
								}
	
								return;	
							} else {
								// We got an idle STT engine.
								BSD.speechEngineId = mySpeechEngineId;
									
								// For this voice call (i.e. vgwSessionId), select a 
								// single result processor UDP channel. Both speakers in this 
								// voice call will use that single result processor instance.
								// This will ensure that the STT results for both the speakers 
								// will reach the same result processor.
								if (has(_vgwSessionToResultProcessorChannelMap, 
									BSD.vgwSessionId) == false) {
									int32 myResultProcessorId = 
										getAnIdleUdpChannel(_idleResultProcessorUdpChannelsList);
									
									if (myResultProcessorId == -1) {
										// This is not good and we should never end up in this situation.
										// This means we have not provisioned sufficient number of result processors to
										// handle the maximum planned concurrent calls. We have to ignore this
										// speech data fragment and hope that an idle UDP channel number will
										// become available by the time the next speech data fragment for this
										// particular vgwSessionId_vgwVoiceChannelNumber combo arrives here. 
										if (BSD.speechDataFragmentCnt == 1) {
											// Display this alert only for the very first data fragment of a 
											// given speaker of a given voice call.
											appTrc(Trace.error, "No idle result processor available at this time for the " +
												"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
												". There are " + (rstring)$numberOfResultProcessors +
												" result processors configured and they are all being used for other" +
												" voice calls at this time. Please start sufficient number of result processors" +
												" next time to handle your maximum expected concurrent calls." +
												" A rule of thumb for your result processor count is to have an" +
												" exactly half the size of the total number of STT engines configured.");
										}
			
										return;	
									} else {
										insertM(_vgwSessionToResultProcessorChannelMap,
											BSD.vgwSessionId, myResultProcessorId);
									}
								} 
								
								// Set the STT result processor id.
								BSD.speechResultProcessorId = 
									_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
	
								// If this call is just beginning, then we will create a
								// tiny text file to indicate that we started receiving 
								// speech data from the IBM Voice Gateway for this new call.
								rstring key1 = BSD.vgwSessionId + "_" + "1";
								rstring key2 = BSD.vgwSessionId + "_" + "2";
								mutable int32 err = 0ul;
								
								// If we have not yet created any entry in our state map for this call,
								// then we can be sure that it is the start of this call.
								if (has(_vgwSessionIdToUdpChannelMap, key1) == false &&
									has(_vgwSessionIdToUdpChannelMap, key2) == false) {
									// We can now write a "Start of Call" indicator file in the
									// application's data directory. e-g: 5362954-call-started.txt
									//
									// If call start date time is not set for some reason, we can set it now.
									// For the actual calls coming through Voice Gateway, this must have been
									// already set inside the Voice Gateway source operator above.
									// This is simply defensive coding to address rare cases when
									// these two attributes arrive here as empty.
									if(BSD.callStartDateTime == "") {
										BSD.callStartDateTime = ctime(getTimestamp());
									}
									
									if(BSD.callStartTimeInEpochSeconds == 0l) {
										BSD.callStartTimeInEpochSeconds = getSeconds(getTimestamp());
									}
									
									// Insert the call start date time values in the state variables.
									insertM(_vgwSessionToCallStartDateTime, BSD.vgwSessionId, BSD.callStartDateTime);
									insertM(_vgwSessionToCallStartTimeInEpochSeconds, BSD.vgwSessionId, BSD.callStartTimeInEpochSeconds);
									
									rstring socsFileName = dataDirectory() + "/" +
										BSD.vgwSessionId + "-call-started.txt";
									uint64 fileHandle = fopen (socsFileName, "w+", err);
									
									if(err == 0) {
										fwriteString ("VGW call session id " + BSD.vgwSessionId + 
											" started at " + BSD.callStartDateTime + ".\n",
											fileHandle, err);
										fwriteString("VGW voice channel number=" +
											(rstring)BSD.vgwVoiceChannelNumber + 
											", Speech processor id=" + 
											(rstring)$idOfThisSpeechProcessor +
											", Speech engine id=" + (rstring)BSD.speechEngineId +
											", Result processor id=" + 
											(rstring)BSD.speechResultProcessorId + "\n",
											fileHandle, err);
										fclose(fileHandle, err);
									}
									
									appTrc(Trace.error, "A new voice call has started. vgwSessionId=" + BSD.vgwSessionId);
								} else if (has(_vgwSessionIdToUdpChannelMap, key1) == false ||
									has(_vgwSessionIdToUdpChannelMap, key2) == false) {
									// This means, one of the two voice channels in a 
									// given VGW session is just beginning to arrive here.
									// More specifically, the next voice channel for a given
									// VGW session is sending its very first speech data packet.
									// We can write some helpful details about it in the 
									// call started file.
									rstring socsFileName = dataDirectory() + "/" +
										BSD.vgwSessionId + "-call-started.txt";
									// Open the file in append mode.
									uint64 fileHandle = fopen (socsFileName, "a+", err);
									
									if(err == 0) {
										fwriteString("VGW voice channel number=" +
											(rstring)BSD.vgwVoiceChannelNumber + 
											", Speech processor id=" + 
											(rstring)$idOfThisSpeechProcessor +
											", Speech engine id=" + (rstring)BSD.speechEngineId +
											", Result processor id=" + 
											(rstring)BSD.speechResultProcessorId + "\n",
											fileHandle, err);
										fclose(fileHandle, err);
									}
								}
									
								// Insert into the state map for future reference.
								insertM(_vgwSessionIdToUdpChannelMap, 
									_key, mySpeechEngineId);									
							} // End of if (mySpeechEngineId == -1)
						} // End of if (has(_vgwSessionIdToUdpChannelMap, _key)

						appTrc(Trace.debug, "vgwSessionId=" + BSD.vgwSessionId +
							", isCustomerSpeechData=" + (rstring)BSD.isCustomerSpeechData +
							", vgwVoiceChannelNumber=" + (rstring)BSD.vgwVoiceChannelNumber +
							", speechDataFragmentCnt=" + (rstring)BSD.speechDataFragmentCnt +
							", totalSpeechDataBytesReceived=" + 
							(rstring)BSD.totalSpeechDataBytesReceived +
							", speechEngineId=" + (rstring)BSD.speechEngineId +
							", speechResultProcessorId=" + (rstring)BSD.speechResultProcessorId); 
						// Set the call start date time values to the tuple attributes.
						if(has(_vgwSessionToCallStartDateTime, BSD.vgwSessionId) == true) {
							BSD.callStartDateTime = _vgwSessionToCallStartDateTime[BSD.vgwSessionId];
							BSD.callStartTimeInEpochSeconds = _vgwSessionToCallStartTimeInEpochSeconds[BSD.vgwSessionId];
						}
						
						// Submit this tuple.
						submit(BSD, BSDF);
					} else {
						// The incoming tuple contains an End of Call Signal (EOCS).
						appTrc(Trace.error, "Received an EOCS at the speech processor id " +
							(rstring)$idOfThisSpeechProcessor + 
							". vgwSessionId=" + BSD.vgwSessionId +
							", voiceChannelNumber=" + (rstring)BSD.vgwVoiceChannelNumber);
						//		
						// Process the end of voice call signal.
						// Since there are two channels in every voice call,
						// those two channels will carry their own "End STT session"
						// message from the Voice Gateway. The logic below takes care of
						// handling two End of Call Signals for every voice call.
						//
						// Get the allocated STT engine id for a given 
						// vgwSessionId_vgwVoiceChannelNumber combo.
						// We should always have an STT engine id. If not, that is a 
						// case where the user didn't provision sufficient number of 
						// STT engines and there was no idle STT engine available for that 
						// given vgwSessionId_vgwVoiceChannelNumber combo. 
						// This situation can be avoided by starting the application with a 
						// sufficient number of STT engines needed for the anticipated 
						// maximum concurrent voice calls. A rule of thumb is to have 
						// two STT engines to process two speakers in every given 
						// concurrent voice call.
						if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
							// Let us send an empty blob to the WatsonSTT operator to indicate that
							// this speaker of a given voice call is done.
							_oTuple = (BinarySpeech_t){};
							// Copy the three input tuple attributes that must
							// match with that of the outgoing tuple.
							assignFrom(_oTuple, BSD);
							// Assign the STT engine id where this voice channel was
							// getting processed until now.
							_oTuple.speechEngineId = _vgwSessionIdToUdpChannelMap[_key];
							// We have to send this tuple to the result processor as well.
							_oTuple.speechResultProcessorId = 
								_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
							submit(_oTuple, BSDF);
							// We are now done with this vgwSessionId_vgwVoiceChannelNumber combo.
							removeM(_vgwSessionIdToUdpChannelMap, _key);
							// Add the STT engine id to this call completed map to be released later in the
							// following if block only after receiving EOCS for both the voice channels of this call.
							insertM(_vgwSessionToCompletedUdpChannelMap, _key, _oTuple.speechEngineId);
						}
	
						// Senthil added this if block on Feb/01/2020.
						if($numberOfEocsNeededForVoiceCallCompletion == 1) {
							// If the user configured this application to handle
							// only one EOCS to treat a voice call as completed, then we
							// will try to clean-up the other voice channel if it exists.
							mutable int32 otherVgwVoiceChannelNumber = 1;
							
							if(BSD.vgwVoiceChannelNumber == 1) {
								otherVgwVoiceChannelNumber = 2;
							}
							
							// Get the sessionId + channelNumber combo string.
							_key = BSD.vgwSessionId + "_" + (rstring)otherVgwVoiceChannelNumber;
							
							if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
								// Let us send an empty blob to the WatsonSTT operator to indicate that
								// this speaker of a given voice call is done.
								_oTuple = (BinarySpeech_t){};
								// Copy the three input tuple attributes that must
								// match with that of the outgoing tuple.
								assignFrom(_oTuple, BSD);
								// Override the following two attributes to reflect the other voice channel.
								// Flip this attribute value.
								if(_oTuple.isCustomerSpeechData == true) {
									_oTuple.isCustomerSpeechData = false;
								} else {
									_oTuple.isCustomerSpeechData = true;
								}
								
								_oTuple.vgwVoiceChannelNumber = otherVgwVoiceChannelNumber;
								
								// Assign the STT engine id where this voice channel was
								// getting processed until now.
								_oTuple.speechEngineId = _vgwSessionIdToUdpChannelMap[_key];
								// We have to send this tuple to the result processor as well.
								_oTuple.speechResultProcessorId = 
									_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
								submit(_oTuple, BSDF);
								// We are now done with this vgwSessionId_vgwVoiceChannelNumber combo.
								removeM(_vgwSessionIdToUdpChannelMap, _key);
								// Add the STT engine id to this call completed map to be released later in the
								// following if block only after receiving EOCS for both the voice channels of this call.
								insertM(_vgwSessionToCompletedUdpChannelMap, _key, _oTuple.speechEngineId);
							}
						}
						
						// Since this voice call is ending, let us release the STT result processor 
						// instance that was allocated above for this voice call.
						if (has(_vgwSessionToResultProcessorChannelMap, 
							BSD.vgwSessionId) == true) {
							// Let us remove the result processor id only after the logic
							// in the previous if-block took care of sending the EOCS for 
							// both the voice channels in a given voice call.
							rstring key1 = BSD.vgwSessionId + "_" + "1";
							rstring key2 = BSD.vgwSessionId + "_" + "2";
							
							// Remove the result processor id only if the EOCS signal
							// was sent for both of the voice channels. That must first 
							// happen before we can release the result processor id.
							//
							// This if condition was changed by Senthil on 
							// Feb/01/2021 for the following reason.
							// If the user configured this application to handle
							// a single EOCS as sufficient to consider a voice call
							// completed for a given VGW session id, we will use the 
							// second || i.e. OR condition. Please refer to the 
							// constant i.e. expression declaration section above to 
							// read the commentary about this idea.
							//
							if (($numberOfEocsNeededForVoiceCallCompletion == 2 &&
								(has(_vgwSessionIdToUdpChannelMap, key1) == false &&
								has(_vgwSessionIdToUdpChannelMap, key2) == false)) || 
								($numberOfEocsNeededForVoiceCallCompletion == 1 &&
								(has(_vgwSessionIdToUdpChannelMap, key1) == false ||
								has(_vgwSessionIdToUdpChannelMap, key2) == false))) {								
								// Since the same result processor is used for both the
								// voice channels in a call, there will be a single
								// result processor UDP channel allocated for a given voice call.
								int32 resultProcessorUdpChannelToBeReleased = 
									_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
								removeM(_vgwSessionToResultProcessorChannelMap, BSD.vgwSessionId);
								// We can go ahead and release the result processor that was
								// used by this voice call.
								appendM(_idleResultProcessorUdpChannelsList, 
									resultProcessorUdpChannelToBeReleased);
								
								// Since the voice call for this VGW session id has ended completely,
								// we can also release the STT engine(s) assigned for this call so that 
								// they can be repurposed for handling any new future calls.
								// We can go ahead and release the STT engine by adding it back to 
								// the idle UDP channels list.
								if(has(_vgwSessionToCompletedUdpChannelMap, key1) == true) {
									appendM(_idleUdpChannelsList, _vgwSessionToCompletedUdpChannelMap[key1]);
									// We are done. Remove it from the map as well.
									removeM(_vgwSessionToCompletedUdpChannelMap, key1);
								}
	
								if(has(_vgwSessionToCompletedUdpChannelMap, key2) == true) {
									appendM(_idleUdpChannelsList, _vgwSessionToCompletedUdpChannelMap[key2]);
									// We are done. Remove it from the map as well.
									removeM(_vgwSessionToCompletedUdpChannelMap, key2);
								}

								// Remove the call start date time values from the state variables.
								if(has(_vgwSessionToCallStartDateTime, BSD.vgwSessionId) == true) {
									removeM(_vgwSessionToCallStartDateTime, BSD.vgwSessionId);
									removeM(_vgwSessionToCallStartTimeInEpochSeconds, BSD.vgwSessionId);
								}

								// At this time, the voice call for this VGW session id has ended.
								// We can now write an "End of Call" indicator file in the
								// application's data directory. e-g: 5362954-call-completed.txt
								mutable int32 err = 0ul;
								rstring eocsFileName = dataDirectory() + "/" +
									BSD.vgwSessionId + "-call-completed.txt";
								uint64 fileHandle = fopen (eocsFileName, "w+", err);
								
								if(err == 0) {
									fwriteString ("VGW call session id " + BSD.vgwSessionId + 
										" ended at " + ctime(getTimestamp()) + ".", fileHandle, err);
									fclose(fileHandle, err);
								}
								
								appTrc(Trace.error, "An ongoing voice call has completed. vgwSessionId=" + BSD.vgwSessionId);
							}
						}
					} // End of if(BSD.endOfCallSignal == false)
				} // End of onTuple BSD
				
			config
				threadedPort: queue(BSD, Sys.Wait);
		} // End of Custom operator.

		// IMPORTANT: IBM STT service on public cloud requires
		// an unexpired valid IAM access token to perform the 
		// speech to text task in a secure manner. One way to meet this
		// requirement is to invoke and use a non-main composite operator
		// that is available as part of the streamsx.sttgateway. By invoking
		// that composite operator, we can make it to generate a new 
		// access token and then periodically refresh it. This composite
		// operator expects three operator parameters for you to
		// provide at the time of invoking it i.e. your
		// STT service instance's API key, IAM token generation/refresh URL and
		// the required IAM access token refresh interval.
		// Output stream of this composite operator is connected to the
		// second input stream of the WatsonSTT operator that is used below.
		// If the sttAPIKey parameter below is set to an empty string,
		// this composite will skip generating an IamAccessToken.
		// For a correct STT operation, user must set only one of these two
		// submission time parameters to a non-empty value: sttAPIKey or sttOnCP4DAccessToken.
		(stream<IAMAccessToken> IamAccessToken as IAT)
			as IamAccessTokenGenerator = IAMAccessTokenGenerator() {
			param
				// This operator takes these four parameters.
				apiKey: $sttApiKey;
				iamTokenURL: $sttIAMTokenURL;
				accessToken: $sttOnCP4DAccessToken;
				// It is possible to set the param values in the
				// IBM Streams app config. If it is done that way,
				// we can pass the app config name here. 
				// If not, pass and empty string.
				appConfigName: "";
		}

		// Invoke one or more instances of the IBMWatsonSpeechToText composite operator.
		// You can send the audio data to this operator all at once or 
		// you can send the audio data for the live-use case as it becomes
		// available from your telephony network switch.
		// Avoid feeding audio data coming from more than one data source into this 
		// parallel region which may cause erroneous transcription results.
		//
		// NOTE: The WatsonSTT operator allows fusing multiple instances of
		// this operator into a single PE. This will help in reducing the 
		// total number of CPU cores used in running the application.
		// First input stream into this operator is the audio blob content.
		// Second input stream into this operator is your STT service instance's IAM access token.
		// Please note that we are using an int32 value (exactly in the
		// range of the available number of STT engines) as our
		// UDP partition key. That will help us to always use that
		// preselected partition for a given voice call thereby 
		// avoiding cross talk and mix up of data from multiple 
		// voice calls landing in a given parallel channel etc.
		@parallel(width = $numberOfSTTEngines, 
		partitionBy=[{port=BSDF, attributes=[speechEngineId]}], broadcast=[IAT])
		(stream<MySTTResult_t> MySTTResult) as SpeechToText = 
			IBMWatsonSpeechToText(BinarySpeechDataFragment as BSDF;
			IamAccessToken as IAT) {
			// If needed, you can decide not to fuse the WatsonSTT operator instances and
			// keep each instance of this operator on its own PE (a.k.a Linux process) by
			// activating this config clause.
			//
			config
				placement : partitionExlocation("sttpartition");
		}

		// Let us invoke the configured number of STT result processors.
		// Please note that we are using an int32 value (exactly half
		// the available number of STT engines) as our
		// UDP partition key. That will help us to always use that
		// preselected partition for a given voice call thereby 
		// avoiding cross talk and mix up of data from multiple 
		// voice calls landing in a given parallel channel etc.
		@parallel(width = $numberOfResultProcessors, 
		partitionBy=[{port=MSR, attributes=[speechResultProcessorId]}])
		() as STTResultProcessorSink = STTResultProcessor(MySTTResult as MSR) {
		}
		
	config
       // To learn about how to create host tags and assign them to the machines in
       // your Streams instance, you can read tips-on-using-streams-host-tags.txt available 
       // in the SPL-Examples-For-Beginners/047_streams_host_tags_at_work/host.tags directory.
       hostPool: 
          spPool = createPool({tags=["SpeechProcessor"]}, Sys.Shared); /*Sys.Exclusive*/
       // Place the operators in this composite only on specifically tagged machine(s) present in the spPool.
       placement: host(spPool);
} // End of the main composite.

// Following is a composite where we are going to perform the
// logic to invoke the WatsonSTT operator for doing the
// Speech To Text transcription.
public composite IBMWatsonSpeechToText(input AudioBlobContent, AccessToken;
	output STTResult) {
	param
		// wss://api.{location}.speech-to-text.watson.cloud.ibm.com/instances/{instance_id}/v1/recognize
		expression<rstring> $sttUri : getSubmissionTimeValue("sttUri");
		expression<rstring> $sttBaseLanguageModel : 
			getSubmissionTimeValue("sttBaseLanguageModel", "en-US_Telephony");
		expression<rstring> $contentType : 
			getSubmissionTimeValue("contentType", "audio/wav");
		expression<rstring> $baseModelVersion : 
			getSubmissionTimeValue("baseModelVersion", "");
		expression<rstring> $customizationId : 
			getSubmissionTimeValue("customizationId", "");
		expression<rstring> $acousticCustomizationId : 
			getSubmissionTimeValue("acousticCustomizationId", "");
		expression<float64> $customizationWeight : 
			(float64)getSubmissionTimeValue("customizationWeight", "0.30");
		expression<int32> $maxUtteranceAlternatives : 
			(int32)getSubmissionTimeValue("maxUtteranceAlternatives", "1");	
		expression<boolean> $sttRequestLogging : 
			(boolean)getSubmissionTimeValue("sttRequestLogging", "false");
		expression<boolean> $filterProfanity : 
			(boolean)getSubmissionTimeValue("filterProfanity", "false");
		expression<float64> $wordAlternativesThreshold : 
			(float64)getSubmissionTimeValue("wordAlternativesThreshold", "0.0");
		expression<boolean> $smartFormattingNeeded : 
			(boolean)getSubmissionTimeValue("smartFormattingNeeded", "false");
		expression<boolean> $redactionNeeded : 
			(boolean)getSubmissionTimeValue("redactionNeeded", "false");
		// Allowed value range for this is from 0.0 to 1.0.
		expression<float64> $speechDetectorSensitivity : 
			(float64)getSubmissionTimeValue("speechDetectorSensitivity", "0.5");
		// Allowed value range for this is from 0.0 to 1.0.
		expression<float64> $backgroundAudioSuppression : 
			(float64)getSubmissionTimeValue("backgroundAudioSuppression", "0.0");
		// Allowed value range for this is from -0.5 to 1.0.	
		expression<float64> $characterInsertionBias : 
			(float64)getSubmissionTimeValue("characterInsertionBias", "0.0");
		expression<float64> $keywordsSpottingThreshold : 
			(float64)getSubmissionTimeValue("keywordsSpottingThreshold", "0.0");
		expression<list<rstring>> $keywordsToBeSpotted : 
			(list<rstring>)getSubmissionTimeValue("keywordsToBeSpotted", "[]");	
		expression<boolean> $sttWebsocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("sttWebsocketLoggingNeeded", "false");
		expression<float64> $cpuYieldTimeInAudioSenderThread : 
			(float64)getSubmissionTimeValue("cpuYieldTimeInAudioSenderThread", "0.001");
		expression<boolean> $sttLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("sttLiveMetricsUpdateNeeded", "true");

	graph
		// For the purpose of scale testing the end to end Voice call data flow,
		// we can use the Voice Data Simulator available in the streamsx.sttgateway
		// toolkit to generate hundreds of voice calls and send it through the
		// VgwDataRouter-->VgwDataRouterToWatsonSTT applications. However, to do
		// such large scale testing, it may take time and effort to setup an
		// STT test infrastructure. If that is doable, it is great. But, in the 
		// absence of a large scale STT environment, we can activate the following
		// Custom operator to return dummy utterances. When we uncomment this
		// Custom operator code block, it is necessary to comment out the
		// WatsonSTT operator code block below to avoid duplicate tuple processing
		// logic as well as potential output tuple collisions and errors.
		//
		// Use this operator only for scale testing. Otherwise, keep this
		// operator code block commented out.
		//
		/*
		stream<MySTTResult_t> STTResult = 
			Custom(AudioBlobContent as ABC; AccessToken as AT) {
			logic
				state: {
					mutable int32 _conversationCnt = 0;
					mutable rstring _conversationId = "";
					mutable rstring _key = "";
					mutable int32 _utteranceNumber = 0;
					mutable MySTTResult_t _oTuple = {};
				}
				
				onTuple ABC: {
					_key = ABC.vgwSessionId + "_" + 
						(rstring)ABC.vgwVoiceChannelNumber;
						
					if (_conversationId != _key) {
						// There may be many blob fragments arriving for a given audio conversation.
						// So, display only when the very first blob fragment for a given audio arrives.
						_conversationId = _key;
						appTrc(Trace.error, "Channel " + (rstring)getChannel() + 
							", Speech input " + (rstring)++_conversationCnt +
							": " + _conversationId);
						// It is a new voice call. Reset the utterance number.
						_utteranceNumber = 0;
					}
					
					// Copy the matching attributes from the incoming tuple.
					assignFrom(_oTuple, ABC);
					// Set the utterance number and utterance text attributes.
					_oTuple.utteranceNumber = ++_utteranceNumber;
					_oTuple.utteranceText = "Utterance " + (rstring)_utteranceNumber;
					_oTuple.finalizedUtterance = true;
					submit(_oTuple, STTResult);
				}
				
				onTuple AT: {
					// Do nothing.
					return;
				}
		}
		*/

		// Remember to comment out this operator code block if the 
		// Custom operator code block above is uncommented for
		// scale testing the end-to-end voice call data flow when 
		// a suitable STT test environment is not in place.
		//
		stream<MySTTResult_t> STTResult = 
			WatsonSTT(AudioBlobContent as ABC; AccessToken as AT) {
			logic
				state: {
					mutable int32 _conversationCnt = 0;
					mutable rstring _conversationId = "";
					mutable rstring _key = "";
				}
				
				onTuple ABC: {
					_key = ABC.vgwSessionId + "_" + 
						(rstring)ABC.vgwVoiceChannelNumber;
						
					if (_conversationId != _key) {
						// There may be many blob fragments arriving for a given audio conversation.
						// So, display only when the very first blob fragment for a given audio arrives.
						_conversationId = _key;
						appTrc(Trace.error, "Channel " + (rstring)getChannel() + 
							", Speech input " + (rstring)++_conversationCnt +
							": " + _conversationId);
					}
				}

			// Just to demonstrate, we are using all the operator parameters below.
			// Except for the first three parameters, every other parameter is an
			// optional one. In real-life applications, such optional parameters
			// can be omitted unless you want to change the default behavior of them.				
			param
				uri: $sttUri;
				baseLanguageModel: $sttBaseLanguageModel;
				contentType: $contentType;
				sttResultMode: partial;
				sttRequestLogging: $sttRequestLogging;
				filterProfanity: $filterProfanity;
				maxUtteranceAlternatives: $maxUtteranceAlternatives;
				wordAlternativesThreshold: $wordAlternativesThreshold;
				smartFormattingNeeded: $smartFormattingNeeded;
				redactionNeeded: $redactionNeeded;
				speechDetectorSensitivity: $speechDetectorSensitivity;
				backgroundAudioSuppression: $backgroundAudioSuppression;
				characterInsertionBias: $characterInsertionBias;
				keywordsSpottingThreshold: $keywordsSpottingThreshold;
				keywordsToBeSpotted: $keywordsToBeSpotted;
				websocketLoggingNeeded: $sttWebsocketLoggingNeeded;
				cpuYieldTimeInAudioSenderThread: $cpuYieldTimeInAudioSenderThread;
				sttLiveMetricsUpdateNeeded : $sttLiveMetricsUpdateNeeded;
								
				// Use the following operator parameters as needed.
				// Point to a specific version of the base model if needed.
				//
				// e-g: "en-US_NarrowbandModel.v07-06082016.06202016"
				baseModelVersion: $baseModelVersion;
				// Language model customization id to be used for the transcription.
				// e-g: "74f4807e-b5ff-4866-824e-6bba1a84fe96"
				customizationId: $customizationId;
				// Acoustic model customization id to be used for the transcription.
				// e-g: "259c622d-82a4-8142-79ca-9cab3771ef31"
				acousticCustomizationId: $acousticCustomizationId;
				// Relative weight to be given to the words in the custom Language model.
				customizationWeight: $customizationWeight;

			// Just for demonstrative purposes, we are showing below the output attribute
			// assignments using all the available custom output functions. In your
			// real-life applications, it is sufficient to do the assignments via
			// custom output functions only as needed.
			//
			// Some of the important output functions that must be used to check
			// the result of the transcription are:
			// getSTTErrorMessage --> It tells whether the transcription succeeded or not.
			// isFinalizedUtterance --> In sttResultMode partial, it tells whether this is a 
			//                          partial utterance or a finalized utterance.
			// isTranscriptionCompleted --> It tells whether the transcription is 
			//                              completed for the current audio conversation or not.
			//
			output
				STTResult: 
					utteranceNumber = getUtteranceNumber(),
					utteranceText = getUtteranceText(),
					finalizedUtterance = isFinalizedUtterance(),
					confidence = getConfidence(),
					sttErrorMessage = getSTTErrorMessage(),
					transcriptionCompleted = isTranscriptionCompleted(),
					// n-best utterance alternative hypotheses.
					utteranceAlternatives = getUtteranceAlternatives(),
					// Confusion networks (a.k.a. Consensus)
					wordAlternatives = getWordAlternatives(),
					wordAlternativesConfidences = getWordAlternativesConfidences(),
					wordAlternativesStartTimes = getWordAlternativesStartTimes(),
					wordAlternativesEndTimes = getWordAlternativesEndTimes(),
					utteranceWords = getUtteranceWords(),
					utteranceWordsConfidences = getUtteranceWordsConfidences(),
					utteranceWordsStartTimes = getUtteranceWordsStartTimes(),
					utteranceWordsEndTimes = getUtteranceWordsEndTimes(),
					utteranceStartTime = getUtteranceStartTime(),
					utteranceEndTime = getUtteranceEndTime(),
					// Speaker label a.k.a. Speaker id
					utteranceWordsSpeakers = getUtteranceWordsSpeakers(),
					utteranceWordsSpeakersConfidences = getUtteranceWordsSpeakersConfidences(),
					// Results from keywords spotting (matching) in an utterance.
					keywordsSpottingResults = getKeywordsSpottingResults();
		}
} // End of the composite IBMWatsonSpeechToText

// Following is a sink composite where we are going to process the
// STT result of a given voice call in specific ways such as
// storing in files, message queues, databases or make it 
// accessible from a web application.
// Please note that this composite has its own parallel region.
public composite STTResultProcessor(input MyTranscriptionResult) {
	param
		// This submission time value decides whether to write the
		// transcription results to files or not.
		expression<boolean> $writeTranscriptionResultsToFiles : (boolean)
			getSubmissionTimeValue("writeTranscriptionResultsToFiles", "true");

		// This submission time value decides whether to send the
		// transcription results to an outside application either
		// via Export or via WebSocket.
		expression<boolean> $sendTranscriptionResultsToOutsideApplication : (boolean)
			getSubmissionTimeValue("sendTranscriptionResultsToOutsideApplication", "true");
		
		// Do you want to include the utterance result reception time?
		expression<boolean> $includeUtteranceResultReceptionTime :
			(boolean)getSubmissionTimeValue("includeUtteranceResultReceptionTime", "false");		
			
	graph
		// In a real-life application, there will be additional operators here with the 
		// necessary logic to look inside the tuples arriving on the STTResult stream and
		// analyze different kinds of speech to text result attributes returned from the STT service.
		// 
		// But, in this simple example we will only collect the results 
		// arriving from the WatsonSTT operator and write along with
		// all the STT related attributes to individual files.
		// As mentioned in several code blocks above, you can simply reuse all
		// the code provided in this example file as it is except for this composite/
		// You can feel free to make any code changes to perform 
		// specific analytics on the STT results as well as make changes
		// to store the STT results elsewhere instead of files or 
		// send the STT results to other downstream systems such as your
		// web dashboarding applications.
		(stream<MyTranscriptionResult> TranscriptionResultForWritingToFile as TRFWTF;
			stream<MyTranscriptionResult> SendTranscriptionResultToOutsideApp as STRTOA) = 
			Custom(MyTranscriptionResult as MTR) {
			logic
				state: {
					mutable rstring _vgwSessionId = "";
					mutable rstring _ciscoGuid = "";
					mutable rstring _callStartDateTime = "";
					mutable int64 _callStartTimeInEpochSeconds = 0l;
				}

				onTuple MTR: {
                    // In some cases, it was observed during testing that the 
                    // call start time details arrive here with empty and 0 values.
                    // To compensate for that, we are going to keep a local copy
                    // of the non-empty and non-zero value of them for correct
                    // calculation of the utterance generation time with reference to
                    // the start time of a given voice call.
                    // We will do it whenever a brand new VGW session id arrives here by
                    // meeting the other conditions included in this statement.
                    if(MTR.vgwSessionId != _vgwSessionId && 
                       MTR.callStartDateTime != "" &&
                       MTR.callStartTimeInEpochSeconds != 0l) {
                       _vgwSessionId = MTR.vgwSessionId;
                       _ciscoGuid = MTR.ciscoGuid;
                       _callStartDateTime = MTR.callStartDateTime;
                       _callStartTimeInEpochSeconds = MTR.callStartTimeInEpochSeconds;
                    }

                    // If we notice any empty or zero values for the call start time details,
                    // We will fill it from the ones we saved in the state variables.
                    if(MTR.ciscoGuid == "") {
                       MTR.ciscoGuid = _ciscoGuid;
                    }

                    if(MTR.callStartDateTime == "") {
                       MTR.callStartDateTime = _callStartDateTime;
                    }

                    if(MTR.callStartTimeInEpochSeconds == 0l) {
                       MTR.callStartTimeInEpochSeconds =  _callStartTimeInEpochSeconds;
                    }

					// If the user opted for including the time of the
					// utterance result reception time, let us add it to
					// the transcription result.
					if($includeUtteranceResultReceptionTime == true) {
						// This is the current time expressed in ctime format.
						MTR.utteranceResultReceptionTime = ctime(getTimestamp());
						// We will also do the utterance reception time expressed
						// in seconds elapsed since the start of the call.
						MTR.utteranceRxTime = 
							getSeconds(getTimestamp()) - MTR.callStartTimeInEpochSeconds;
					}
					
					// We will write the transcription results to 
					// files if the user wanted it that way.
					if ($writeTranscriptionResultsToFiles == true) {
						submit(MTR, TRFWTF);
					} 
					
					// If we have to send the utterances to an outside application,
					// let us do it now.
					if($sendTranscriptionResultsToOutsideApplication == true) {
						submit(MTR, STRTOA);
					}	
				}
		}
		
		// If we have to send the utterances to an outside application,
		// replace this operator with an Export operator. 
		// But, exporting from multiple speech processors may not be
		// the right approach. A preferred and better approach would be
		// to use a WebSocketSendReceive operator to send the utterances to be 
		// received on the other end by a WebSocketSource operator. 
		// That approach will scale well without putting more stress on the 
		// Streams instance since the data transfer is done outside the 
		// confines of the Streams runtime. If more details are needed about 
		// this preferred approach, please contact the author of this code for help.
		// 
		// For now, we will have a dummy Custom sink here.
		() as NoOpSink = Custom(SendTranscriptionResultToOutsideApp as STRTOA) {
			logic
				onTuple STRTOA: {
					// Do nothing here.
					// Please read the commentary above this operator to
					// follow a suitable approach to make use of this incoming tuple.
					return;
				}
		}
		
		() as MySink1 = FileSink(TranscriptionResultForWritingToFile as TRFWTF) {
			param
				// This file will contain a comprehensive set of full STT results.
				file: TRFWTF.vgwSessionId + "-full-result.txt";
				closeMode: dynamic;
				quoteStrings: false;
				append: true;
				flush: 1u;

			config
				threadedPort: queue(TRFWTF, Sys.Wait);
		}
		
		// In this operator, we will filter only the utterances and send it 
		// to the downstream operator to be written to the individual files.
		(stream<rstring vgwSessionId,
		 rstring ciscoGuid, 
		 boolean isCustomerSpeechData,
		 int32 vgwVoiceChannelNumber,
		 rstring callStartDateTime, 
		 rstring utteranceResultReceptionTime,
		 int64 utteranceRxTime, 
		 rstring utteranceText> Utterance as U) 
			as UtteranceFilter = Custom(TranscriptionResultForWritingToFile as TRFWTF) {
			logic
				state: {
					mutable Utterance _oTuple = {};
				}
					
				onTuple TRFWTF: {
					if (TRFWTF.finalizedUtterance == true) {
						// There is no need to send partially analyzed utterances.
						// Send it only if it is a finalized utterance.
						assignFrom(_oTuple, TRFWTF);
						submit(_oTuple, U);
					}
				}
				
			config
				threadedPort: queue(TRFWTF, Sys.Wait);
		}

		// Write only the utterances to a file.
		() as MySink2 = FileSink(Utterance as U) {
			param
				// This file will contain only a small subset of the
				// STT results (Unique call id, channel number, caller or agent, utterance).
				file: U.vgwSessionId + "-utterance-result.txt";
				closeMode: dynamic;
				quoteStrings: false;
				append: true;
				flush: 1u;

			config
				threadedPort: queue(U, Sys.Wait);
		}
} // End of the composite STTResultProcessor

// Following are the common utility functions for 
// use within the composites declared above.
//
// This function validates if the number of STT engines configured is correct.
public stateful boolean validateSpeechEnginesConfiguration(int32 numberOfSTTEngines) {
	// Check if there is an even number of speech engines configured for each speech processor job.
	if(numberOfSTTEngines % 2 != 0) {
		// This is not a correct configuration. Let us abort now.
		appTrc(Trace.error, 
			"_XXXXX Incorrect configuration for the number of speech engines in this speech processor job. " + 
			"There should be an even number of speech engines running in every speech processor job.");
		abort();
	}
	
	return(true);
}

// This function validates if the number of result processors configured is correct.
public stateful boolean validateResultProcessorConfiguration(int32 numberOfSTTEngines,
	int32 numberOfResultProcessors) {
	// Check if there is an even number of speech engines configured for each speech processor job.
	if(numberOfSTTEngines % 2 != 0) {
		// This is not a correct configuration. Let us abort now.
		appTrc(Trace.error, 
			"_YYYYY Incorrect configuration for the number of speech engines in this speech processor job. " + 
			"There should be an even number of speech engines running in every speech processor job.");
		abort();
	}

	// Check if the number of result processors configured is exactly half the
	// size of the total speech engines configured for each speech processor job.
	int32 expectedNumberOfResultProcessors = numberOfSTTEngines / 2;
		
	if(numberOfResultProcessors != expectedNumberOfResultProcessors) {
		// This is not a correct configuration. Let us abort now.
		appTrc(Trace.error, 
			"_YYYYY Incorrect configuration found for the number of result processors. " + 
			"Your total number of result processors should be configured to " +
			(rstring)expectedNumberOfResultProcessors + ".");
		abort();
	}
	
	return(true);
}

// This function creates a new list with all the UDP channel numbers in it. 
// That means, all such channels are idle at this time. This list holds the
// idle UDP channels for the STT engines. 
public list<int32> prepareIdleUdpChannelsList(int32 sttEngineCount) {
	mutable list<int32> myList = [];
	
	for(int32 idx in range(sttEngineCount)) {
		appendM(myList, idx);
	}
	
	return(myList);
}

// This function takes the idle UDP channels list as an input and
// returns an idle UDP channel number from the top of the list.
// If all are busy at this time, it will return -1.
public int32 getAnIdleUdpChannel(mutable list<int32> myList) {
	if (size(myList) > 0) {
		// Get the channel number available at the very top of the list.
		int32 channelNumber = myList[0];
		// Remove the topmost channel number.
		removeM(myList, 0);
		return(channelNumber);
	} else {
		// There is no idle UDP channel number available at this time.
		return(-1);
	}
}

// This function creates a new list with all the result processor 
// UDP channel numbers in it. That means, all such channels are idle to begin with. 
public list<int32> prepareIdleResultProcessorUdpChannelsList(int32 resultProcessorCnt) {
	mutable list<int32> myList = [];
	
	for(int32 idx in range(resultProcessorCnt)) {
		appendM(myList, idx);
	}
	
	return(myList);
}
