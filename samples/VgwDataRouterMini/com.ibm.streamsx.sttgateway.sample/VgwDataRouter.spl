/*
==============================================
# Licensed Materials - Property of IBM
# Copyright IBM Corp. 2018, 2022
==============================================
*/

/*
==============================================
First created on: Nov/24/2020
Last modified on: May/10/2022

A) What does this example application do?
   --------------------------------------
This example demonstrates the integration of the following three products to
achieve Real-Time Speech-To-Text transcription to get the text ready for
any further analytics. The task of receiving the voice calls' speech data 
from the Voice Gateway product and distributing it to one or more 
speech processor jobs is solely done by this VgwDataRouter application. 

1) IBM Voice Gateway v1.0.3.0 or higher
2) IBM Streams v4.2.1.6 or higher
3) IBM Watson Speech 2 Text (Embedded in an IBM Streams WatsonS2T operator v2.12.0) (OR)
   IBM Watson Speech To Text running on IBM public cloud or on IBM Cloud Pak for Data.

These three components will work in the following sequence:

Your Telephony SIPREC-->IBM Voice Gateway-->IBM Streams-->WatsonS2T Operator
(OR)
Your Telephony SIPREC-->IBM Voice Gateway-->IBM Streams-->WatsonSTT Operator

Note
----
This is a miniature version of a similarly named application that can be found
in the samples directory of the streamsx.sttgateway toolkit. In this mini version,
the application logic is trimmed so that it will not have other features such as 
call replay, call recording, additional logging of utterances/full results,
sending utterance results via HTTP etc. If you have a need for such features,
please use the other non-miniature version.

B) Other IBM Streams toolkits you will need for this application
   -------------------------------------------------------------
1) You must first ensure that your Streams machine where you will be compiling
this example application has these toolkits fully built and ready. 
This application has a dependency on these toolkits.
   a) streamsx.sttgateway toolkit (v2.2.3 or higher)
      https://github.com/IBMStreams/streamsx.sttgateway
   b) com.ibm.streamsx.websocket (v1.0.6 or higher)
      https://github.com/IBMStreams/streamsx.websocket
   c) STTGatewayUtils
      --> This is another SPL project available in the samples directory of
          the streamsx.sttgateway toolkit.
         
C) Building this example application
   ---------------------------------
You can build this example in a Linux terminal window via the make command by using the
Makefile available in the top-level directory of this example. Please follow
these steps to build this application:

1) streamsx.sttgateway toolkit also needs a specific version of the C++ boost library and 
the C++ websocketpp library. Please follow the instructions available in the documentation of 
that toolkit to download and prepare those libraries via the ant tool before you can 
compile this example application.

2) You have to either set the following environment variables on a terminal window where
you will run the make command or you can edit the Makefile and change these toolkit directories
to point to the correct location on your IBM Streams machine. 

   a) It will be necessary to export the STREAMS_STTGATEWAY_TOOLKIT (v2.2.5 or higher) environment variable by
      pointing it to the full path of your com.ibm.streamsx.sttgateway directory. 

   b) You have to export the STREAMS_WEBSOCKET_TOOLKIT (v1.0.6 or higher) and point to the correct directory.

   c) You have to export the STT_GATEWAY_UTILS and point it to the STTGatewayUtils directory available in
      the samples directory of the streamsx.sttgateway toolkit.

   d) If you don't want to set the above-mentioned environment variables every time you want to
      build this application, you can also edit the Makefile and enter these directories there directly.

3) If you prefer to build this example inside the Streams Studio instead of the command line based
Makefile, there are certain build configuration settings needed. Please refer to the streamsx.sttgateway
toolkit documentation to learn more about those Streams Studio configuration settings.

D) Running this example application
   --------------------------------
1) In order to run this example, you must first configure the IBM Voice Gateway v1.0.3.0 or higher 
to send the binary speech data to this IBM Streams application. That involves configuring the 
IBM Voice Gateway media relay and the IBM Voice Gateway SIP integrator components and then
deploying them. Please refer to the streamsx.sttgateway documentation section titled 
"Requirements for this toolkit".

2) Once you are sure that the IBM Voice Gateway can send the speech data to this
application, then you can give the following command to deploy this application in distributed mode.

There are certain important submission time parameters that are required while
launching this application.

Mandatory submission time parameter(s) for the VgwDataRouter application:
Total number of speech processor jobs. e-g: -P totalNumberOfSpeechProcessorJobs=10
Number of Speech Engines in each speech processor job. e-g:  -P numberOfSpeechEnginesPerSpeechProcessorJob=30
TLS port number for VGW to send speech data to. e-g: -P tlsPortForVgwDataRx=8443
TLS port number for STT jobs to connect to for receiving speech data. e-g: -P tlsPortForVgwDataTx=8444

It would be better if we can make the IBMVoiceGatewaySource and the WebSocketSink (both WebSocket based)
server operators in this application to always land in the same machine in the IBM Streams cluster.
By doing that, we can make sure that the speech processor jobs can be scripted to point to
the well known host name where the WebSocketSink operator is running. This can be done via the
Job Configuration Overlay (JCO) file at the time of launching this application. In order to
achieve this function, you have to create a host tag VgwDataRouter on any one of your IBM Streams
cluster machines. Please look for more details in the etc/run-vgw-data-router.sh.

st submitjob -d  <YOUR_STREAMS_DOMAIN>  -i  <YOUR_STREAMS_INSTANCE>  -g etc/VgwDataRouter.jco.json -P tlsPortForVgwDataRx=8443 -P tlsPortForVgwDataTx=8444 -P numberOfSpeechEnginesPerSpeechProcessorJob=30 -P totalNumberOfSpeechProcessorJobs=10 -P vgwSessionLoggingNeeded=false -P ipv6Available=false -C fusionScheme=legacy com.ibm.streamsx.sttgateway.sample.VgwDataRouter.sab

3) After starting this application, now you must go ahead and launch either the VgwDataRouterToWatsonS2T or
the VgwDataRouterToWatsonSTT application one or more times to exactly match with the number that you
specified for the -P TotalNumberOfSpeechProcessorJobs submission time parameter in the above-mentioned command.
Please read the detailed commentary available at the top of those applications to learn more about
how to launch those applications correctly.

E) Running this example without IBM Voice Gateway
   ----------------------------------------------
You can use the VoiceDataSimulator test application from the samples directory to emulate Voice Gateway.
In this case the VoiceDataSimulator must read audio files with 8-bit pcm u-law samples.
You can convert the WAV files in the samples/audio-files directory of the sttgateway 
toolkit using the detailed instructions available in the 
etc/Steps-To-Convert-WAV-To-MuLaw.txt file which can be found in the other major
Streams applications present in the samples directory.
==============================================
*/
namespace com.ibm.streamsx.sttgateway.sample;

// This namespace contains the IBMVoiceGatewaySource operator that we use below.
use com.ibm.streamsx.sttgateway.watson::*;
// A few  C++ native functions are used from this namespace.
use com.ibm.streamsx.sttgateway.utils::*;
use spl.file::*;

// WebSocketSink comes from this namespace..
use com.ibm.streamsx.websocket.op::*;

// The following block of commentary is needed for the spldoc creation during the ant build process.
/**
 * 
 * What does this example application do?
 * 
 *  * This example demonstrates the integration of the following three products to
 * achieve Real-Time Speech-To-Text transcription to get the text ready for
 * any further analytics.
 * 
 * 1) IBM Voice Gateway v1.0.3.0 or higher
 * 2) IBM Streams v4.2.1.6 or higher
 * 3) IBM Watson Speech 2 Text (Embedded in an IBM Streams WatsonS2T operator v2.12.0) (OR)
 *    IBM Watson Speech To Text running on IBM public cloud or on IBM Cloud Pak for Data.
 * 
 * @param	tlsPortForVgwDataRx	TLS port on which this application will listen for receiving speech data from the IBM Voice Gateway.
 * 
 * @param	tlsPortForVgwDataTx	TLS port on which this application will accept connections from the specch processor jobs to distribute the sppech data.
 * 
 * @param	totalNumberOfSpeechProcessorJobs	Specify the number of speech processor jobs to which this application must route/distribute the speech data to.
 *
 * @param	numberOfSpeechEnginesPerSpeechProcessorJob	Specify the number of speech engines configured to run in each speech processor job.
 
 * @param	nonTlsEndpointNeeded	User can optionally specify whether they want a non-TLS endpoint.
 * 
 * @param	nonTlsPort	Non-TLS (Plain) port on which this application will (optionally) listen for communicating with the IBM Voice Gateway.
 * 
 * @param	certificateFileName		Server side certificate (.pem) file for the WebSocket server. 
 * It is necessary for the users to create a Root CA signed 
 * server side certificate file and point to that file at the time of 
 * starting this application. If the user doesn't point to this file 
 * at the time of starting the application, then the application will 
 * look for a default file named ws-server.pem inside the etc sub-directory 
 * of the application. This certificate will be presented to the 
 * IBM Voice Gateway for validation when it establishes a WebSocket 
 * connection with this application. For doing quick tests, you may save 
 * time and effort needed in getting a proper Root CA signed certificate 
 * by going with a simpler option of creating your own self-signed 
 * certificate. Please ensure that using a self-signed certificate is 
 * allowed in your environment. We have provided a set of instructions to 
 * create a self signed certificate. Please refer to the following 
 * file in the etc sub-directory of this application: 
 * etc/creating-a-self-signed-certificate.txt
 * 
 * @param   certificatePassword This parameter specifies a password needed for decrypting the WebSocket server's private key in the PEM file. Default is an empty string.
 *
 * @param	vgwLiveMetricsUpdateNeeded	Is live metrics needed for the IBMVoiceGatewaySource operator?
 * 
 * @param	vgwWebsocketLoggingNeeded	Is WebSocket library low level logging needed?
 * 
 * @param	vgwSessionLoggingNeeded	Is IBM Voice Gateway message exchange logging needed for debugging?
 * 
 * @param	initDelayBeforeSendingDataToSpeechProcessors	Time in seconds to wait before sending data to the speech processors.
 * 
 * @param	vgwStaleSessionPurgeInterval	Time interval in seconds during which the VGW source operator below should 
 * do memory cleanup of any Voice Gateway sessions that end abruptly in the 
 * middle of a voice call.
 * 
 * @param	ipv6Available	Is ipv6 protocol stack available in the Streams machine where the 
 * IBMVoiceGatewaySource operator is going to run? 
 * Most of the Linux machines will have ipv6. In that case, 
 * you can keep the following line as it is. 
 * If you don't have ipv6 in your environment, you can set the 
 * following submission time value to false. 
 * 
*/
// This is the main composite for this application.
public composite VgwDataRouter {
	param
		// IBM Voice Gateway related submission time values are defined below.
		// TLS port on which this application will listen for
		// communicating with the IBM Voice Gateway for receiving speech data.
		expression<uint32> $tlsPortForVgwDataRx : 
			(uint32)getSubmissionTimeValue("tlsPortForVgwDataRx", "443");
		// TLS port on which this application will listen to accept connections from
		// the speech processor job in order to route/distribute/send speech data to them.
		expression<uint32> $tlsPortForVgwDataTx : 
			(uint32)getSubmissionTimeValue("tlsPortForVgwDataTx", "444");		
		// User can optionally specify whether they want a non-TLS endpoint.
		expression<boolean> $nonTlsEndpointNeeded : 
			(boolean)getSubmissionTimeValue("nonTlsEndpointNeeded", "false");
		// Non-TLS (Plain) port on which this application will
		// (optionally) listen for communicating with the IBM Voice Gateway.
		expression<uint32> $nonTlsPort : 
			(uint32)getSubmissionTimeValue("nonTlsPort", "80");
		// Non-TLS (Plain) port on which this application will
		// (optionally) listen for communicating with the IBM Voice Gateway.
		expression<uint32> $nonTlsPortForWebSocketSink : 
			(uint32)getSubmissionTimeValue("nonTlsPortForWebSocketSink", "8081");
		// Server side certificate (.pem) file for the WebSocket server.
		// It is necessary for the users to create a Root CA signed 
		// server side certificate file and point to that file at the time of
		// starting this application. If the user doesn't point to this file
		// at the time of starting the application, then the application will
		// look for a default file named ws-server.pem inside the etc sub-directory
		// of the application. This certificate will be presented to the
		// IBM Voice Gateway for validation when it establishes a WebSocket 
		// connection with this application. For doing quick tests, you may save
		// time and effort needed in getting a proper Root CA signed certificate 
		// by going with a simpler option of creating your own self-signed 
		// certificate. Please ensure that using a self-signed certificate is 
		// allowed in your environment. We have provided a set of instructions to
		// create a self signed certificate. Please refer to the following
		// file in the etc sub-directory of this application:
		// etc/creating-a-self-signed-certificate.txt
		expression<rstring> $certificateFileName :
			getSubmissionTimeValue("certificateFileName", "");
		// Is a password needed for the private key in the certificate file?
		expression<rstring> $certificatePassword : 
			getSubmissionTimeValue("certificatePassword", "");
		// Do you want to specify a file name that contains the public certificates of
		// the trusted client(s). If this file name is not empty, then the
		// WebSocketSink operator will perform a client (mutual) authentication.
		expression<rstring> $trustedClientCertificateFileName :
			getSubmissionTimeValue("trustedClientCertificateFileName", "");	
		// Do you want to specify a list of identifiers present in the 
		// trusted client's X509 certificate's subject line. If that certificate is
		// self signed, then it will help during the client (mutual) authentication to approve
		// that client's identity as a known one.
		// 
		// Following are some examples of the subject line as it appears in an X509 public certificate.
		// /C=US/ST=NY/L=Yorktown Heights/O=IBM/OU=AI/CN=websocket.streams/emailAddress=websocket.streams@ibm.com
		// /C=US/O=Let's Encrypt/CN=Let's Encrypt Authority X3
		// /C=BE/O=GlobalSign nv-sa/CN=GlobalSign CloudSSL CA - SHA256 - G3
		// /C=US/O=Google Trust Services/CN=GTS CA 1O1
		// /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA
		// /C=US/ST=New York/L=Armonk/O=IBM/CN=www.ibm.com
		//
		// So your value for this submission time parameter can be as shown here.
		// ['emailAddress=websocket.streams@ibm.com', 'CN=www.ibm.com']
		expression<list<rstring>> $trustedClientX509SubjectIdentifiers :
			(list<rstring>)getSubmissionTimeValue("trustedClientX509SubjectIdentifiers", "[]");	
		// Do you want to use a specific URL context path for the WebSocketSink operator?
        // It can either be a single or a multi-part path.
        // e-g: Orders (OR) MyServices/Banking/Deposit
        // With that example, WebSocket server URL should either be 
        // https://host:port/Orders   (OR)
        // https://host:port/MyServices/Banking/Deposit
        // Default is an empty list to indicate no url context path.
        // You can expose any number of context paths for the 
        // remote clients to access this WebSocket server endpoint. 
        // e-g: []    (OR)    ['Orders', '', 'MyServices/Banking/Deposit']
        //
        // We will allow different speech processor jobs with their own
        // unique ids to connect to this application for getting the
        // speech data to get routed for processing. Such speech processor jobs will
        // use their respective unique speech processor id at the end of the
        // URL as context path. Hence, we will prepare the WebSocketSink operator
        // invoked in this application to allow a set of context paths.
        // e-g: wss://MyHostName:8444/14  This URL must be used by a 
        // speech processor whose unique id  is 14.
		expression<list<rstring>> $webSocketSinkurlContextPath : (list<rstring>)
			getSubmissionTimeValue("webSocketSinkurlContextPath", 
			"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', " +
			"'11', '12', '13', '14', '15', '16', '17', '18', '19', '20', " + 
			"'21', '22', '23', '24', '25', '26', '27', '28', '29', '30']");
		// Is live metrics needed for the WebSocketSink operator?
		expression<boolean> $websocketLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("websocketLiveMetricsUpdateNeeded", "true");
		// Is WebSocket library low level logging needed?
		expression<boolean> $websocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("websocketLoggingNeeded", "false");
		// Is WebSocket client connection logging needed?
		expression<boolean> $wsConnectionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("wsConnectionLoggingNeeded", "false");
		// Is client message exchange logging needed for debugging?
		expression<boolean> $wsClientSessionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("wsClientSessionLoggingNeeded", "false");
		// NOTE: In most application scenarios, this feature will not be used at all.
		expression<uint32> $websocketStaleConnectionPurgeInterval :(uint32)
			getSubmissionTimeValue("websocketStaleConnectionPurgeInterval", "0");
		// Whitelist to accept connections only from specific
		// IP addresses belonging to the remote WebSocket clients.
		// Default is an empty list to indicate all client connections
		// are accepted without any restrictions. If there is a need to
		// accept connections only from certain clients, then a list
		// as shown below can be used  by including wild cards as needed.
		// e-g: "['172.34.18.212', '10.5.23.17', '172.*.42.*', '10.29.42.*']" 
		expression<list<rstring>> $clientWhitelist : (list<rstring>)
			getSubmissionTimeValue("clientWhitelist", "[]");
		// Specify a maximum number of concurrent client connections to be
		// allowed by the WebSocket server available inside the WebSocketSink operator.
		expression<uint32> $maxClientConnectionsAllowed : (uint32)
			getSubmissionTimeValue('maxClientConnectionsAllowed', "64");	
		// Is live metrics needed for the IBMVoiceGatewaySource operator?
		expression<boolean> $vgwLiveMetricsUpdateNeeded : 
			(boolean)getSubmissionTimeValue("vgwLiveMetricsUpdateNeeded", "true");
		// Is WebSocket library low level logging needed?
		expression<boolean> $vgwWebsocketLoggingNeeded : 
			(boolean)getSubmissionTimeValue("vgwWebsocketLoggingNeeded", "false");
		// Is IBM Voice Gateway message exchange logging needed for debugging?
		expression<boolean> $vgwSessionLoggingNeeded : 
			(boolean)getSubmissionTimeValue("vgwSessionLoggingNeeded", "false");
		// Under some circumstances, if the IBMVoiceGatewaySource operator sends 
		// only one EOCS (End Of Call Signal) tuple instead of two as required for
		// the two voice channels, that may eventually cause the application logic
		// below not be able to release the speech processor jobs properly at the end of
		// a voice call for a given VGW session id. We have seen it in certain
		// customer environments. To avoid that condition, such customers can
		// configure this application to treat the very first EOCS tuple as 
		// sufficient to treat a voice call as a "completed call". In that case,
		// it will simply ignore if and when a second EOCS tuple arrives.
		// This feature can be activated to compensate for the situation described
		// above if it happens in some customer environments.
		// (Senthil added this on Feb/01/2021).
		expression<int32> $numberOfEocsNeededForVoiceCallCompletion : 
			(int32)getSubmissionTimeValue("numberOfEocsNeededForVoiceCallCompletion", "2");
		//
		// IBM Watson STT related submission time values are defined below.
		expression<int32> $totalNumberOfSpeechProcessorJobs :(int32)
			getSubmissionTimeValue("totalNumberOfSpeechProcessorJobs", "10") ;		
		expression<int32> $numberOfSpeechEnginesPerSpeechProcessorJob :(int32)
			getSubmissionTimeValue("numberOfSpeechEnginesPerSpeechProcessorJob", "10") ;
		// Time in seconds to wait before sending data to the speech processor.
		expression<float64> $initDelayBeforeSendingDataToSpeechProcessors :
			(float64)getSubmissionTimeValue("initDelayBeforeSendingDataToSpeechProcessors", "15.0"); 
		// Time interval in seconds during which the VGW source operator below should
		// do memory cleanup of any Voice Gateway sessions that end abruptly in the
		// middle of a voice call.
		expression<uint32> $vgwStaleSessionPurgeInterval : (uint32)
			getSubmissionTimeValue("vgwStaleSessionPurgeInterval", "10800");
		// Is ipv6 protocol stack available in the Streams machine where the
		// IBMVoiceGatewaySource operator is going to run?
		// Most of the Linux machines will have ipv6. In that case,
		// you can keep the following line as it is.
		// If you don't have ipv6 in your environment, you can set the
		// following submission time value to false.
		expression<boolean> $ipv6Available : (boolean)
			getSubmissionTimeValue("ipv6Available", "true");
		// This tells what is the maximum number of concurrent voice calls 
		// allowed by the Voice Gateway source operator.
		expression<uint32> $maxConcurrentCallsAllowed : (uint32)
			getSubmissionTimeValue("maxConcurrentCallsAllowed", "10");
			
	graph
		// Ingest the speech data coming from the IBM Voice Gateway.
		// Such speech data arrives here in multiple fragments directly from
		// a live voice call. This operator is capable of receiving speech data
		// from multiple calls that can all happen at the very same time between
		// different pairs of speakers.
		// It is very important to note that the IBM Voice Gateway will keep
		// sending the speech data of the caller and the agent on two 
		// voice channels i.e. one for the caller and the other for the agent.
		// Irrespective of those two speakers talk or remain silent during the
		// call, their assigned voice channel will always carry some binary
		// data. That means, there is no way to know who is currently
		// talking. This constraint limits us from sending only one of the
		// channel's data to a single speech engine at any given time.
		// Instead, this constraint forces us to dedicate a single speech engine
		// per voice channel in a given voice call and keep sending the
		// data being received on that channel continuously to that
		// dedicated speech engine irrespective of whether that channel carries
		// silence or active speech data. In summary, we will need two
		// speech engines to do the Speech To Text for every ongoing voice call.
		// So, you have to plan ahead of time about the number of speech engines
		// you will start for handling the maximum number of concurrent calls.
		// As an example, for handling a maximum of 100 concurrent voice calls,
		// you will have to start 200 speech engines.
		//
		// In your own real-life applications, you may want to simply 
		// copy and reuse the code from this example and then make the 
		// changes only where it is really needed.
		// This example presents the following application design pattern:
		// IBMVoiceGatewaySource-->Speech Data Router-->Speech Processor->Speech Engine--> Speech Result Processor
		// You should be fine to simply use the entire pattern as it is except for
		// making changes in the Speech Result Processor composite and beyond to address
		// your own needs of further analytics on the Speech results as well as
		// specific ways of delivering the Speech results to other 
		// downstream systems rather than only writing to files as this example does below.
		(stream<BinarySpeech_t> BinarySpeechData as BSD) as VgwDataRouterSource = 
			IBMVoiceGatewaySource() {
			logic
				state: {
					// Initialize the default TLS certificate file name if the 
					// user didn't provide his or her own.
					rstring _certificateFileName = 
						($certificateFileName != "") ?
						$certificateFileName : getThisToolkitDir() + "/etc/ws-server.pem";
				}
				
			param
				tlsPort: $tlsPortForVgwDataRx;
				certificateFileName: _certificateFileName;
				nonTlsEndpointNeeded: $nonTlsEndpointNeeded;
				nonTlsPort: $nonTlsPort;
				// Initial delay before generating the very first tuple.
				// This is a one time delay when this operator starts up.
				// This delay should give sufficient time for the
				// speech processor jobs to come up and be ready to
				// receive the speech data tuples sent by this operator.
				initDelay: $initDelayBeforeSendingDataToSpeechProcessors;
				vgwLiveMetricsUpdateNeeded: $vgwLiveMetricsUpdateNeeded;
				websocketLoggingNeeded: $vgwWebsocketLoggingNeeded;
				vgwSessionLoggingNeeded: $vgwSessionLoggingNeeded;
				vgwStaleSessionPurgeInterval: $vgwStaleSessionPurgeInterval;
				ipv6Available: $ipv6Available;
				maxConcurrentCallsAllowed: $maxConcurrentCallsAllowed;
			
			// Get these values via custom output functions	provided by this operator.
			output
				BSD: vgwSessionId = getIBMVoiceGatewaySessionId(),
					callSequenceNumber = getCallSequenceNumber(),
				    callStartDateTime = getCallStartDateTime(), 
				    callStartTimeInEpochSeconds = getCallStartTimeInEpochSeconds(),
					isCustomerSpeechData = isCustomerSpeechData(),
					vgwVoiceChannelNumber = getVoiceChannelNumber(),
					callerPhoneNumber = getCallerPhoneNumber(),
					agentPhoneNumber = getAgentPhoneNumber(),
					ciscoGuid = getCiscoGuid(), 
					speechDataFragmentCnt = getTupleCnt(),
					totalSpeechDataBytesReceived = getTotalSpeechDataBytesReceived();

			config
				// Always place it on a machine that carries the VgwDataRouter host tag.
				placement: host(VgwPool[0]);
		}

		// Every incoming speech data for a given voice call must always be 
		// sent to the same speech processor id.
		// This operator selects an available speech processor id to where the
		// incoming speech data tuple can be sent for processing. It also does
		// the releasing of a previously assigned speech processor when a 
		// voice call assigned to it is ending. Such handling of speech processor
		// assignment and release should be done within a single operator that
		// doesn't have parallel instances.
		//
		// We have to always route the speech data bytes (fragments) coming from  
		// a given vgwSessionId_vgwVoiceChannelNumber to a particular 
		// speech processor job. 
		// We already explained in detail in the previous operator's
		// commentary section about why it must be done this way.
		// This idea of pairing up a vgwSessionId_vgwVoiceChannelNumber combo
		// to a particular parallel region channel is a must for the 
		// speech data bytes of a given speaker in a voice call to always land in 
		// the same speech processor job. This stickiness (a.k.a channel affinity) is
		// important to continuously transcribe the speech data arriving on both the
		// voice channels at all the time including the silence time of a speaker.
		// This is needed because the IBM Voice Gateway keeps sending the 
		// speech data bytes of both the speakers (whether active or silent) at 
		// all the time on two voice channels by dedicating one channel to an
		// agent and the other channel to the caller. So, this requires 
		// extra logic to locate an unused speech processor 
		// i.e. an idle speech engine in a speech processor job to be assigned for a 
		// given vgwSessionId_vgwVoiceChannelNumber.
		(stream<BinarySpeech_t> CallDataWithSpeechProcessorId as CDWSPI) as
			SpeechProcessorIdSelector = Custom(BinarySpeechData as BSD) {
			logic
				state: {
					// This variable tells us how many total concurrent calls can be
					// handled by the way in which the user has deployed the application.
					// Every voice call has two voice channels (agent and customer).
					// So, any given call will require two dedicated speech engines.
					int32 _numberOfConcurrentCallsAllowedPerSpeechProcessor = 
						getNumberOfConcurrentCallsAllowedPerSpeechProcessor(
						$numberOfSpeechEnginesPerSpeechProcessorJob);
					// This list tells us how many voice calls are being processed at any 
					// given time by all the given speech processors that are configured to run.
					mutable list<int32> _speechProcessorStatusList = 
						prepareIdleSpeechProcessorsList($totalNumberOfSpeechProcessorJobs);
					// This map tells us which speech processor is processing a given vgwSessionId.
					// Key=vgwSessionId, Value=Speech Processor Id.
					mutable map<rstring, int32> _vgwSessionIdToSpeechProcessorMap = {};
					// This map tells us which speech processor can be released after completing
					// the speech to text work for a given vgwSessionId_vgwVoiceChannelNumber.
					// After getting released, such speech processors will become available for 
					// doing speech to text work for any new voice calls.
					mutable map<rstring, boolean> _vgwSessionVgwVoiceChannelNumberCompletedMap = {};
					mutable rstring _key = "";
				}
				
				onTuple BSD: {
					// Get the sessionId + channelNumber combo string.
					_key = BSD.vgwSessionId + "_" + (rstring)BSD.vgwVoiceChannelNumber;
					mutable int32 speechProcessorId = -1;

					// We will get the regular binary speech data and the End Of Call Signal (EOCS) in
					// the same input stream. This design change was done on Feb/09/2021 to avoid any
					// any port locks and/or tuple ordering issues that may happen if we choose to 
        			// do it using two different output ports. The incoming tuple has an attribute
        			// that is set to true or false by the IBMVoiceGatewaySource operator to indicate
        			// whether it is sending binary speech data or an EOCS.
        			if(BSD.endOfCallSignal == false) {
        				// The incoming tuple contains binary speech data.
        				// If we are sent an empty speech blob, let us ignore that as
        				// it will cause unwanted side effects in the downstream logic.
        				if(blobSize(BSD.speech) <= 0ul) {
							appTrc(Trace.error, 
								"_ZZZZZ Empty speech packet received for " +
								"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
								" We are going to ignore this packet by not sending it to" +
								" a speech processor.");
        					return;
        				}
        				
						// We have to first check if this speech data belongs to a 
						// brand new voice call or an already ongoing voice call.
						if(has(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId) == false) {
							// This is a brand new voice call. Get a speech processor id to
							// send the speech data belonging to this call.
							// Store this VGW session id to change the status of this 
							// voice call from "brand new" to "ongoing".
							speechProcessorId = 
								getSpeechProcessorIdForNewCallProcessing(
								_numberOfConcurrentCallsAllowedPerSpeechProcessor,
								_speechProcessorStatusList);
							
							if(speechProcessorId == -1) {
								// This condition should not happen as long as there are enough
								// number of speech processors with more than sufficient number of
								// speech engines configured to run in them.
								appTrc(Trace.error, 
									"_XXXXX No speech processor job is available at this time for the " +
									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
									" We are not going to process the currently received speech data bytes" +
									" of this speaker in this voice call." +
									" Please start sufficient number of speech processor jobs " +
									" next time to handle your maximum expected concurrent calls.");
								return;
							}
							
							// Let us store the speech processor id of this call for
							// future use as the speech data keeps coming.
							insertM(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId, speechProcessorId);
							appTrc(Trace.error, "A new call with vgwSessionId=" + BSD.vgwSessionId +
								" is being assigned to speech processor id " + (rstring)speechProcessorId);
							 						
						} else {
							// This incoming tuple carries speech data belonging to a 
							// voice call that we have already started processing.
							// So, get the already assigned speech processor id for this voice call.
							speechProcessorId = _vgwSessionIdToSpeechProcessorMap[BSD.vgwSessionId];
						}
						
						// Set the chosen speech processor id in the tuple attribute.
						BSD.speechProcessorId = speechProcessorId;
						// For the downstream operator that is within a parallel region,
						// we will send this tuple to a correct parallel channel that handles
						// a given speech processor id. Since parallel channels range from
						// 0 to N-1, we will subtract the chosen speech processor id value by
						// 1 as speech processor id ranges from 1 to N.
						BSD.parallelChannel = speechProcessorId - 1;
						// Send this tuple away.
						submit(BSD, CDWSPI);
					} else {
						//The incoming tuple contains an End of Call Signal (EOCS).
						//
						// Process the end of voice call signal.
						// Since there are two channels in every voice call,
						// those two channels will carry their own "End STT session"
						// message from the Voice Gateway. The logic below takes care of
						// handling two End of Call Signals for every voice call.
						//
						// Get the allocated speech processor id for a given vgwSessionId.
						// We should always have a speech processor id. If not, that is a 
						// case where the user didn't provision sufficient number of 
						// Speech engines and there was no idle speech processor available for that given vgwSessionId. 
						// This situation can be avoided by starting the application with a 
						// sufficient number of speech processors along with sufficient
						// speech engines needed for the anticipated maximum concurrent voice calls. 
						// A rule of thumb is to have two speech engines to process 
						// two speakers in every given concurrent voice call.
						//					
						if (has(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId) == true) {
							int32 speechProcessorId = 
								_vgwSessionIdToSpeechProcessorMap[BSD.vgwSessionId];
								
							// Set the chosen speech processor id in the tuple attribute.
							BSD.speechProcessorId = speechProcessorId;
							// For the downstream operator that is within a parallel region,
							// we will send this tuple to a correct parallel channel that handles
							// a given speech processor id. Since parallel channels range from
							// 0 to N-1, we will subtract the chosen speech processor id value by
							// 1 as speech processor id ranges from 1 to N.
							BSD.parallelChannel = speechProcessorId - 1;
							// Send this tuple away.
							submit(BSD, CDWSPI);
								
							// Add the _key to the call completed list for the speech processor id to be
							// released later in the following if block only after receiving EOCS for 
							// both the voice channels of this call.
							insertM(_vgwSessionVgwVoiceChannelNumberCompletedMap, _key, true);
							
							rstring key1 = BSD.vgwSessionId + "_" + "1";
							rstring key2 = BSD.vgwSessionId + "_" + "2";						
							// Since this voice call is ending, let us release the speech processor id 
							// that was allocated above for this voice call.
							// Remove the speech processor id only if the EOCS signal
							// was sent for both of the voice channels. That must first 
							// happen before we can release the speech processor id.
							boolean key1Exists = has(_vgwSessionVgwVoiceChannelNumberCompletedMap, key1);
							boolean key2Exists = has(_vgwSessionVgwVoiceChannelNumberCompletedMap, key2);
							
							if ($numberOfEocsNeededForVoiceCallCompletion == 2 &&
								(key1Exists == true && key2Exists == true)) {
								// Since the voice call for this VGW session id has ended completely,
								// we can also release the speech processor id assigned for this call so that 
								// it can be repurposed for handling any new future calls.
								// We can go ahead and release the speech processor id by adding it back to 
								// the speech processor status list.
								// Let us decrement the given speech processor's current call handling count.
								// It is a zero based indexed array. Hence, we have to subtract by 1 to get the
								// current index in that array.
								_speechProcessorStatusList[speechProcessorId-1] = 
									_speechProcessorStatusList[speechProcessorId-1] - 1;
								
								// We can now do the clean-up in our state variables.
								removeM(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId);
								removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key1);
								removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key2);
								appTrc(Trace.error, "i) A call with vgwSessionId=" + BSD.vgwSessionId +
									" ended and its speech processor id " + (rstring)speechProcessorId + 
									" got released.");						
							} else if ($numberOfEocsNeededForVoiceCallCompletion == 1 &&
								(key1Exists == true || key2Exists == true)) {
								// If the user configured this application to handle
								// a single EOCS as sufficient to consider a voice call
								// completed for a given VGW session id, we will use this
								// block of code. Please refer to the constant i.e. expression
								// declaration section above to read the commentary about this idea.
								//
								// Since the voice call for this VGW session id has ended completely,
								// we can also release the speech processor id assigned for this call so that 
								// it can be repurposed for handling any new future calls.
								// We can go ahead and release the speech processor id by adding it back to 
								// the speech processor status list.
								// Let us decrement the given speech processor's current call handling count.
								// It is a zero based indexed array. Hence, we have to subtract by 1 to get the
								// current index in that array.
								_speechProcessorStatusList[speechProcessorId-1] = 
									_speechProcessorStatusList[speechProcessorId-1] - 1;
								
								// We can now do the clean-up in our state variables.
								removeM(_vgwSessionIdToSpeechProcessorMap, BSD.vgwSessionId);
	
								if(key1Exists == true) {
									removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key1);
								}
								
								if(key2Exists == true) {
									removeM(_vgwSessionVgwVoiceChannelNumberCompletedMap, key2);
								}
								
								appTrc(Trace.error, "ii) A call with vgwSessionId=" + BSD.vgwSessionId +
									" ended and its speech processor id " + (rstring)speechProcessorId + 
									" got released.");						
							}
						} else {
							// We couldn't get a speech processor id assigned for this voice call.
							// Flag an error only when the user configured for two
							// EOCS tuples to be received for considering a voice call
							// as completed.
							if ($numberOfEocsNeededForVoiceCallCompletion == 2) {
								appTrc(Trace.error, 
									"_YYYYY No speech processor id is available at this time for the " +
									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
									" We are not going to process the currently received EOCS " +
									" of this speaker in this voice call. This is a serious error.");
							}					
						}						
					} // End of if(BSD.endOfCallSignal == false)				
			} // End of onTuple BSD

			config
				// Depending on the speech data load from the number of concurrent calls, 
				// queue size below can be increased or decreased. 
				threadedPort: queue(BSD, Sys.Wait, 300000);
		} // End of the SpeechProcessorIdSelector operator.

		// Speech data distribution logic happens inside this operator.
		//
		// Let us do partitioned parallelism for this operator to evenly distribute the
		// high volume speech payload sent here by the source operator shown above.
		// Incoming speech data will carry a unique call sequence number of type int32 that
		// will be used to partition the tuples consistently to a particular parallel channel.
		@parallel(width=$totalNumberOfSpeechProcessorJobs, partitionBy=[{port=BSD, attributes=[parallelChannel]}])
		(stream<WebSocketSinkSendData_t> CallDataForSpeechProcessor as CDFSP) as
			VoiceCallDataRouter = Custom(CallDataWithSpeechProcessorId as BSD) {
			logic
				state: {
					mutable DataFromVgwRouter_t _serializedTuple = {};
					mutable WebSocketSinkSendData_t _oTuple = {};
				}
			
				// Process the Binary Speech Data.
				onTuple BSD: {
					// We will get the regular binary speech data and the End Of Call Signal (EOCS) in
					// the same input stream. This design change was done on Feb/09/2021 to avoid any
					// any port locks and/or tuple ordering issues that may happen if we choose to 
        			// do it using two different output ports. The incoming tuple has an attribute
        			// that is set to true or false by the IBMVoiceGatewaySource operator to indicate
        			// whether it is sending binary speech data or an EOCS.
        			if(BSD.endOfCallSignal == false) {
						// We can prepare this speech data to be sent to the chosen speech processor id.
						// We will do double serialization of the speech data as shown below.
						//
						// 1) Let us serialize the received speech data tuple.
						// msgType = 1 means that it carries the binary speech data tuple.
						_serializedTuple.msgType = 1;
						clearM(_serializedTuple.payload);
						// Call a native function to do the Tuple-->Blob conversion.
						serializeTuple(BSD, _serializedTuple.payload);
						
						// 2) Let us now serialize it one more time to be sent as the final tuple.
						_oTuple.strData = "";
						// We can tell the downstream WebSocketSink operator about to which
						// speech processor id our serialized data should be sent.
						// That is done by specifying the URL context path of the WebSocketSink
						// operator where a selected speech processor has connected to.
						// As shown in the param section of the downstream sink operator,
						// we have configured it to allow URL context path such as "1","2","3" and so on.
						// With this arrangement, remote speech processor jobs can connect to our
						// WebSocketSink via a distinct URL that carries their respective speech processor ids.
						_oTuple.sendToUrlContextPaths = [(rstring)BSD.speechProcessorId];
						// Set the parallelChannel in the final tuple so that the 
						// WebSocket sink operator can use it to find a correct operator instance
						// in a parallel region. See more details about it at the top of the
						// WebSocket sink operator invocation in the code below.
						// Parallel channels have a range from 0 to N-1 in a
						// N-way parallel region. However, our speech processor id always 
						// starts at 1. To align it correctly with the zero based parallel 
						// channel number, we will decrement the speech processor id by 1.
						_oTuple.parallelChannel = BSD.speechProcessorId - 1;
						serializeTuple(_serializedTuple, _oTuple.blobData);
						submit(_oTuple, CDFSP);
					} else {
						//The incoming tuple contains an End of Call Signal (EOCS).
						// Let us send the EOCS to the chosen speech processor id.
						// We will do double serialization of the data as shown below.
						// 1) Let us serialize the received speech data tuple.
						// msgType = 2 means that it carries the EOCS tuple.
						_serializedTuple.msgType = 2;
						clearM(_serializedTuple.payload);
						// Call a native function to do the Tuple-->Blob conversion.
						serializeTuple(BSD, _serializedTuple.payload);
						
						// 2) Let us now serialize it one more time to be sent as the final tuple.
						_oTuple.strData = "";
						// We can tell the downstream WebSocketSink operator about to which
						// speech processor id our serialized data should be sent.
						// That is done by specifying the URL context path of the WebSocketSink
						// operator where a selected speech processor has connected to.
						// As shown in the param section of the downstream sink operator,
						// we have configured it to allow URL context path such as "1","2","3" and so on.
						// With this arrangement, remote speech processor jobs can connect to our
						// WebSocketSink via a distinct URL that carries their respective speech processor ids.
						_oTuple.sendToUrlContextPaths = [(rstring)BSD.speechProcessorId];
						// Set the parallelChannel in the final tuple so that the 
						// WebSocket sink operator can use it to find a correct operator instance
						// in a parallel region. See more details about it at the top of the
						// WebSocket sink operator invocation in the code below.
						// Parallel channels have a range from 0 to N-1 in a
						// N-way parallel region. However, our speech processor id always 
						// starts at 1. To align it correctly with the zero based parallel 
						// channel number, we will decrement the speech processor id by 1.
						_oTuple.parallelChannel = BSD.speechProcessorId - 1;
						serializeTuple(_serializedTuple, _oTuple.blobData);
						submit(_oTuple, CDFSP);										
					} // End of if(BSD.endOfCallSignal == false)				
				} // End of onTuple BSD
								
			config
				// Depending on the speech data load from the number of concurrent calls, 
				// queue size below can be increased or decreased. 
				threadedPort: queue(BSD, Sys.Wait, 15000);
		} // End of Custom operator.

		// Invoke one or more instances of the WebSocketSink operator.
		// This operator listens on a TLS port and is configured with
		// multiple URL context paths to accept connections from the
		// remote speech processor jobs that will each connect to a
		// particular context path expressed by their speech processor id.
		// It then routes the voice calls' speech data and EOCS to the
		// corresponding speech processor job.
		//
		// To distribute the speech data load in a well performing manner,
		// We will have N parallel channels ranging from 0 to N-1.
		// Here, N is equal to the total number of speech processor jobs running.
		// Each parallel instance of this operator will listen on a port that is
		// equal to base_port_numer + zero_based_parallel_channel_number.
		// The incoming tuples will have an attribute named parallelChannel which will
		// be in the range of 0 to N-1 to align with the range of the configured
		// parallel channels. This will let us do the data partitioning based on that
		// attribute so that the speech data destined for a given speech processor id will
		// land on the correct parallel channel. Now, all that is remaining is for the
		// individual speech processor jobs to connect to a WebSocket sink operator 
		// instance running on a correct parallel channel.
		// e-g: 
		// Speech processor 1 will connect to parallel channel 0,
		// Speech processor 2 will connect to parallel channel 1,
		// Speech processor 3 will connect to parallel channel 2 and so on.
		//
		// This can be easily done at the time of starting the speech processor jobs to
		// simply connect to the base_port_number + (speech_processor_id - 1). You can
		// refer to the etc/start-mini.sh script available in the 
		// VgwDataRouterToWatsonSTTMini project and see how the speech processors are
		// started to connect to a specific WebSocket sink port.
		//
		@parallel(width=$totalNumberOfSpeechProcessorJobs, partitionBy=[{port=CDFSP, attributes=[parallelChannel]}])
		() as VgwDataRouterSink = WebSocketSink(CallDataForSpeechProcessor as CDFSP) {
			logic
				state: {
					// Initialize the default TLS certificate file name if the 
					// user didn't provide his or her own.
					rstring _certificateFileName = 
						($certificateFileName != "") ?
						$certificateFileName : getThisToolkitDir() + "/etc/ws-server.pem";
				}
				
			param
				tlsPort: $tlsPortForVgwDataTx + (uint32)getChannel();
				certificateFileName: _certificateFileName;
				certificatePassword: $certificatePassword;
				trustedClientCertificateFileName: $trustedClientCertificateFileName;
				// Use this only when you have trouble authenticating clients that 
				// have self signed certificates.
				trustedClientX509SubjectIdentifiers: $trustedClientX509SubjectIdentifiers;
				nonTlsEndpointNeeded: $nonTlsEndpointNeeded;
				nonTlsPort: $nonTlsPortForWebSocketSink + (uint32)getChannel();
				urlContextPath: $webSocketSinkurlContextPath;
				websocketLiveMetricsUpdateNeeded: $websocketLiveMetricsUpdateNeeded;
				websocketLoggingNeeded: $websocketLoggingNeeded;
				wsConnectionLoggingNeeded: $wsConnectionLoggingNeeded;
				wsClientSessionLoggingNeeded: $wsClientSessionLoggingNeeded;
				websocketStaleConnectionPurgeInterval: $websocketStaleConnectionPurgeInterval;
				ipv6Available: $ipv6Available;
				clientWhitelist: $clientWhitelist;
				maxClientConnectionsAllowed: $maxClientConnectionsAllowed;
				
			config
				// Always place it on a machine that carries the VgwDataRouter host tag.
				// Depending on the speech data load from the number of concurrent calls, 
				// queue size below can be increased or decreased. 
				placement: host(VgwPool[0]);
				threadedPort: queue(CDFSP, Sys.Wait, 15000);
		} // End of WebSocketSink operator invocation.
		
	// This is a composite level configuration to declare a hostpool using host tags.
	config
		// To learn about how to create host tags and assign them to the machines in
		// your Streams instance, you can read tips-on-using-streams-host-tags.txt available 
		// in the SPL-Examples-For-Beginners/047_streams_host_tags_at_work/host.tags directory.
		hostPool: 
			VgwPool = createPool({size=1u, tags=["VgwDataRouter"]}, Sys.Shared); /*Sys.Exclusive*/
} // End of the main composite.

// Following are the common utility functions for 
// use within the composites declared above.
//
// This function computes the number of allowed concurrent calls per speech processor job.
public stateful int32 getNumberOfConcurrentCallsAllowedPerSpeechProcessor(
	int32 numberOfSpeechEnginesPerSpeechProcessorJob) {
	// Check if there is an even number of speech engines configured for each speech processor job.
	if(numberOfSpeechEnginesPerSpeechProcessorJob % 2 != 0) {
		// This is not a correct configuration. Let us abort now.
		appTrc(Trace.error, 
			"_XXXXX Incorrect configuration for the number of speech engines per speech processor job. " + 
			"There should be an even number of speech engines running in every speech processor job.");
		abort();
	}
	
	// There are two voice channels in a voice call.
	// Each voice channel will require a dedicated speech engine.
	// So, for a given voice call, two speech engines will be required.
	int32 x = numberOfSpeechEnginesPerSpeechProcessorJob / 2;
	// That means each speech processor can handle a maximum of x concurrent calls.
	return(x);
}

// This function creates a new list with as many elements as 
// the number of speech processors configured to run.
// Every element will be initialized to 0 to mean that   
// all such speech processors are idle at this time. 
public list<int32> prepareIdleSpeechProcessorsList(int32 speechProcessorsCount) {
	mutable list<int32> myList = [];
	mutable int32 idx = 1;
	
	while(idx <= speechProcessorsCount) {
		// Initialize it with 0 to indicate that no voice call 
		// is currently being processed by a given speech processor.
		appendM(myList, 0);
		idx++;
	}
	
	return(myList);
}

// This function takes the speech processors status list as an input and
// returns a speech processor id that has unused call processing capacity.
// If all the speech processors are busy at this time, it will return -1.
public int32 getSpeechProcessorIdForNewCallProcessing(
	int32 numberOfConcurrentCallsAllowedPerSpeechProcessor,
	mutable list<int32> speechProcessorStatusList) {
	if (size(speechProcessorStatusList) <= 0) {
		return(-1);
	}
	
	mutable int32 idx = -1;
	mutable boolean speechProcessorAvailable = false;
	
	for(int32 x in speechProcessorStatusList) {
		idx++;
		
		if(x < numberOfConcurrentCallsAllowedPerSpeechProcessor) {
			// This speech processor is not handling its max allowed concurrent calls.
			speechProcessorAvailable = true;
			break;
		}
	} // End of for loop.
	
	if(speechProcessorAvailable == false) {
		// All the speech processors are busy at this time.
		// This is not good news for a newly arrived voice call.
		return(-1);
	}
	
	// A newly arrived call's processing will be assigned to this speech processor.
	// Let us increment the given speech processor's current call handling count.
	speechProcessorStatusList[idx] = speechProcessorStatusList[idx] + 1;
	// Since it is a zero based index array, we will return a value by adding 1 to it.
	return(idx + 1);
}
