<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en-us" lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="reference"/>
<meta name="DC.Title" content="SPL File AudioRawWatsonSTT.spl"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="spldoc_compilationunit"/>
<link rel="stylesheet" type="text/css" href="../../html/commonltr.css"/>
<link rel="stylesheet" type="text/css" href="../../html/spldoc.css"/>
<title>SPL File AudioRawWatsonSTT.spl</title>
</head>
<body id="spldoc_compilationunit">


<h1 class="title topictitle1">SPL File <tt class="ph tt">AudioRawWatsonSTT.spl</tt></h1>

<div class="body refbody">
<div class="section">
<p class="p">
<a class="xref" href="../toolkits/toolkits.html">Gateway to the IBM Speech To Text (STT) cloud service samples</a> &gt; <a class="xref" href="tk$AudioRawWatsonSTT.html">AudioRawWatsonSTT 1.0.6</a> &gt; <a class="xref" href="ns$com.ibm.streamsx.sttgateway.sample.watsonstt.html">com.ibm.streamsx.sttgateway.sample.watsonstt</a> &gt; AudioRawWatsonSTT.spl</p>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Content</h2>
  
  <dl class="dl">
    <dt class="dt dlterm"/>
<dd class="dd"/>

    
      <dt class="dt dlterm splhead-2">Operators</dt>

      <dd class="dd">
<ul class="sl simple">
<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsonstt$AudioRawWatsonSTT.html#spldoc_compilationunit__composite_operator__AudioRawWatsonSTT">AudioRawWatsonSTT</a></strong>: This example demonstrates transcription of raw audio data to text using the WatsonSTT operator.
</li>

</ul>

      </dd>

    
  </dl>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Composites</h2>
  
</div>

<div class="section" id="spldoc_compilationunit__composite_operator__AudioRawWatsonSTT"><h2 class="title sectiontitle splpart">composite AudioRawWatsonSTT</h2>
  
</div>

<div class="section splgraph">
  <embed class="image" src="../../image/tk$AudioRawWatsonSTT/op$com.ibm.streamsx.sttgateway.sample.watsonstt$AudioRawWatsonSTT.svg" width="442" height="154"/>
</div>

<div class="section">

<p class="p">This example demonstrates transcription of raw audio data to text using the WatsonSTT operator.  This example code is suitable for processing the raw audio binary data that is either  received directly from the voice network switches via TCP/UDP (OR) read from the WAV files  on your own as audio blob data (in full or in partial blob fragments). Such raw binary audio data is then sent to the WatsonSTT operator for transcription.
</p>

<p class="p">This example shows how to send the raw binary audio data to the WatsonSTT operator to  get the transcription results. If you are looking to pick up the audio files  available in a given directory and simply send the name of the file to the  WatsonSTT operator to get the transcription results, then please refer to a  different example project named AudioFileWatonSTT.
</p>

<p class="p">WAV files can be in uncompressed PCM, 16-bit little endian,  8 kHz (narrowband) or 16 KHz (broadband) sampling rate, mono format. 
</p>

<div class="p">To either downsample or upsample a WAV file, you can use the following ffmpeg command: 
<pre class="pre codeblock">
ffmpeg -i MyFile.wav -ac 1 -ar 8000 MyNewFile.wav
</pre>


</div>

<p class="p">You can build this example from command line via the make command by using the Makefile available in the top-level directory of this example. It will be necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by pointing it to the full path of your  streamsx.sttgateway/com.ibm.streamsx.sttgateway directory.
</p>

<p class="p">If you want to build this example inside the Streams Studio, there are certain build configuration settings needed. Please refer to the streamsx.sttgateway toolkit documentation to learn more about those Streams Studio configuration settings.
</p>

<p class="p">IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the  Watson STT cloud service. For the STT service on IBM Public Cloud,  one must use the unexpired IAM access token (generated by using your  IBM Public cloud STT service instance's API key).  So, user must provide here his/her API key. The operator IAMAccessTokenGenerator  will use the user provided API key to generate the IAM access token and  send that to the WatsonSTT operator. The operator IAMAccessTokenGenerator will keep refreshing that IAM access token periodically in order for it to stay unexpired. You should leave this submission time value empty when not using STT on IBM public cloud. https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
</p>

</div>

<div class="section"><h2 class="title sectiontitle">Parameters</h2>

<ul class="sl simple">
<li class="sli"><strong class="ph b">sttApiKey</strong>: Specify either the public cloud IAM Token fetch/refresh URL. 
</li>

<li class="sli"><strong class="ph b">sttIAMTokenURL</strong>: Specify either the public cloud IAM Token fetch/refresh URL.                      Default https://iam.cloud.ibm.com/identity/token

</li>

<li class="sli"><strong class="ph b">sttOnCP4DAccessToken</strong>: Specify the IBM STT on Cloud Pak for Data (CP4D i.e. private cloud) access token.      You should leave this submission time value empty when not using STT on CP4D.

</li>

<li class="sli"><strong class="ph b">audioDir</strong>: directory with the audio files
</li>

<li class="sli"><strong class="ph b">nonFinalUtterancesNeeded</strong>: nonFinalUtterancesNeeded; Default false
</li>

<li class="sli"><strong class="ph b">numberOfSTTEngines</strong>: the number of stt engines
</li>

<li class="sli"><strong class="ph b">sttUri</strong>: the stt service uri
</li>

<li class="sli"><strong class="ph b">sttBaseLanguageModel</strong>: base language model; Default: en-US_NarrowbandModel
</li>

<li class="sli"><strong class="ph b">contentType</strong>: content type; Default audio/wav
</li>

<li class="sli"><strong class="ph b">smartFormattingNeeded</strong>: smartFormattingNeeded; Default false
</li>

<li class="sli"><strong class="ph b">websocketLoggingNeeded</strong>: websocketLoggingNeeded; Default false
</li>

<li class="sli"><strong class="ph b">audioBlobFragmentSize</strong>: You can specify any audio blob fragment size (in number of bytes) to read      from a given audio file and sent to the WatsonSTT operator for transcription. 
     You must not give a size of 0.

</li>

</ul>

</div>

<div class="section">
</div>

<div class="section">
</div>

<div class="section"><h2 class="title sectiontitle splhead-2">SPL Source Code</h2>
  
</div>


<div class="section">
   <pre class="pre codeblock">

 public composite AudioRawWatsonSTT {
 
 	param
 		expression&lt;rstring&gt; $sttApiKey:                      getSubmissionTimeValue("sttApiKey", "");
 		expression&lt;rstring&gt; $sttIAMTokenURL:                 getSubmissionTimeValue("sttIAMTokenURL", "https://iam.cloud.ibm.com/identity/token");
 		expression&lt;rstring&gt; $sttOnCP4DAccessToken:           getSubmissionTimeValue("sttOnCP4DAccessToken", "");
 		expression&lt;rstring&gt; $audioDir:                       getSubmissionTimeValue("audioDir", "../../audio-files");
 		expression&lt;boolean&gt; $nonFinalUtterancesNeeded: (boolean)getSubmissionTimeValue("nonFinalUtterancesNeeded", "false");
 		expression&lt;int32&gt;   $numberOfSTTEngines:      (int32)getSubmissionTimeValue("numberOfSTTEngines", "5") ;
 		expression&lt;rstring&gt; $sttUri:                         getSubmissionTimeValue("sttUri");
 		expression&lt;rstring&gt; $sttBaseLanguageModel:           getSubmissionTimeValue("sttBaseLanguageModel", "en-US_NarrowbandModel");
 		expression&lt;rstring&gt; $contentType:                    getSubmissionTimeValue("contentType", "audio/wav");
 		expression&lt;boolean&gt; $smartFormattingNeeded: (boolean)getSubmissionTimeValue("smartFormattingNeeded", "false");
 		expression&lt;boolean&gt; $websocketLoggingNeeded:(boolean)getSubmissionTimeValue("websocketLoggingNeeded", "false");
 		expression&lt;uint32&gt;  $audioBlobFragmentSize : (uint32)getSubmissionTimeValue("audioBlobFragmentSize", "512");
 
 	type
 		// This STT result type
 		STTResult_t =
 			int32 sequence,
 			rstring conversationId,
 			boolean finalizedUtterance,
 			boolean transcriptionCompleted,
 			rstring sttErrorMessage,
 			float64 utteranceStartTime,
 			float64 utteranceEndTime,
 			float64 confidence,
 			rstring utterance;
 
 	graph
 		// IMPORTANT: IBM STT service on public cloud requires
 		// an unexpired valid IAM access token to perform the 
 		// speech to text task in a secure manner. You may either 
 		// provide the token in parameter sttOnCP4DAccessToken, or provide 
 		// the parameter sttIAMTokenURL and sttApiKey. 
 		// If you provide sttIAMTokenURL and sttApiKey, the operator IAMAccessTokenGenerator
 		// generates a new access token and then periodically refreshes it. 
 		// Output stream of this composite operator is connected to the
 		// second input stream of the WatsonSTT operator that is used below.
 		// For a correct STT operation, user must set only one of these two
 		// submission time parameters to a non-empty value: sttAPIKey or sttOnCP4DAccessToken.
 		stream&lt;IAMAccessToken&gt; IamAccessToken = IAMAccessTokenGenerator() {
 			param
 				apiKey: $sttApiKey;
 				iamTokenURL: $sttIAMTokenURL;
 				accessToken: $sttOnCP4DAccessToken;
 				// All connection parameter are taken from params.
 				// So if we clean the app config name, we avoid error logs
 				appConfigName: "";
 			config
 				placement : partitionColocation("somePartitionColocationId");
 		}
 
 		// Scan a directory periodically to pick up the audio files and 
 		// send the name of that audio file to the WatsonSTT operator.
 		//
 		// The output stream from this operator will be connected to the
 		// WatsonSTT operator's input stream. That operator expects one of its
 		// input stream attributes to be named as 'speech' with an SPL type of
 		// either rstring or blob. That operator can take as input an 
 		// audio filename (rstring) or raw binary audio data (blob).
 		// In this example, we have to simply send the name of the audio file to the
 		// WatsonSTT operator which will read the binary audio data on its own from that file.
 		// In addition to that mandatory input stream attribute, additional
 		// attributes can be sent in the input stream to be auto assigned to
 		// the output stream of the WatsonSTT operator.
 		stream&lt;rstring fileName&gt; AudioFileName as O = DirectoryScan() {
 			param
 				directory : $audioDir;
 				pattern : "\\.wav$";
 				sortBy: name;
 				// Give sufficient delay here so that the previous operator can complete generating the
 				// IAM access token and send it to the WatsonSTT operator.
 				// This is not a requirement but avoids error logs in WatsonSTT operator
 				initDelay: 5.0;
 			config
 				placement : partitionColocation("somePartitionColocationId");
 		}
 		
 		// Read the file content into the blob and count the segments
 		// The catch annotation ensures that the application crashes if the file is removed prematurely
 		@catch(exception=all, tupleTrace=true, stackTrace=true)
 		stream&lt;int32 sequence, rstring conversationId, blob speech&gt; AudioBlobData as O = FileSource(AudioFileName as I) {
 			logic
 				state: {
 					mutable int32 fileCounter = 0;
 					mutable rstring tmp = "";
 				}
 				onTuple I:
 					tmp = fileName;
 			param
 				format: block;
 				blockSize: $audioBlobFragmentSize;
 			output O:
 				conversationId = tmp,
 				sequence = fileCounter++;
 			config
 				placement : partitionColocation("somePartitionColocationId");
 		}
 		
 		// Invoke one or more instances of the WatsonSTT operator.
 		// Avoid feeding audio data coming from more than one data source into this 
 		// parallel region which may cause erroneous transcription results.
 		
 		// NOTE: The WatsonSTT operator allows fusing multiple instances of
 		// this operator into a single PE. This will help in reducing the 
 		// total number of CPU cores used in running the application.
 		// First input stream into this operator is the fully qualified audio file name.
 		// Second input stream into this operator is your STT service instance's IAM access token.
 		@parallel(width = $numberOfSTTEngines, partitionBy=[{port=I, attributes=[conversationId]}], broadcast=[AT])
 		stream&lt;STTResult_t&gt; STTResult as O = WatsonSTT(AudioBlobData as I; IamAccessToken as AT) {
 			logic
 				onTuple I: {
 					// Print the input
 					printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " Channel " + (rstring)getChannel() + 
 						", Speech sequence " + (rstring)sequence + ": " + I.conversationId);
 				}
 			param
 				sttResultMode: partial;
 				nonFinalUtterancesNeeded: $nonFinalUtterancesNeeded;
 				uri: $sttUri;
 				baseLanguageModel: $sttBaseLanguageModel;
 				contentType: $contentType;
 				smartFormattingNeeded: $smartFormattingNeeded;
 				websocketLoggingNeeded: $websocketLoggingNeeded;
 			output O:
 				finalizedUtterance = isFinalizedUtterance(),
 				transcriptionCompleted = isTranscriptionCompleted(),
 				sttErrorMessage = getSTTErrorMessage(),
 				utteranceStartTime = getUtteranceStartTime(),
 				utteranceEndTime = getUtteranceEndTime(),
 				confidence = getConfidence(),
 				utterance = getUtteranceText();
 			config
 				placement : partitionColocation("somePartitionColocationId");
 				// The input queue must be big enough to store one ore more files
 				// otherwise one parallel channel may cause back pressure and the other channel may be starved out
 				threadedPort : queue(I, Sys.Wait, 100000);
 		}
 		
 		// Print the results
 		() as MySink1 = Custom(STTResult as SR) {
 			logic state: mutable int32 _conversationCnt = 0;
 				onTuple SR: {
 					printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " " + (rstring)++_conversationCnt + 
 						") STT result: " + (rstring)SR);
 				}
 				// Window marker have no meaning in this merged stream
 				// Each tuple represents an full audio transcription or one error
 				onPunct SR: printStringLn(hourMinuteSecondMillisec(getTimestamp()) + (rstring)_conversationCnt + 
 						") STT result: " + (rstring)currentPunct());
 			config
 				placement : partitionColocation("somePartitionColocationId");
 		}
 
 	config restartable: false;
 } // End of composite AudioFileWatsonSTT (Main composite)

   </pre>

</div>

</div>


</body>
</html>