<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en-us" lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="reference"/>
<meta name="DC.Title" content="SPL File VoiceGatewayToStreamsToWatsonS2T.spl"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="spldoc_compilationunit"/>
<link rel="stylesheet" type="text/css" href="../../html/commonltr.css"/>
<link rel="stylesheet" type="text/css" href="../../html/spldoc.css"/>
<title>SPL File VoiceGatewayToStreamsToWatsonS2T.spl</title>
</head>
<body id="spldoc_compilationunit">


<h1 class="title topictitle1">SPL File <tt class="ph tt">VoiceGatewayToStreamsToWatsonS2T.spl</tt></h1>

<div class="body refbody">
<div class="section">
<p class="p">
<a class="xref" href="../toolkits/toolkits.html">Gateway to the IBM Speech To Text (STT) cloud service samples</a> &gt; <a class="xref" href="tk$VoiceGatewayToStreamsToWatsonS2T.html">VoiceGatewayToStreamsToWatsonS2T 1.0.1</a> &gt; <a class="xref" href="ns$com.ibm.streamsx.sttgateway.sample.watsons2t.html">com.ibm.streamsx.sttgateway.sample.watsons2t</a> &gt; VoiceGatewayToStreamsToWatsonS2T.spl</p>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Content</h2>
  
  <dl class="dl">
    <dt class="dt dlterm"/>
<dd class="dd"/>

    
      <dt class="dt dlterm splhead-2">Operators</dt>

      <dd class="dd">
<ul class="sl simple">
<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsons2t$VoiceGatewayToStreamsToWatsonS2T.html#spldoc_compilationunit__composite_operator__VoiceGatewayToStreamsToWatsonS2T">VoiceGatewayToStreamsToWatsonS2T</a></strong>: What does this example application do? 
</li>

</ul>

      </dd>

    
    
      <dt class="dt dlterm splhead-2">Types</dt>

      <dd class="dd">
<ul class="sl simple">
<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsons2t$VoiceGatewayToStreamsToWatsonS2T.html#spldoc_compilationunit__type__S2TDiagnosticsResult_t">S2TDiagnosticsResult_t</a></strong>: The result type for the diagnostics 
</li>

<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsons2t$VoiceGatewayToStreamsToWatsonS2T.html#spldoc_compilationunit__type__S2TResult_t">S2TResult_t</a></strong>: This S2T result type contains many attributes to  demonstrate all the basic and very advanced features of  the Watson S2T engine.
</li>

</ul>

      </dd>

    
  </dl>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Composites</h2>
  
</div>

<div class="section" id="spldoc_compilationunit__composite_operator__VoiceGatewayToStreamsToWatsonS2T"><h2 class="title sectiontitle splpart">composite VoiceGatewayToStreamsToWatsonS2T</h2>
  
</div>

<div class="section splgraph">
  <embed class="image" src="../../image/tk$VoiceGatewayToStreamsToWatsonS2T/op$com.ibm.streamsx.sttgateway.sample.watsons2t$VoiceGatewayToStreamsToWatsonS2T.svg" width="442" height="116"/>
</div>

<div class="section">

<p class="p">What does this example application do?
</p>

<div class="p">
<ul class="ul">
<li class="li"> This example demonstrates the integration of the following three products to</li>

</ul>
achieve Real-Time Speech-To-Text transcription to get the text ready for any further analytics.
</div>

<p class="p">1) IBM Voice Gateway v1.0.3.0 or higher 2) IBM Streams v4.2.1.6 or higher 3) IBM Watson Speech 2 Text (Embedded in an IBM Streams WatsonS2T operator v2.12.0)
</p>

</div>

<div class="section"><h2 class="title sectiontitle">Parameters</h2>

<ul class="sl simple">
<li class="sli"><strong class="ph b">tlsPort</strong>: TLS port on which this application will listen for communicating with the IBM Voice Gateway. 
</li>

<li class="sli"><strong class="ph b">nonTlsEndpointNeeded</strong>: User can optionally specify whether they want a non-TLS endpoint. 
</li>

<li class="sli"><strong class="ph b">nonTlsPort</strong>: Non-TLS (Plain) port on which this application will (optionally) listen for communicating with the IBM Voice Gateway. 
</li>

<li class="sli"><strong class="ph b">certificateFileName</strong>: Server side certificate (.pem) file for the WebSocket server.  It is necessary for the users to create a Root CA signed  server side certificate file and point to that file at the time of  starting this application. If the user doesn't point to this file  at the time of starting the application, then the application will  look for a default file named ws-server.pem inside the etc sub-directory  of the application. This certificate will be presented to the  IBM Voice Gateway for validation when it establishes a WebSocket  connection with this application. For doing quick tests, you may save  time and effort needed in getting a proper Root CA signed certificate  by going with a simpler option of creating your own self-signed  certificate. Please ensure that using a self-signed certificate is  allowed in your environment. We have provided a set of instructions to  create a self signed certificate. Please refer to the following  file in the etc sub-directory of this application:  etc/creating-a-self-signed-certificate.txt 
</li>

<li class="sli"><strong class="ph b">vgwLiveMetricsUpdateNeeded</strong>: Is live metrics needed for the IBMVoiceGatewaySource operator? 
</li>

<li class="sli"><strong class="ph b">vgwWebsocketLoggingNeeded</strong>: Is WebSocket library low level logging needed? 
</li>

<li class="sli"><strong class="ph b">vgwSessionLoggingNeeded</strong>: Is IBM Voice Gateway message exchange logging needed for debugging? 
</li>

<li class="sli"><strong class="ph b">numberOfS2TEngines</strong>: numberOfS2TEngines 
</li>

<li class="sli"><strong class="ph b">initDelayBeforeSendingDataToS2TEngines</strong>: Time in seconds to wait before sending data to the S2T engines. 
</li>

<li class="sli"><strong class="ph b">vgwStaleSessionPurgeInterval</strong>: Time interval in seconds during which the VGW source operator below should  do memory cleanup of any Voice Gateway sessions that end abruptly in the  middle of a voice call. 
</li>

<li class="sli"><strong class="ph b">ipv6Available</strong>: Is ipv6 protocol stack available in the Streams machine where the  IBMVoiceGatewaySource operator is going to run?  Most of the Linux machines will have ipv6. In that case,  you can keep the following line as it is.  If you don't have ipv6 in your environment, you can set the  following submission time value to false.  
</li>

</ul>

</div>

<div class="section">
</div>

<div class="section">
</div>

<div class="section"><h2 class="title sectiontitle splhead-2">SPL Source Code</h2>
  
</div>


<div class="section">
   <pre class="pre codeblock">

 public composite VoiceGatewayToStreamsToWatsonS2T {
 	param
 		// IBM Voice Gateway related submission time values are defined below.
 		// TLS port on which this application will listen for
 		// communicating with the IBM Voice Gateway.
 		expression&lt;uint32&gt; $tlsPort : 
 			(uint32)getSubmissionTimeValue("tlsPort", "443");
 		// User can optionally specify whether they want a non-TLS endpoint.
 		expression&lt;boolean&gt; $nonTlsEndpointNeeded : 
 			(boolean)getSubmissionTimeValue("nonTlsEndpointNeeded", "false");
 		// Non-TLS (Plain) port on which this application will
 		// (optionally) listen for communicating with the IBM Voice Gateway.
 		expression&lt;uint32&gt; $nonTlsPort : 
 			(uint32)getSubmissionTimeValue("nonTlsPort", "80");
 		// Server side certificate (.pem) file for the WebSocket server.
 		// It is necessary for the users to create a Root CA signed 
 		// server side certificate file and point to that file at the time of
 		// starting this application. If the user doesn't point to this file
 		// at the time of starting the application, then the application will
 		// look for a default file named ws-server.pem inside the etc sub-directory
 		// of the application. This certificate will be presented to the
 		// IBM Voice Gateway for validation when it establishes a WebSocket 
 		// connection with this application. For doing quick tests, you may save
 		// time and effort needed in getting a proper Root CA signed certificate 
 		// by going with a simpler option of creating your own self-signed 
 		// certificate. Please ensure that using a self-signed certificate is 
 		// allowed in your environment. We have provided a set of instructions to
 		// create a self signed certificate. Please refer to the following
 		// file in the etc sub-directory of this application:
 		// etc/creating-a-self-signed-certificate.txt
 		expression&lt;rstring&gt; $certificateFileName :
 			getSubmissionTimeValue("certificateFileName", "");
 		// Is live metrics needed for the IBMVoiceGatewaySource operator?
 		expression&lt;boolean&gt; $vgwLiveMetricsUpdateNeeded : 
 			(boolean)getSubmissionTimeValue("vgwLiveMetricsUpdateNeeded", "true");
 		// Is WebSocket library low level logging needed?
 		expression&lt;boolean&gt; $vgwWebsocketLoggingNeeded : 
 			(boolean)getSubmissionTimeValue("vgwWebsocketLoggingNeeded", "false");
 		// Is IBM Voice Gateway message exchange logging needed for debugging?
 		expression&lt;boolean&gt; $vgwSessionLoggingNeeded : 
 			(boolean)getSubmissionTimeValue("vgwSessionLoggingNeeded", "false");
 		//
 		// IBM Watson S2T related submission time values are defined below.
 		expression&lt;int32&gt; $numberOfS2TEngines :(int32)
 			getSubmissionTimeValue("numberOfS2TEngines", "10") ;
 		// Time in seconds to wait before sending data to the S2T engines.
 		expression&lt;float64&gt; $initDelayBeforeSendingDataToS2TEngines :
 			(float64)getSubmissionTimeValue("initDelayBeforeSendingDataToS2TEngines", "15.0"); 
 		// Time interval in seconds during which the VGW source operator below should
 		// do memory cleanup of any Voice Gateway sessions that end abruptly in the
 		// middle of a voice call.
 		expression&lt;uint32&gt; $vgwStaleSessionPurgeInterval :(uint32)
 			getSubmissionTimeValue("vgwStaleSessionPurgeInterval", "10800");
 		// Is ipv6 protocol stack available in the Streams machine where the
 		// IBMVoiceGatewaySource operator is going to run?
 		// Most of the Linux machines will have ipv6. In that case,
 		// you can keep the following line as it is.
 		// If you don't have ipv6 in your environment, you can set the
 		// following submission time value to false.
 		expression&lt;boolean&gt; $ipv6Available : (boolean)
 			getSubmissionTimeValue("ipv6Available", "true");
 
 	type
 		// The following is the schema of the first output stream for the
 		// IBMVoiceGatewaySource operator. The first four attributes are
 		// very important and the other ones are purely optional if some
 		// scenarios really require them.
 		// blob speech --&gt; Speech fragments of a live conversation as captured and sent by the IBM Voice Gateway.
 		// rstring vgwSessionId --&gt; Unique identifier of a voice call. 
 		// boolean isCustomerSpeechData --&gt; Every voice call will have a customer channel and an agent channel.
 		//                                  This attribute tells whether this output stream carries customer speech data or not.
 		// int32 vgwVoiceChannelNumber --&gt; This indicates the voice channel number i.e. 1 or 2.
 		//                                 Whoever (caller or agent) sends the first round of 
 		//                                 speech data bytes will get assigned a voice channel of 1. 
 		//                                 The next one to follow will get assigned a voice channel of 2.
 		// rstring id --&gt; This attribute is needed by the WatsonS2T operator. 
 		//                It is set to vgwSessionId_vgwVoiceChannelNumber
 		// rstring callerPhoneNumber --&gt; Details about the caller's phone number.
 		// rstring agentPhoneNumber --&gt; Details about the agent's phone number.
 		// int32 speechDataFragmentCnt --&gt; Number of fragments (tuples) emitted so far on a given channel (customer or agent) for a given vgwSessionId.
 		// int32 totalSpeechDataBytesReceived --&gt; Number of speech bytes received so far on a given channel (customer or agent) for a given vgwSessionId.
 		// int32 s2tEngineId --&gt; This attribute will be set in the next operator. (Please, read the comments there.)
 		// int32 s2tResultProcessorId --&gt; This attribute will be set in the next operator. (Please, read the comments there.)
 		BinarySpeech_t = blob speech, rstring vgwSessionId, boolean isCustomerSpeechData, 
 			int32 vgwVoiceChannelNumber, rstring id,
 			rstring callerPhoneNumber, rstring agentPhoneNumber, 
 			int32 speechDataFragmentCnt, int32 totalSpeechDataBytesReceived, 
 			int32 s2tEngineId, int32 s2tResultProcessorId;
 		// The following schema is for the second output stream of the
 		// IBMVoiceGatewaySource operator. It has three attributes indicating
 		// the speaker channel (vgwVoiceChannelNumber) of a given voice call (vgwSessionId) who
 		// got completed with the call as well as an indicator (isCustomerSpeechData) to 
 		// denote whether the speech data we received on this channel belonged
 		// to a caller or an agent.
 		EndOfCallSignal_t = rstring vgwSessionId, 
 			boolean isCustomerSpeechData, int32 vgwVoiceChannelNumber;
 
 	graph
 		// Ingest the speech data coming from the IBM Voice Gateway.
 		// Such speech data arrives here in multiple fragments directly from
 		// a live voice call. This operator is capable of receiving speech data
 		// from multiple calls that can all happen at the very same time between
 		// different pairs of speakers.
 		// It is very important to note that the IBM Voice Gateway will keep
 		// sending the speech data of the caller and the agent on two 
 		// voice channels i.e. one for the caller and the other for the agent.
 		// Irrespective of those two speakers talk or remain silent during the
 		// call, their assigned voice channel will always carry some binary
 		// data. That means, there is no way to know who is currently
 		// talking. This constraint limits us from sending only one of the
 		// channel's data to a single S2T engine at any given time.
 		// Instead, this constraint forces us to dedicate a single S2T engine
 		// per voice channel in a given voice call and keep sending the
 		// data being received on that channel continuously to that
 		// dedicated S2T engine irrespective of whether that channel carries
 		// silence or active speech data. In summary, we will need two
 		// S2T engines to do the Speech 2 Text for every ongoing voice call.
 		// So, you have to plan ahead of time about the number of S2T engines
 		// you will start for handling the maximum number of concurrent calls.
 		// As an example, for handling a maximum of 100 concurrent voice calls,
 		// you will have to start 200 S2T engines.
 		//
 		// In your own real-life applications, you may want to simply 
 		// copy and reuse the code from this example and then make the 
 		// changes only where it is really needed.
 		// This example presents the following application design pattern:
 		// IBMVoiceGatewaySource--&gt;Speech Data Router--&gt;S2T Engine-&gt;S2T Result Processor
 		// You should be fine to simply use the entire pattern as it is except for
 		// making changes in the S2T Result Processor composite and beyond to address
 		// your own needs of further analytics on the S2T results as well as
 		// specific ways of delivering the S2T results to other 
 		// downstream systems rather than only writing to files as this example does below.
 		(stream&lt;BinarySpeech_t&gt; BinarySpeechData as BSD;
 		 stream&lt;EndOfCallSignal_t&gt; EndOfCallSignal as EOCS) as VoiceGatewayInferface = 
 			IBMVoiceGatewaySource() {
 			logic
 				state: {
 					// Initialize the default TLS certificate file name if the 
 					// user didn't provide his or her own.
 					rstring _certificateFileName = 
 						($certificateFileName != "") ?
 						$certificateFileName : getThisToolkitDir() + "/etc/ws-server.pem";
 				}
 				
 			param
 				tlsPort: $tlsPort;
 				certificateFileName: _certificateFileName;
 				nonTlsEndpointNeeded: $nonTlsEndpointNeeded;
 				nonTlsPort: $nonTlsPort;
 				// Initial delay before generating the very first tuple.
 				// This is a one time delay when this operator starts up.
 				// This delay should give sufficient time for the
 				// WatsonS2T operator(s) to come up and be ready to
 				// receive the speech data tuples sent by this operator.
 				initDelay: $initDelayBeforeSendingDataToS2TEngines;
 				vgwLiveMetricsUpdateNeeded: $vgwLiveMetricsUpdateNeeded;
 				websocketLoggingNeeded: $vgwWebsocketLoggingNeeded;
 				vgwSessionLoggingNeeded: $vgwSessionLoggingNeeded;
 				vgwStaleSessionPurgeInterval: $vgwStaleSessionPurgeInterval;
 				ipv6Available: $ipv6Available;
 			
 			// Get these values via custom output functions	provided by this operator.
 			output
 				BSD: vgwSessionId = getIBMVoiceGatewaySessionId(),
 					isCustomerSpeechData = isCustomerSpeechData(),
 					vgwVoiceChannelNumber = getVoiceChannelNumber(),
 					callerPhoneNumber = getCallerPhoneNumber(),
 					agentPhoneNumber = getAgentPhoneNumber(),
 					speechDataFragmentCnt = getTupleCnt(),
 					totalSpeechDataBytesReceived = getTotalSpeechDataBytesReceived();
 		}
 
 		// We have to always route the speech data bytes (fragments) coming from  
 		// a given vgwSessionId_vgwVoiceChannelNumber to a particular 
 		// WatsonS2T operator instance available within a parallel region. 
 		// We already explained in detail in the previous operator's
 		// commentary section about why it must be done this way.
 		// This idea of pairing up a vgwSessionId_vgwVoiceChannelNumber combo
 		// to a particular parallel region channel is a must for the 
 		// speech data bytes of a given speaker in a voice call to always land in 
 		// the same WatsonS2T engine. This stickiness (a.k.a channel affinity) is
 		// important to continuously transcribe the speech data arriving on both the
 		// voice channels at all the time including the silence time of a speaker.
 		// This is needed because the IBM Voice Gateway keeps sending the 
 		// speech data bytes of both the speakers (whether active or silent) at 
 		// all the time on two voice channels by dedicating one channel to an
 		// agent and the other channel to the caller. So, this requires 
 		// extra logic to locate an unused parallel channel 
 		// i.e. an idle S2T engine to be assigned for a 
 		// given vgwSessionId_vgwVoiceChannelNumber.
 		// That special logic happens inside this operator.
 		(stream&lt;BinarySpeech_t&gt; BinarySpeechDataFragment as BSDF) as
 			BinarySpeechDataRouter = Custom(BinarySpeechData as BSD;
 			EndOfCallSignal as EOCS) {
 			logic
 				state: {
 					// This map tells us which UDP channel is processing a 
 					// given vgwSessionId_vgwVoiceChannelNumber combo.
 					mutable map&lt;rstring, int32&gt; _vgwSessionIdToUdpChannelMap = {};
 					// This list tells us which UDP channels are 
 					// idle at any given time.
 					mutable list&lt;int32&gt; _idleUdpChannelsList = 
 						prepareIdleUdpChannelsList($numberOfS2TEngines);
 					// This map tells us which UDP channel is going to process
 					// the given voice call's (i.e. vgwSessionId) transcription
 					// results in the S2TResultProcessor composite that appears
 					// below in this SPL source file.
 					mutable map&lt;rstring, int32&gt; _vgwSessionToResultProcessorChannelMap = {};
 					mutable BinarySpeech_t _oTuple = {};
 					mutable rstring _key = "";
 				}
 			
 				// Process the Binary Speech Data.
 				onTuple BSD: {
 					// Get the sessionId + channelNumber combo string.
 					_key = BSD.vgwSessionId + "_" + (rstring)BSD.vgwVoiceChannelNumber;
 					
 					// Check if this vgwSessionId_vgwVoiceChannelNumber combo already 
 					// has an S2T engine allocated for it via an UDP channel.
 					if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
 						// This is a speaker of an ongoing voice call who has 
 						// already been assigned to an S2T engine.
 						// Always send this speaker's speech data fragment to 
 						// that same S2T engine.
 						BSD.s2tEngineId = _vgwSessionIdToUdpChannelMap[_key];
 						// We can always assume that there is a preselected 
 						// S2T result processor UDP channel available for this 
 						// voice call (i.e. vgwSessionId). Because, it is already 
 						// done in the else block below when this voice call's 
 						// first speaker's speech data arrives here.
 						// Let us fetch and assign it here.
 						if (has(_vgwSessionToResultProcessorChannelMap, 
 							BSD.vgwSessionId) == true) {
 							BSD.s2tResultProcessorId = 
 								_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
 						} else {
 							// This should never happen since the call will end
 							// for both the speakers almost at the same time after 
 							// which there will be no speech data from any of the
 							// speakers participating in a given voice call.
 							// This else block is just part of defensive coding.
 							appTrc(Trace.error, 
 								"_XXXXX No S2T result processor engine available at this time for the " +
 								"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
 								". This should be a rare occurrence towards the very end of the call." + 
 								" We are not going to process the speech data bytes" +
 								" of this speaker in this voice call.");
 							return;
 						}
 					} else {
 						// If we are here, that means this is a brand new speaker of a
 						// voice call for whom we must find an idle UDP channel a.k.a
 						// an idle S2T engine that can process this speaker's speech data.
 						int32 myS2TEngineId = getAnIdleUdpChannel(_idleUdpChannelsList);
 						
 						if (myS2TEngineId == -1) {
 							// This is not good and we should never end up in this situation.
 							// This means we have not provisioned sufficient number of S2T engines to
 							// handle the maximum planned concurrent calls. We have to ignore this
 							// speech data fragment and hope that an idle UDP channel number will
 							// become available by the time the next speech data fragment for this
 							// particular vgwSessionId_vgwVoiceChannelNumber combo arrives here. 
 							if (BSD.speechDataFragmentCnt == 1) {
 								// Display this alert only for the very first data fragment of a 
 								// given speaker of a given voice call.
 								appTrc(Trace.error, "No idle S2T engine available at this time for the " +
 									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
 									". There are " + (rstring)$numberOfS2TEngines +
 									" S2T engines configured and they are all processing other" +
 									" voice calls at this time. Please start sufficient number of S2T engines" +
 									" next time to handle your maximum expected concurrent calls." +
 									" A rule of thumb is to have two S2T engines to process" +
 									" two speakers in every given concurrent voice call.");
 							}
 
 							return;	
 						} else {
 							// We got an idle S2T engine.
 							BSD.s2tEngineId = myS2TEngineId;
 							// Insert into the state map for future reference.
 							insertM(_vgwSessionIdToUdpChannelMap, 
 								_key, myS2TEngineId);
 								
 							// For this voice call (i.e. vgwSessionId), select a 
 							// single result processor UDP channel. Both speakers in this 
 							// same voice call will use that same result processor instance.
 							// This will ensure that the S2T results for both the speakers 
 							// will reach the same result processor.
 							if (has(_vgwSessionToResultProcessorChannelMap, 
 								BSD.vgwSessionId) == false) {
 								insertM(_vgwSessionToResultProcessorChannelMap,
 									BSD.vgwSessionId, myS2TEngineId);
 							} 
 							
 							// Set the S2T result processor id.
 							BSD.s2tResultProcessorId = 
 								_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
 						} // End of if (myS2TEngineId == -1)
 					} // End of if (has(_vgwSessionIdToUdpChannelMap, _key)
 
 					appTrc(Trace.debug, "vgwSessionId=" + BSD.vgwSessionId +
 						", isCustomerSpeechData=" + (rstring)BSD.isCustomerSpeechData +
 						", vgwVoiceChannelNumber=" + (rstring)BSD.vgwVoiceChannelNumber +
 						", speechDataFragmentCnt=" + (rstring)BSD.speechDataFragmentCnt +
 						", totalSpeechDataBytesReceived=" + 
 						(rstring)BSD.totalSpeechDataBytesReceived +
 						", s2tEngineId=" + (rstring)BSD.s2tEngineId +
 						", s2tResultProcessorId=" + (rstring)BSD.s2tResultProcessorId); 
 					// Submit this tuple.
 					submit(BSD, BSDF);
 				} // End of onTuple BSD
 				
 				// Process the end of voice call signal.
 				// Since there are two channels in every voice call,
 				// those two channels will carry their own "End S2T session"
 				// message from the Voice Gateway. The logic below takes care of
 				// handling two End of Call Signals for every voice call.
 				onTuple EOCS: {
 					// Get the allocated S2T engine id for a given 
 					// vgwSessionId_vgwVoiceChannelNumber combo.
 					// We should always have an S2T engine id. If not, that is a 
 					// case where the user didn't provision sufficient number of 
 					// S2T engines and there was no idle S2T engine available for that 
 					// given vgwSessionId_vgwVoiceChannelNumber combo. 
 					// This situation can be avoided by starting the application with a 
 					// sufficient number of S2T engines needed for the anticipated 
 					// maximum concurrent voice calls. A rule of thumb is to have 
 					// two S2T engines to process two speakers in every given 
 					// concurrent voice call.
 					//
 					// Get the sessionId + channelNumber combo string.
 					_key = EOCS.vgwSessionId + "_" + (rstring)EOCS.vgwVoiceChannelNumber;
 					
 					if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
 						// Let us send an empty blob to the WatsonS2T operator to indicate that
 						// this speaker of a given voice call is done.
 						_oTuple = (BinarySpeech_t){};
 						// Copy the three input tuple attributes that must
 						// match with that of the outgoing tuple.
 						assignFrom(_oTuple, EOCS);
 						// Assign the S2T engine id where this voice channel was
 						// getting processed until now.
 						_oTuple.s2tEngineId = _vgwSessionIdToUdpChannelMap[_key];
 						submit(_oTuple, BSDF);
 						// We are now done with this vgwSessionId_vgwVoiceChannelNumber combo.
 						// So, we can release the S2T engine and add it to the idle UDP channels list.
 						removeM(_vgwSessionIdToUdpChannelMap, _key);
 						appendM(_idleUdpChannelsList, _oTuple.s2tEngineId);
 					}
 					
 					// Since this voice call is ending, let us release the S2T result processor 
 					// instance that was allocated above for this voice call.
 					if (has(_vgwSessionToResultProcessorChannelMap, 
 						EOCS.vgwSessionId) == true) {
 						removeM(_vgwSessionToResultProcessorChannelMap, EOCS.vgwSessionId);
 					}
 				}
 				
 			config
 				threadedPort: queue(BSD, Sys.Wait), queue(EOCS, Sys.Wait);
 		} // End of Custom operator.
 
 		// Invoke one or more instances of the IBMWatsonSpeech2Text composite operator.
 		// You can send the audio data to this operator all at once or 
 		// you can send the audio data for the live-use case as it becomes
 		// available from your telephony network switch.
 		// Avoid feeding audio data coming from more than one data source into this 
 		// parallel region which may cause erroneous transcription results.
 		//
 		// A single input stream into this operator is the audio blob content.
 		// Please note that we are using an int32 value (exactly in the
 		// range of the available number of S2T engines) as our
 		// UDP partition key. That will help us to always use that
 		// preselected partition for a given voice call thereby 
 		// avoiding cross talk and mix up of data from multiple 
 		// voice calls landing in a given parallel channel etc.
 		@parallel(width = $numberOfS2TEngines, 
 		partitionBy=[{port=BSDF, attributes=[s2tEngineId]}])
 		(stream&lt;S2TResult_t&gt; MyS2TResult; 
 		 stream&lt;S2TDiagnosticsResult_t&gt; Diagnostics) as Speech2Text = 
 			IBMWatsonSpeech2Text(BinarySpeechDataFragment as BSDF) {
 			// You must not fuse the WatsonS2T operator instances since the
 			// WatsonS2T engine's library code needs to run on its own Linux process.
 			// So, you must keep each instance of this operator on its own PE (a.k.a Linux process).
 			config
 				placement : partitionExlocation("s2tpartition");
 		}
 
 		// Let us invoke the same number of S2T result processors as 
 		// there are S2T engines.
 		// Please note that we are using an int32 value (exactly in the
 		// range of the available number of S2T engines) as our
 		// UDP partition key. That will help us to always use that
 		// preselected partition for a given voice call thereby 
 		// avoiding cross talk and mix up of data from multiple 
 		// voice calls landing in a given parallel channel etc.
 		@parallel(width = $numberOfS2TEngines, 
 		partitionBy=[{port=MSR, attributes=[s2tResultProcessorId]}])
 		() as S2TResultProcessorSink = S2TResultProcessor(MyS2TResult as MSR) {
 		}
 
 		// =================================================================
 		// This code block is here purely for debugging purposes to 
 		// capture the raw audio data received in a given voice channel 
 		// for a given VGW session id. You have to remove the block comment
 		// below to activate this section of the code when you need it.
 		// The following block of code is good only for testing it with
 		// a single voice call. If you use the following section of code
 		// for multiple voice calls, then the speech data bytes from
 		// multiple calls will get mixed up. So, use it to debug the
 		// speech data contents for just a single voice call.
 		// In a real-life application, you will not have a need for
 		// this section of the code.
 		/*
 		(stream&lt;BinarySpeech_t&gt; BinarySpeechDataFragmentInVoiceChannel1;
 		 stream&lt;BinarySpeech_t&gt; BinarySpeechDataFragmentInVoiceChannel2)
 			as BinarySpeechDataFilterByVoiceChannel = 
 			Split(BinarySpeechDataFragment as BSDF) {
 			param
 				// We will only have either channel number 1 or 2.
 				// So, send the speech data received via channel number 1 to 
 				// output port index 0 (i.e. first port).
 				// Send the speech data received via channel number 2 to 
 				// output port index 1 (i.e. second port).
 				index: (BSDF.vgwVoiceChannelNumber == 1) ? 0 : 1;
 		}
 		
 		// Send only the blob part of the incoming tuple from voice channel 1.
 		(stream&lt;blob speech&gt; SpeechDataInVoiceChannel1 as SD1)
 			as SpeechDataFromVoiceChannel1 = 
 			Functor(BinarySpeechDataFragmentInVoiceChannel1) {	
 		}
 
 		// Send only the blob part of the incoming tuple from voice channel 2.
 		(stream&lt;blob speech&gt; SpeechDataInVoiceChannel2 as SD2)
 			as SpeechDataFromVoiceChannel2 = 
 			Functor(BinarySpeechDataFragmentInVoiceChannel2) {	
 		}
 		
 		// Write the speech data bytes received on voice channel 1 to its own binary file.
 		() as VoiceChannelSink1 = FileSink(SpeechDataInVoiceChannel1 as SD) {
 			param
 				// You can use this command to convert this 
 				// mulaw formatted audio file into a WAV file in order to
 				// play it using Audacity, QuickTime Player etc.:
 				file: "voice-channel1-speech-data.bin";
 				format: block;
 				flush: 1u;
 
 			config
 				threadedPort: queue(SD, Sys.Wait);
 		}
 
 		// Write the speech data bytes received on voice channel 2 to its own binary file.
 		() as VoiceChannelSink2 = FileSink(SpeechDataInVoiceChannel2 as SD) {
 			param
 				// You can use this command to convert this 
 				// mulaw formatted audio file into a WAV file in order to
 				// play it using Audacity, QuickTime Player etc.:
 				// ffmpeg -f mulaw -ar 8000 -i &lt;raw data&gt; -codec:a pcm_mulaw &lt;wav-filename&gt;
 				file: "voice-channel2-speech-data.bin";
 				format: block;
 				flush: 1u;
 
 			config
 				threadedPort: queue(SD, Sys.Wait);
 		}
 		*/
 		// =================================================================
 	config restartable : false;
 } // End of the main composite.

   </pre>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Types</h2>
  
</div>

<div class="section" id="spldoc_compilationunit__type__S2TResult_t"><h2 class="title sectiontitle splpart">S2TResult_t</h2>
  
</div>
<div class="section">

<p class="p">This S2T result type contains many attributes to  demonstrate all the basic and very advanced features of  the Watson S2T engine. Not all real-life applications will need  all these attributes. You can decide to include or omit these  attributes based on the specific S2T features your application will need.  Trimming the unused attributes will also help in  reducing the S2T processing overhead and in turn  help in receiving the S2T results faster.  Read the com.ibm.streams.speech2text toolkit documentation to learn about  what features are available, how they work and how different attributes are  related to those features 
</p>

</div>

<div class="section">
   <p class="p">
<tt class="ph tt">S2TResult_t = rstring vgwSessionId, boolean isCustomerSpeechData, int32 vgwVoiceChannelNumber, rstring callerPhoneNumber, rstring agentPhoneNumber, int32 speechDataFragmentCnt, int32 totalSpeechDataBytesReceived, int32 s2tEngineId, int32 s2tResultProcessorId, rstring id, float64 utteranceStartTime, float64 utteranceEndTime, int32 utteranceNumber, rstring utteranceText, float64 utteranceConfidence, list&lt;rstring&gt; utteranceWords, list&lt;int32&gt; utteranceSpeakers, list&lt;float64&gt; utteranceWordConfidences, list&lt;rstring&gt; nBestHypotheses, list&lt;rstring&gt; wordAlternatives, list&lt;list&lt;float64&gt;&gt; wordAlternativesConfidences, list&lt;float64&gt; wordAlternativesStartTimes;</tt>
   </p>

</div>

<div class="section" id="spldoc_compilationunit__type__S2TDiagnosticsResult_t"><h2 class="title sectiontitle splpart">S2TDiagnosticsResult_t</h2>
  
</div>
<div class="section">

<p class="p">The result type for the diagnostics 
</p>

</div>

<div class="section">
   <p class="p">
<tt class="ph tt">S2TDiagnosticsResult_t = rstring log;</tt>
   </p>

</div>

</div>


</body>
</html>