<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en-us" lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="reference"/>
<meta name="DC.Title" content="SPL File VoiceGatewayToStreamsToWatsonSTT.spl"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="spldoc_compilationunit"/>
<link rel="stylesheet" type="text/css" href="../../html/commonltr.css"/>
<link rel="stylesheet" type="text/css" href="../../html/spldoc.css"/>
<title>SPL File VoiceGatewayToStreamsToWatsonSTT.spl</title>
</head>
<body id="spldoc_compilationunit">


<h1 class="title topictitle1">SPL File <tt class="ph tt">VoiceGatewayToStreamsToWatsonSTT.spl</tt></h1>

<div class="body refbody">
<div class="section">
<p class="p">
<a class="xref" href="../toolkits/toolkits.html">Gateway to the IBM Speech To Text (STT) cloud service samples</a> &gt; <a class="xref" href="tk$VoiceGatewayToStreamsToWatsonSTT.html">VoiceGatewayToStreamsToWatsonSTT 1.0.1</a> &gt; <a class="xref" href="ns$com.ibm.streamsx.sttgateway.sample.watsonstt.html">com.ibm.streamsx.sttgateway.sample.watsonstt</a> &gt; VoiceGatewayToStreamsToWatsonSTT.spl</p>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Content</h2>
  
  <dl class="dl">
    <dt class="dt dlterm"/>
<dd class="dd"/>

    
      <dt class="dt dlterm splhead-2">Operators</dt>

      <dd class="dd">
<ul class="sl simple">
<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsonstt$VoiceGatewayToStreamsToWatsonSTT.html#spldoc_compilationunit__composite_operator__VoiceGatewayToStreamsToWatsonSTT">VoiceGatewayToStreamsToWatsonSTT</a></strong>: This example demonstrates the integration of the following three products to  achieve Real-Time Speech-To-Text transcription to get the text ready for  any further analytics.
</li>

</ul>

      </dd>

    
    
      <dt class="dt dlterm splhead-2">Types</dt>

      <dd class="dd">
<ul class="sl simple">
<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsonstt$VoiceGatewayToStreamsToWatsonSTT.html#spldoc_compilationunit__type__STTResult_t">STTResult_t</a></strong>: This STT result type contains many attributes to  demonstrate all the basic and very advanced features of  the Watson STT service.
</li>

</ul>

      </dd>

    
  </dl>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Composites</h2>
  
</div>

<div class="section" id="spldoc_compilationunit__composite_operator__VoiceGatewayToStreamsToWatsonSTT"><h2 class="title sectiontitle splpart">composite VoiceGatewayToStreamsToWatsonSTT</h2>
  
</div>

<div class="section splgraph">
  <embed class="image" src="../../image/tk$VoiceGatewayToStreamsToWatsonSTT/op$com.ibm.streamsx.sttgateway.sample.watsonstt$VoiceGatewayToStreamsToWatsonSTT.svg" width="442" height="154"/>
</div>

<div class="section">

<p class="p">This example demonstrates the integration of the following three products to  achieve Real-Time Speech-To-Text transcription to get the text ready for  any further analytics.
</p>

<div class="p">
<pre class="pre codeblock">
	1) IBM Voice Gateway 
	2) IBM Streams 
	3) IBM Watson Speech To Text (on Cloud Pak for Data or on IBM public cloud) 
</pre>


</div>

<p class="p">These three products will work in the following sequence:
</p>

<div class="p">
<pre class="pre codeblock">
	Your Telephony SIPREC--&gt;IBM Voice Gateway--&gt;IBM Streams--&gt;IBM Watson STT
</pre>

You can build this example from command line via the make command by using the  Makefile available in the top-level directory of this example. It will be  necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by  pointing it to the full path of your  streamsx.sttgateway/com.ibm.streamsx.sttgateway directory. 
</div>

<p class="p">If you want to build this example inside the Streams Studio, there are certain  build configuration settings needed. Please refer to the streamsx.sttgateway  toolkit documentation to learn more about those Streams Studio configuration settings. 
</p>

</div>

<div class="section"><h2 class="title sectiontitle">Parameters</h2>

<ul class="sl simple">
<li class="sli"><strong class="ph b">tlsPort</strong>: TLS port on which this application will listen for communicating with the IBM Voice Gateway. 
</li>

<li class="sli"><strong class="ph b">nonTlsEndpointNeeded</strong>: User can optionally specify whether they want a non-TLS endpoint. 
</li>

<li class="sli"><strong class="ph b">nonTlsPort</strong>: Non-TLS (Plain) port on which this application will (optionally) listen for communicating with the IBM Voice Gateway. 
</li>

<li class="sli"><strong class="ph b">certificateFileName</strong>: Server side certificate (.pem) file for the WebSocket server.  It is necessary for the users to create a Root CA signed   server side certificate file and point to that file at the time of  starting this application. If the user doesn't point to this file  at the time of starting the application, then the application will  look for a default file named ws-server.pem inside the etc sub-directory  of the application. This certificate will be presented to the  IBM Voice Gateway for validation when it establishes a WebSocket   connection with this application. For doing quick tests, you may save  time and effort needed in getting a proper Root CA signed certificate   by going with a simpler option of creating your own self-signed   certificate. Please ensure that using a self-signed certificate is   allowed in your environment. We have provided a set of instructions to  create a self signed certificate. Please refer to the following  file in the etc sub-directory of this application:  etc/creating-a-self-signed-certificate.txt 
</li>

<li class="sli"><strong class="ph b">vgwLiveMetricsUpdateNeeded</strong>: Is live metrics needed for the IBMVoiceGatewaySource operator? 
</li>

<li class="sli"><strong class="ph b">vgwWebsocketLoggingNeeded</strong>: Is WebSocket library low level logging needed? 
</li>

<li class="sli"><strong class="ph b">vgwSessionLoggingNeeded</strong>: Is IBM Voice Gateway message exchange logging needed for debugging? 
</li>

<li class="sli"><strong class="ph b">sttApiKey</strong>: IBM Watson STT related submission time values are defined below.  IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the   Watson STT cloud service. For the STT service on IBM Public Cloud,   one must use the unexpired IAM access token (generated by using your   IBM Public cloud STT service instance's API key).   So, user must provide here his/her API key. We have some logic below that   will use the user provided API key to generate the IAM access token and   send that to the WatsonSTT operator.  There is additional logic available below to keep refreshing that  IAM access token periodically in order for it to stay unexpired.  You should leave this submission time value empty when not using STT on IBM public cloud.  https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen 
</li>

<li class="sli"><strong class="ph b">sttIAMTokenURL</strong>: Specify either the public cloud IAM Token fetch/refresh URL. 
</li>

<li class="sli"><strong class="ph b">sttOnCP4DAccessToken</strong>: Specify the access token refresh interval in minutes. 
</li>

<li class="sli"><strong class="ph b">numberOfSTTEngines</strong>: Number of stt engines 
</li>

<li class="sli"><strong class="ph b">initDelayBeforeSendingDataToSttEngines</strong>: Time in seconds to wait before sending data to the STT engines. 
</li>

<li class="sli"><strong class="ph b">vgwStaleSessionPurgeInterval</strong>: Time interval in seconds during which the VGW source operator below should  do memory cleanup of any Voice Gateway sessions that end abruptly in the  middle of a voice call. 
</li>

<li class="sli"><strong class="ph b">ipv6Available</strong>: Is ipv6 protocol stack available in the Streams machine where the  IBMVoiceGatewaySource operator is going to run?  Most of the Linux machines will have ipv6. In that case,  you can keep the following line as it is.  If you don't have ipv6 in your environment, you can set the  following submission time value to false. 
</li>

<li class="sli"><strong class="ph b">sttUri</strong>: sttUri default wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize 
</li>

<li class="sli"><strong class="ph b">sttBaseLanguageModel</strong>: sttBaseLanguageModel 
</li>

<li class="sli"><strong class="ph b">contentType</strong>: contentType 
</li>

<li class="sli"><strong class="ph b">nonFinalUtterancesNeeded</strong>: nonFinalUtterancesNeeded 
</li>

<li class="sli"><strong class="ph b">baseModelVersion</strong>: baseModelVersion 
</li>

<li class="sli"><strong class="ph b">customizationId</strong>: customizationId 
</li>

<li class="sli"><strong class="ph b">acousticCustomizationId</strong>: acousticCustomizationId 
</li>

<li class="sli"><strong class="ph b">customizationWeight</strong>: customizationWeight 
</li>

<li class="sli"><strong class="ph b">maxUtteranceAlternatives</strong>: maxUtteranceAlternatives 
</li>

<li class="sli"><strong class="ph b">sttRequestLogging</strong>: sttRequestLogging 
</li>

<li class="sli"><strong class="ph b">filterProfanity</strong>: filterProfanity 
</li>

<li class="sli"><strong class="ph b">wordAlternativesThreshold</strong>: wordAlternativesThreshold
</li>

<li class="sli"><strong class="ph b">smartFormattingNeeded</strong>: smartFormattingNeeded
</li>

<li class="sli"><strong class="ph b">keywordsSpottingThreshold</strong>: keywordsSpottingThreshold
</li>

<li class="sli"><strong class="ph b">keywordsToBeSpotted</strong>: keywordsToBeSpotted
</li>

<li class="sli"><strong class="ph b">sttWebsocketLoggingNeeded</strong>: sttWebsocketLoggingNeeded
</li>

<li class="sli"><strong class="ph b">cpuYieldTimeInAudioSenderThread</strong>: cpuYieldTimeInAudioSenderThread
</li>

<li class="sli"><strong class="ph b">waitTimeBeforeSTTServiceConnectionRetry</strong>: waitTimeBeforeSTTServiceConnectionRetry
</li>

<li class="sli"><strong class="ph b">connectionAttemptsThreshold</strong>: connectionAttemptsThreshold
</li>

<li class="sli"><strong class="ph b">sttLiveMetricsUpdateNeeded</strong>: sttLiveMetricsUpdateNeeded 
</li>

</ul>

</div>

<div class="section">
</div>

<div class="section">
</div>

<div class="section"><h2 class="title sectiontitle splhead-2">SPL Source Code</h2>
  
</div>


<div class="section">
   <pre class="pre codeblock">

 public composite VoiceGatewayToStreamsToWatsonSTT {
 	param
 		// IBM Voice Gateway related submission time values are defined below.
 		expression&lt;uint32&gt; $tlsPort : (uint32)getSubmissionTimeValue("tlsPort", "443");
 		expression&lt;boolean&gt; $nonTlsEndpointNeeded : (boolean)getSubmissionTimeValue("nonTlsEndpointNeeded", "false");
 		expression&lt;uint32&gt; $nonTlsPort : 
 			(uint32)getSubmissionTimeValue("nonTlsPort", "80");
 		// Server side certificate (.pem) file for the WebSocket server.
 		// It is necessary for the users to create a Root CA signed 
 		// server side certificate file and point to that file at the time of
 		// starting this application. If the user doesn't point to this file
 		// at the time of starting the application, then the application will
 		// look for a default file named ws-server.pem inside the etc sub-directory
 		// of the application. This certificate will be presented to the
 		// IBM Voice Gateway for validation when it establishes a WebSocket 
 		// connection with this application. For doing quick tests, you may save
 		// time and effort needed in getting a proper Root CA signed certificate 
 		// by going with a simpler option of creating your own self-signed 
 		// certificate. Please ensure that using a self-signed certificate is 
 		// allowed in your environment. We have provided a set of instructions to
 		// create a self signed certificate. Please refer to the following
 		// file in the etc sub-directory of this application:
 		// etc/creating-a-self-signed-certificate.txt
 		expression&lt;rstring&gt; $certificateFileName :
 			getSubmissionTimeValue("certificateFileName", "");
 		// Is live metrics needed for the IBMVoiceGatewaySource operator?
 		expression&lt;boolean&gt; $vgwLiveMetricsUpdateNeeded : 
 			(boolean)getSubmissionTimeValue("vgwLiveMetricsUpdateNeeded", "true");
 		// Is WebSocket library low level logging needed?
 		expression&lt;boolean&gt; $vgwWebsocketLoggingNeeded : 
 			(boolean)getSubmissionTimeValue("vgwWebsocketLoggingNeeded", "false");
 		// Is IBM Voice Gateway message exchange logging needed for debugging?
 		expression&lt;boolean&gt; $vgwSessionLoggingNeeded : 
 			(boolean)getSubmissionTimeValue("vgwSessionLoggingNeeded", "false");
 		//
 		// IBM Watson STT related submission time values are defined below.
 		// IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the 
 		// Watson STT cloud service. For the STT service on IBM Public Cloud, 
 		// one must use the unexpired IAM access token (generated by using your 
 		// IBM Public cloud STT service instance's API key). 
 		// So, user must provide here his/her API key. We have some logic below that 
 		// will use the user provided API key to generate the IAM access token and 
 		// send that to the WatsonSTT operator.
 		// There is additional logic available below to keep refreshing that
 		// IAM access token periodically in order for it to stay unexpired.
 		// You should leave this submission time value empty when not using STT on IBM public cloud.
 		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
 		expression&lt;rstring&gt; $sttApiKey : getSubmissionTimeValue("sttApiKey", "");
 		// Specify either the public cloud IAM Token fetch/refresh URL.
 		expression&lt;rstring&gt; $sttIAMTokenURL : 
 			getSubmissionTimeValue("sttIAMTokenURL", "https://iam.cloud.ibm.com/identity/token");
 		// Specify the access token refresh interval in minutes.
 		expression&lt;rstring&gt; $sttOnCP4DAccessToken : getSubmissionTimeValue("sttOnCP4DAccessToken", "");
 		expression&lt;int32&gt; $numberOfSTTEngines :(int32)
 			getSubmissionTimeValue("numberOfSTTEngines", "10") ;
 		// Time in seconds to wait before sending data to the STT engines.
 		expression&lt;float64&gt; $initDelayBeforeSendingDataToSttEngines :
 			(float64)getSubmissionTimeValue("initDelayBeforeSendingDataToSttEngines", "15.0"); 
 		// Time interval in seconds during which the VGW source operator below should
 		// do memory cleanup of any Voice Gateway sessions that end abruptly in the
 		// middle of a voice call.
 		expression&lt;uint32&gt; $vgwStaleSessionPurgeInterval :(uint32)
 			getSubmissionTimeValue("vgwStaleSessionPurgeInterval", "10800");
 		// Is ipv6 protocol stack available in the Streams machine where the
 		// IBMVoiceGatewaySource operator is going to run?
 		// Most of the Linux machines will have ipv6. In that case,
 		// you can keep the following line as it is.
 		// If you don't have ipv6 in your environment, you can set the
 		// following submission time value to false.
 		expression&lt;boolean&gt; $ipv6Available : (boolean)
 			getSubmissionTimeValue("ipv6Available", "true");
 		// 
 		// Following are the WatsonSTT operator related submission time values.
 		//
 		expression&lt;rstring&gt; $sttUri : getSubmissionTimeValue("sttUri",
 			"wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize");
 		expression&lt;rstring&gt; $sttBaseLanguageModel : 
 			getSubmissionTimeValue("sttBaseLanguageModel", "en-US_NarrowbandModel");
 		expression&lt;rstring&gt; $contentType : 
 			getSubmissionTimeValue("contentType", "audio/wav");
 		expression&lt;boolean&gt; $nonFinalUtterancesNeeded:
 			(boolean)getSubmissionTimeValue("nonFinalUtterancesNeeded", "false");
 		expression&lt;rstring&gt; $baseModelVersion : 
 			getSubmissionTimeValue("baseModelVersion", "");
 		expression&lt;rstring&gt; $customizationId : 
 			getSubmissionTimeValue("customizationId", "");
 		expression&lt;rstring&gt; $acousticCustomizationId : 
 			getSubmissionTimeValue("acousticCustomizationId", "");
 		expression&lt;float64&gt; $customizationWeight : 
 			(float64)getSubmissionTimeValue("customizationWeight", "0.30");
 		expression&lt;int32&gt; $maxUtteranceAlternatives : 
 			(int32)getSubmissionTimeValue("maxUtteranceAlternatives", "1");	
 		expression&lt;boolean&gt; $sttRequestLogging : 
 			(boolean)getSubmissionTimeValue("sttRequestLogging", "false");
 		expression&lt;boolean&gt; $filterProfanity : 
 			(boolean)getSubmissionTimeValue("filterProfanity", "false");
 		expression&lt;float64&gt; $wordAlternativesThreshold : 
 			(float64)getSubmissionTimeValue("wordAlternativesThreshold", "0.0");
 		expression&lt;boolean&gt; $smartFormattingNeeded : 
 			(boolean)getSubmissionTimeValue("smartFormattingNeeded", "false");
 		expression&lt;float64&gt; $keywordsSpottingThreshold : 
 			(float64)getSubmissionTimeValue("keywordsSpottingThreshold", "0.0");
 		expression&lt;list&lt;rstring&gt;&gt; $keywordsToBeSpotted : 
 			(list&lt;rstring&gt;)getSubmissionTimeValue("keywordsToBeSpotted", "[]");	
 		expression&lt;boolean&gt; $sttWebsocketLoggingNeeded : 
 			(boolean)getSubmissionTimeValue("sttWebsocketLoggingNeeded", "false");
 		expression&lt;float64&gt; $cpuYieldTimeInAudioSenderThread : 
 			(float64)getSubmissionTimeValue("cpuYieldTimeInAudioSenderThread", "0.001");
 		expression&lt;float64&gt; $waitTimeBeforeSTTServiceConnectionRetry : 
 			(float64)getSubmissionTimeValue("waitTimeBeforeSTTServiceConnectionRetry", "3.0");
 		expression&lt;int32&gt; $connectionAttemptsThreshold : 
 			(int32)getSubmissionTimeValue("connectionAttemptsThreshold", "10");
 		expression&lt;boolean&gt; $sttLiveMetricsUpdateNeeded : 
 			(boolean)getSubmissionTimeValue("sttLiveMetricsUpdateNeeded", "true");
 
 	type
 		// The following is the schema of the first output stream for the
 		// IBMVoiceGatewaySource operator. The first four attributes are
 		// very important and the other ones are purely optional if some
 		// scenarios really require them.
 		// blob speech --&gt; Speech fragments of a live conversation as captured and sent by the IBM Voice Gateway.
 		// rstring vgwSessionId --&gt; Unique identifier of a voice call. 
 		// boolean isCustomerSpeechData --&gt; Every voice call will have a customer channel and an agent channel.
 		//                                  This attribute tells whether this output stream carries customer speech data or not.
 		// int32 vgwVoiceChannelNumber --&gt; This indicates the voice channel number i.e. 1 or 2.
 		//                                 Whoever (caller or agent) sends the first round of 
 		//                                 speech data bytes will get assigned a voice channel of 1. 
 		//                                 The next one to follow will get assigned a voice channel of 2.
 		// rstring callerPhoneNumber --&gt; Details about the caller's phone number.
 		// rstring agentPhoneNumber --&gt; Details about the agent's phone number.
 		// int32 speechDataFragmentCnt --&gt; Number of fragments (tuples) emitted so far on a given channel (customer or agent) for a given vgwSessionId.
 		// int32 totalSpeechDataBytesReceived --&gt; Number of speech bytes received so far on a given channel (customer or agent) for a given vgwSessionId.
 		// int32 sttEngineId --&gt; This attribute will be set in the next operator. (Please, read the comments there.)
 		// int32 sttResultProcessorId --&gt; This attribute will be set in the next operator. (Please, read the comments there.)
 		BinarySpeech_t = blob speech, rstring vgwSessionId, boolean isCustomerSpeechData, 
 			int32 vgwVoiceChannelNumber, rstring callerPhoneNumber,
 			rstring agentPhoneNumber, int32 speechDataFragmentCnt,
 			int32 totalSpeechDataBytesReceived, int32 sttEngineId, 
 			int32 sttResultProcessorId;
 		// The following schema is for the second output stream of the
 		// IBMVoiceGatewaySource operator. It has three attributes indicating
 		// the speaker channel (vgwVoiceChannelNumber) of a given voice call (vgwSessionId) who
 		// got completed with the call as well as an indicator (isCustomerSpeechData) to 
 		// denote whether the speech data we received on this channel belonged
 		// to a caller or an agent.
 		EndOfCallSignal_t = rstring vgwSessionId, 
 			boolean isCustomerSpeechData, int32 vgwVoiceChannelNumber;
 		
 	graph
 		// Ingest the speech data coming from the IBM Voice Gateway.
 		// Such speech data arrives here in multiple fragments directly from
 		// a live voice call. This operator is capable of receiving speech data
 		// from multiple calls that can all happen at the very same time between
 		// different pairs of speakers.
 		// It is very important to note that the IBM Voice Gateway will keep
 		// sending the speech data of the caller and the agent on two 
 		// voice channels i.e. one for the caller and the other for the agent.
 		// Irrespective of those two speakers talk or remain silent during the
 		// call, their assigned voice channel will always carry some binary
 		// data. That means, there is no way to know who is currently
 		// talking. This constraint limits us from sending only one of the
 		// channel's data to a single STT engine at any given time.
 		// Instead, this constraint forces us to dedicate a single STT engine
 		// per voice channel in a given voice call and keep sending the
 		// data being received on that channel continuously to that
 		// dedicated STT engine irrespective of whether that channel carries
 		// silence or active speech data. In summary, we will need two
 		// STT engines to do the Speech To Text for every ongoing voice call.
 		// So, you have to plan ahead of time about the number of STT engines
 		// you will start for handling the maximum number of concurrent calls.
 		// As an example, for handling a maximum of 100 concurrent voice calls,
 		// you will have to start 200 STT engines.
 		//
 		// In your own real-life applications, you may want to simply 
 		// copy and reuse the code from this example and then make the 
 		// changes only where it is really needed.
 		// This example presents the following application design pattern:
 		// IBMVoiceGatewaySource--&gt;Speech Data Router--&gt;STT Engine-&gt;STT Result Processor
 		// You should be fine to simply use the entire pattern as it is except for
 		// making changes in the STT Result Processor composite and beyond to address
 		// your own needs of further analytics on the STT results as well as
 		// specific ways of delivering the STT results to other 
 		// downstream systems rather than only writing to files as this example does below.
 		(stream&lt;BinarySpeech_t&gt; BinarySpeechData as BSD;
 		 stream&lt;EndOfCallSignal_t&gt; EndOfCallSignal as EOCS) as VoiceGatewayInferface = 
 			IBMVoiceGatewaySource() {
 			logic
 				state: {
 					// Initialize the default TLS certificate file name if the 
 					// user didn't provide his or her own.
 					rstring _certificateFileName = 
 						($certificateFileName != "") ?
 						$certificateFileName : getThisToolkitDir() + "/etc/ws-server.pem";
 				}
 				
 			param
 				tlsPort: $tlsPort;
 				certificateFileName: _certificateFileName;
 				nonTlsEndpointNeeded: $nonTlsEndpointNeeded;
 				nonTlsPort: $nonTlsPort;
 				// Initial delay before generating the very first tuple.
 				// This is a one time delay when this operator starts up.
 				// This delay should give sufficient time for the
 				// WatsonSTT operator(s) to come up and be ready to
 				// receive the speech data tuples sent by this operator.
 				initDelay: $initDelayBeforeSendingDataToSttEngines;
 				vgwLiveMetricsUpdateNeeded: $vgwLiveMetricsUpdateNeeded;
 				websocketLoggingNeeded: $vgwWebsocketLoggingNeeded;
 				vgwSessionLoggingNeeded: $vgwSessionLoggingNeeded;
 				vgwStaleSessionPurgeInterval: $vgwStaleSessionPurgeInterval;
 				ipv6Available: $ipv6Available;
 			
 			// Get these values via custom output functions	provided by this operator.
 			output
 				BSD: vgwSessionId = getIBMVoiceGatewaySessionId(),
 					isCustomerSpeechData = isCustomerSpeechData(),
 					vgwVoiceChannelNumber = getVoiceChannelNumber(),
 					callerPhoneNumber = getCallerPhoneNumber(),
 					agentPhoneNumber = getAgentPhoneNumber(),
 					speechDataFragmentCnt = getTupleCnt(),
 					totalSpeechDataBytesReceived = getTotalSpeechDataBytesReceived();
 		}
 
 		// We have to always route the speech data bytes (fragments) coming from  
 		// a given vgwSessionId_vgwVoiceChannelNumber to a particular 
 		// WatsonSTT operator instance available within a parallel region. 
 		// We already explained in detail in the previous operator's
 		// commentary section about why it must be done this way.
 		// This idea of pairing up a vgwSessionId_vgwVoiceChannelNumber combo
 		// to a particular parallel region channel is a must for the 
 		// speech data bytes of a given speaker in a voice call to always land in 
 		// the same WatsonSTT engine. This stickiness (a.k.a channel affinity) is
 		// important to continuously transcribe the speech data arriving on both the
 		// voice channels at all the time including the silence time of a speaker.
 		// This is needed because the IBM Voice Gateway keeps sending the 
 		// speech data bytes of both the speakers (whether active or silent) at 
 		// all the time on two voice channels by dedicating one channel to an
 		// agent and the other channel to the caller. So, this requires 
 		// extra logic to locate an unused parallel channel 
 		// i.e. an idle STT engine to be assigned for a 
 		// given vgwSessionId_vgwVoiceChannelNumber.
 		// That special logic happens inside this operator.
 		(stream&lt;BinarySpeech_t&gt; BinarySpeechDataFragment as BSDF) as
 			BinarySpeechDataRouter = Custom(BinarySpeechData as BSD;
 			EndOfCallSignal as EOCS) {
 			logic
 				state: {
 					// This map tells us which UDP channel is processing a 
 					// given vgwSessionId_vgwVoiceChannelNumber combo.
 					mutable map&lt;rstring, int32&gt; _vgwSessionIdToUdpChannelMap = {};
 					// This list tells us which UDP channels are 
 					// idle at any given time.
 					mutable list&lt;int32&gt; _idleUdpChannelsList = 
 						prepareIdleUdpChannelsList($numberOfSTTEngines);
 					// This map tells us which UDP channel is going to process
 					// the given voice call's (i.e. vgwSessionId) transcription
 					// results in the STTResultProcessor composite that appears
 					// below in this SPL source file.
 					mutable map&lt;rstring, int32&gt; _vgwSessionToResultProcessorChannelMap = {};
 					mutable BinarySpeech_t _oTuple = {};
 					mutable rstring _key = "";
 				}
 			
 				// Process the Binary Speech Data.
 				onTuple BSD: {
 					// Get the sessionId + channelNumber combo string.
 					_key = BSD.vgwSessionId + "_" + (rstring)BSD.vgwVoiceChannelNumber;
 					
 					// Check if this vgwSessionId_vgwVoiceChannelNumber combo already 
 					// has an STT engine allocated for it via an UDP channel.					
 					if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
 						// This is a speaker of an ongoing voice call who has 
 						// already been assigned to an STT engine.
 						// Always send this speaker's speech data fragment to 
 						// that same STT engine.
 						BSD.sttEngineId = _vgwSessionIdToUdpChannelMap[_key];
 						// We can always assume that there is a preselected 
 						// STT result processor UDP channel available for this 
 						// voice call (i.e. vgwSessionId). Because, it is already 
 						// done in the else block below when this voice call's 
 						// first speaker's speech data arrives here.
 						// Let us fetch and assign it here.
 						if (has(_vgwSessionToResultProcessorChannelMap, 
 							BSD.vgwSessionId) == true) {
 							BSD.sttResultProcessorId = 
 								_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
 						} else {
 							// This should never happen since the call will end
 							// for both the speakers almost at the same time after 
 							// which there will be no speech data from any of the
 							// speakers participating in a given voice call.
 							// This else block is just part of defensive coding.
 							appTrc(Trace.error, 
 								"_XXXXX No STT result processor engine available at this time for the " +
 								"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
 								". This should be a rare occurrence towards the very end of the call." + 
 								" We are not going to process the speech data bytes" +
 								" of this speaker in this voice call.");
 							return;
 						}
 					} else {
 						// If we are here, that means this is a brand new speaker of a
 						// voice call for whom we must find an idle UDP channel a.k.a
 						// an idle STT engine that can process this speaker's speech data.
 						int32 mySttEngineId = getAnIdleUdpChannel(_idleUdpChannelsList);
 						
 						if (mySttEngineId == -1) {
 							// This is not good and we should never end up in this situation.
 							// This means we have not provisioned sufficient number of STT engines to
 							// handle the maximum planned concurrent calls. We have to ignore this
 							// speech data fragment and hope that an idle UDP channel number will
 							// become available by the time the next speech data fragment for this
 							// particular vgwSessionId_vgwVoiceChannelNumber combo arrives here. 
 							if (BSD.speechDataFragmentCnt == 1) {
 								// Display this alert only for the very first data fragment of a 
 								// given speaker of a given voice call.
 								appTrc(Trace.error, "No idle STT engine available at this time for the " +
 									"vgwSessionId_vgwVoiceChannelNumber: " + _key + 
 									". There are " + (rstring)$numberOfSTTEngines +
 									" STT engines configured and they are all processing other" +
 									" voice calls at this time. Please start sufficient number of STT engines" +
 									" next time to handle your maximum expected concurrent calls." +
 									" A rule of thumb is to have two STT engines to process" +
 									" two speakers in every given concurrent voice call.");
 							}
 
 							return;
 						} else {
 							// We got an idle STT engine.
 							BSD.sttEngineId = mySttEngineId;
 							// Insert into the state map for future reference.
 							insertM(_vgwSessionIdToUdpChannelMap, 
 								_key, mySttEngineId);
 								
 							// For this voice call (i.e. vgwSessionId), select a 
 							// single result processor UDP channel. Both speakers in this 
 							// same voice call will use that same result processor instance.
 							// This will ensure that the STT results for both the speakers 
 							// will reach the same result processor.
 							if (has(_vgwSessionToResultProcessorChannelMap, 
 								BSD.vgwSessionId) == false) {
 								insertM(_vgwSessionToResultProcessorChannelMap,
 									BSD.vgwSessionId, mySttEngineId);
 							} 
 							
 							// Set the STT result processor id.
 							BSD.sttResultProcessorId = 
 								_vgwSessionToResultProcessorChannelMap[BSD.vgwSessionId];
 						} // End of if (mySttEngineId == -1)
 					} // End of if (has(_vgwSessionIdToUdpChannelMap, _key)
 
 					appTrc(Trace.debug, "vgwSessionId=" + BSD.vgwSessionId +
 						", isCustomerSpeechData=" + (rstring)BSD.isCustomerSpeechData +
 						", vgwVoiceChannelNumber=" + (rstring)BSD.vgwVoiceChannelNumber +
 						", speechDataFragmentCnt=" + (rstring)BSD.speechDataFragmentCnt +
 						", totalSpeechDataBytesReceived=" + 
 						(rstring)BSD.totalSpeechDataBytesReceived +
 						", sttEngineId=" + (rstring)BSD.sttEngineId +
 						", sttResultProcessorId=" + (rstring)BSD.sttResultProcessorId); 
 					// Submit this tuple.
 					submit(BSD, BSDF);
 				} // End of onTuple BSD
 				
 				// Process the end of voice call signal.
 				// Since there are two channels in every voice call,
 				// those two channels will carry their own "End STT session"
 				// message from the Voice Gateway. The logic below takes care of
 				// handling two End of Call Signals for every voice call.
 				onTuple EOCS: {
 					// Get the allocated STT engine id for a given 
 					// vgwSessionId_vgwVoiceChannelNumber combo.
 					// We should always have an STT engine id. If not, that is a 
 					// case where the user didn't provision sufficient number of 
 					// STT engines and there was no idle STT engine available for that 
 					// given vgwSessionId_vgwVoiceChannelNumber combo. 
 					// This situation can be avoided by starting the application with a 
 					// sufficient number of STT engines needed for the anticipated 
 					// maximum concurrent voice calls. A rule of thumb is to have 
 					// two STT engines to process two speakers in every given 
 					// concurrent voice call.
 					//
 					// Get the sessionId + channelNumber combo string.
 					_key = EOCS.vgwSessionId + "_" + (rstring)EOCS.vgwVoiceChannelNumber;
 					
 					if (has(_vgwSessionIdToUdpChannelMap, _key) == true) {
 						// Let us send an empty blob to the WatsonSTT operator to indicate that
 						// this speaker of a given voice call is done.
 						_oTuple = (BinarySpeech_t){};
 						// Copy the three input tuple attributes that must
 						// match with that of the outgoing tuple.
 						assignFrom(_oTuple, EOCS);
 						// Assign the STT engine id where this voice channel was
 						// getting processed until now.
 						_oTuple.sttEngineId = _vgwSessionIdToUdpChannelMap[_key];
 						submit(_oTuple, BSDF);
 						// We are now done with this vgwSessionId_vgwVoiceChannelNumber combo.
 						// So, we can release the STT engine and add it to the idle UDP channels list.
 						removeM(_vgwSessionIdToUdpChannelMap, _key);
 						appendM(_idleUdpChannelsList, _oTuple.sttEngineId);
 					}
 					
 					// Since this voice call is ending, let us release the STT result processor 
 					// instance that was allocated above for this voice call.
 					if (has(_vgwSessionToResultProcessorChannelMap, 
 						EOCS.vgwSessionId) == true) {
 						removeM(_vgwSessionToResultProcessorChannelMap, EOCS.vgwSessionId);
 					}
 				}
 				
 			config
 				threadedPort: queue(BSD, Sys.Wait), queue(EOCS, Sys.Wait);
 		} // End of Custom operator.
 
 		// IMPORTANT: IBM STT service on public cloud requires
 		// an unexpired valid IAM access token to perform the 
 		// speech to text task in a secure manner. You may either 
 		// provide the token in parameter sttOnCP4DAccessToken, or provide 
 		// the parameter sttIAMTokenURL and sttApiKey and the operator IAMAccessTokenGenerator
 		// generates a new access token and then periodically refresh it. 
 		// Output stream of this composite operator is connected to the
 		// second input stream of the WatsonSTT operator that is used below.
 		// For a correct STT operation, user must set only one of these two
 		// submission time parameters to a non-empty value: sttAPIKey or sttOnCP4DAccessToken.
 		(stream&lt;IAMAccessToken&gt; IamAccessToken as IAT)
 			as IamAccessTokenGenerator = IAMAccessTokenGenerator() {
 			param
 				// This operator takes these four parameters.
 				apiKey: $sttApiKey;
 				iamTokenURL: $sttIAMTokenURL;
 				accessToken: $sttOnCP4DAccessToken;
 				appConfigName: ""; //This setting avoids error logs when the app config is missing
 		}
 		
 
 		// Invoke one or more instances of the IBMWatsonSpeechToText composite operator.
 		// You can send the audio data to this operator all at once or 
 		// you can send the audio data for the live-use case as it becomes
 		// available from your telephony network switch.
 		// Avoid feeding audio data coming from more than one data source into this 
 		// parallel region which may cause erroneous transcription results.
 		//
 		// NOTE: The WatsonSTT operator allows fusing multiple instances of
 		// this operator into a single PE. This will help in reducing the 
 		// total number of CPU cores used in running the application.
 		// First input stream into this operator is the audio blob content.
 		// Second input stream into this operator is your STT service instance's IAM access token.
 		// Please note that we are using an int32 value (exactly in the
 		// range of the available number of STT engines) as our
 		// UDP partition key. That will help us to always use that
 		// preselected partition for a given voice call thereby 
 		// avoiding cross talk and mix up of data from multiple 
 		// voice calls landing in a given parallel channel etc.
 		@parallel(width = $numberOfSTTEngines, 
 		partitionBy=[{port=BSDF, attributes=[sttEngineId]}], broadcast=[AT])
 		(stream&lt;STTResult_t&gt; MySTTResult) as SpeechToText = 
 			IBMWatsonSpeechToText(BinarySpeechDataFragment as BSDF;
 			IamAccessToken as AT) {
 			// If needed, you can decide not to fuse the WatsonSTT operator instances and
 			// keep each instance of this operator on its own PE (a.k.a Linux process) by
 			// removing the block comment around this config clause.
 			/*
 			config
 				placement : partitionExlocation("sttpartition");
 			*/
 		}
 
 		// Let us invoke the same number of STT result processors as 
 		// there are STT engines.
 		// Please note that we are using an int32 value (exactly in the
 		// range of the available number of STT engines) as our
 		// UDP partition key. That will help us to always use that
 		// preselected partition for a given voice call thereby 
 		// avoiding cross talk and mix up of data from multiple 
 		// voice calls landing in a given parallel channel etc.
 		@parallel(width = $numberOfSTTEngines, 
 		partitionBy=[{port=MSR, attributes=[sttResultProcessorId]}])
 		() as STTResultProcessorSink = STTResultProcessor(MySTTResult as MSR) {
 		}
 		
 		// =================================================================
 		// This code block is here purely for debugging purposes to 
 		// capture the raw audio data received in a given voice channel 
 		// for a given VGW session id. You have to remove the block comment
 		// below to activate this section of the code when you need it.
 		// The following block of code is good only for testing it with
 		// a single voice call. If you use the following section of code
 		// for multiple voice calls, then the speech data bytes from
 		// multiple calls will get mixed up. So, use it to debug the
 		// speech data contents for just a single voice call.
 		// In a real-life application, you will not have a need for
 		// this section of the code.
 		/*
 		(stream&lt;BinarySpeech_t&gt; BinarySpeechDataFragmentInVoiceChannel1;
 		 stream&lt;BinarySpeech_t&gt; BinarySpeechDataFragmentInVoiceChannel2)
 			as BinarySpeechDataFilterByVoiceChannel = 
 			Split(BinarySpeechDataFragment as BSDF) {
 			param
 				// We will only have either channel number 1 or 2.
 				// So, send the speech data received via channel number 1 to 
 				// output port index 0 (i.e. first port).
 				// Send the speech data received via channel number 2 to 
 				// output port index 1 (i.e. second port).
 				index: (BSDF.vgwVoiceChannelNumber == 1) ? 0 : 1;
 		}
 		
 		// Send only the blob part of the incoming tuple from voice channel 1.
 		(stream&lt;blob speech&gt; SpeechDataInVoiceChannel1 as SD1)
 			as SpeechDataFromVoiceChannel1 = 
 			Functor(BinarySpeechDataFragmentInVoiceChannel1) {	
 		}
 
 		// Send only the blob part of the incoming tuple from voice channel 2.
 		(stream&lt;blob speech&gt; SpeechDataInVoiceChannel2 as SD2)
 			as SpeechDataFromVoiceChannel2 = 
 			Functor(BinarySpeechDataFragmentInVoiceChannel2) {	
 		}
 		
 		// Write the speech data bytes received on voice channel 1 to its own binary file.
 		() as VoiceChannelSink1 = FileSink(SpeechDataInVoiceChannel1 as SD) {
 			param
 				// You can use this command to convert this 
 				// mulaw formatted audio file into a WAV file in order to
 				// play it using Audacity, QuickTime Player etc.:
 				file: "voice-channel1-speech-data.bin";
 				format: block;
 				flush: 1u;
 
 			config
 				threadedPort: queue(SD, Sys.Wait);
 		}
 
 		// Write the speech data bytes received on voice channel 2 to its own binary file.
 		() as VoiceChannelSink2 = FileSink(SpeechDataInVoiceChannel2 as SD) {
 			param
 				// You can use this command to convert this 
 				// mulaw formatted audio file into a WAV file in order to
 				// play it using Audacity, QuickTime Player etc.:
 				// ffmpeg -f mulaw -ar 8000 -i &lt;raw data&gt; -codec:a pcm_mulaw &lt;wav-filename&gt;
 				file: "voice-channel2-speech-data.bin";
 				format: block;
 				flush: 1u;
 
 			config
 				threadedPort: queue(SD, Sys.Wait);
 		}
 		*/
 		// =================================================================
 
 	config restartable: false;
 } // End of the main composite.

   </pre>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Types</h2>
  
</div>

<div class="section" id="spldoc_compilationunit__type__STTResult_t"><h2 class="title sectiontitle splpart">STTResult_t</h2>
  
</div>
<div class="section">

<p class="p">This STT result type contains many attributes to  demonstrate all the basic and very advanced features of  the Watson STT service. Not all real-life applications will need  all these attributes. You can decide to include or omit these  attributes based on the specific STT features your application will need.  Trimming the unused attributes will also help in  reducing the STT processing overhead and in turn  help in receiving the STT results faster.  Read the streamsx.sttgateway toolkit documentation to learn about  what features are available, how they work and how different attributes are  related to those features. 
</p>

</div>

<div class="section">
   <p class="p">
<tt class="ph tt">STTResult_t = rstring vgwSessionId, boolean isCustomerSpeechData, int32 vgwVoiceChannelNumber, rstring callerPhoneNumber, rstring agentPhoneNumber, int32 speechDataFragmentCnt, int32 totalSpeechDataBytesReceived, int32 sttEngineId, int32 sttResultProcessorId, int32 utteranceNumber, rstring utteranceText, boolean finalizedUtterance, float64 confidence, rstring sttErrorMessage, boolean transcriptionCompleted, list&lt;rstring&gt; utteranceAlternatives, list&lt;list&lt;rstring&gt;&gt; wordAlternatives, list&lt;list&lt;float64&gt;&gt; wordAlternativesConfidences, list&lt;float64&gt; wordAlternativesStartTimes, list&lt;float64&gt; wordAlternativesEndTimes, list&lt;rstring&gt; utteranceWords, list&lt;float64&gt; utteranceWordsConfidences, list&lt;float64&gt; utteranceWordsStartTimes, list&lt;float64&gt; utteranceWordsEndTimes, float64 utteranceStartTime, float64 utteranceEndTime, list&lt;int32&gt; utteranceWordsSpeakers, list&lt;float64&gt; utteranceWordsSpeakersConfidences, map&lt;rstring, list&lt;map&lt;rstring, float64&gt;&gt;&gt; keywordsSpottingResults;</tt>
   </p>

</div>

</div>


</body>
</html>