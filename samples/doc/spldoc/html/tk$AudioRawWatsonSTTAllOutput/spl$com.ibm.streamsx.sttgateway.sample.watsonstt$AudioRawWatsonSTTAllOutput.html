<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en-us" lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="reference"/>
<meta name="DC.Title" content="SPL File AudioRawWatsonSTTAllOutput.spl"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="spldoc_compilationunit"/>
<link rel="stylesheet" type="text/css" href="../../html/commonltr.css"/>
<link rel="stylesheet" type="text/css" href="../../html/spldoc.css"/>
<title>SPL File AudioRawWatsonSTTAllOutput.spl</title>
</head>
<body id="spldoc_compilationunit">


<h1 class="title topictitle1">SPL File <tt class="ph tt">AudioRawWatsonSTTAllOutput.spl</tt></h1>

<div class="body refbody">
<div class="section">
<p class="p">
<a class="xref" href="../toolkits/toolkits.html">Gateway to the IBM Speech To Text (STT) cloud service samples</a> &gt; <a class="xref" href="tk$AudioRawWatsonSTTAllOutput.html">AudioRawWatsonSTTAllOutput 1.0.6</a> &gt; <a class="xref" href="ns$com.ibm.streamsx.sttgateway.sample.watsonstt.html">com.ibm.streamsx.sttgateway.sample.watsonstt</a> &gt; AudioRawWatsonSTTAllOutput.spl</p>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Content</h2>
  
  <dl class="dl">
    <dt class="dt dlterm"/>
<dd class="dd"/>

    
      <dt class="dt dlterm splhead-2">Operators</dt>

      <dd class="dd">
<ul class="sl simple">
<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsonstt$AudioRawWatsonSTTAllOutput.html#spldoc_compilationunit__composite_operator__AudioRawWatsonSTTAllOutput">AudioRawWatsonSTTAllOutput</a></strong>: This example demonstrates transcription of raw audio data to text using the WatsonSTT operator.
</li>

</ul>

      </dd>

    
  </dl>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Composites</h2>
  
</div>

<div class="section" id="spldoc_compilationunit__composite_operator__AudioRawWatsonSTTAllOutput"><h2 class="title sectiontitle splpart">composite AudioRawWatsonSTTAllOutput</h2>
  
</div>

<div class="section splgraph">
  <embed class="image" src="../../image/tk$AudioRawWatsonSTTAllOutput/op$com.ibm.streamsx.sttgateway.sample.watsonstt$AudioRawWatsonSTTAllOutput.svg" width="442" height="154"/>
</div>

<div class="section">

<p class="p">This example demonstrates transcription of raw audio data to text using the WatsonSTT operator.  This example code is suitable for processing the raw audio binary data that is either  received directly from the voice network switches via TCP/UDP (OR) read from the WAV files  on your own as audio blob data (in full or in partial blob fragments). Such raw binary audio data is then sent to the WatsonSTT operator for transcription.
</p>

<p class="p">This example shows how to send the raw binary audio data to the WatsonSTT operator to  get the transcription results. If you are looking to pick up the audio files  available in a given directory and simply send the name of the file to the  WatsonSTT operator to get the transcription results, then please refer to a  different example project named AudioFileWatonSTT.
</p>

<p class="p">Raw audio can be in uncompressed PCM, 16-bit little endian,  8 kHz (narrowband) or 16 KHz (broadband) sampling rate, mono format. 
</p>

<div class="p">To either downsample or upsample a WAV file, you can use the following ffmpeg command: 
<pre class="pre codeblock">
ffmpeg -i MyFile.wav -ac 1 -ar 8000 MyNewFile.wav
</pre>


</div>

<p class="p">You can build this example from command line via the make command by using the Makefile available in the top-level directory of this example. It will be necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by pointing it to the full path of your  streamsx.sttgateway/com.ibm.streamsx.sttgateway directory.
</p>

<p class="p">If you want to build this example inside the Streams Studio, there are certain build configuration settings needed. Please refer to the streamsx.sttgateway toolkit documentation to learn more about those Streams Studio configuration settings.
</p>

<p class="p">IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the  Watson STT cloud service. For the STT service on IBM Public Cloud,  one must use the unexpired IAM access token (generated by using your  IBM Public cloud STT service instance's API key).  So, user must provide here his/her API key. We have some logic below that  will use the user provided API key to generate the IAM access token and  send that to the WatsonSTT operator. There is additional logic available below to keep refreshing that IAM access token periodically in order for it to stay unexpired. You should leave this submission time value empty when not using STT on IBM public cloud. https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
</p>

</div>

<div class="section"><h2 class="title sectiontitle">Parameters</h2>

<ul class="sl simple">
<li class="sli"><strong class="ph b">sttApiKey</strong>: Specify either the public cloud IAM Token fetch/refresh URL. 
</li>

<li class="sli"><strong class="ph b">sttIAMTokenURL</strong>: Specify either the public cloud IAM Token fetch/refresh URL. Default https://iam.cloud.ibm.com/identity/token 
</li>

<li class="sli"><strong class="ph b">sttOnCP4DAccessToken</strong>: Specify the IBM STT on Cloud Pak for Data (CP4D i.e. private cloud) access token.  You should leave this submission time value empty when not using STT on CP4D. 
</li>

<li class="sli"><strong class="ph b">audioDir</strong>: directory with the audio files 
</li>

<li class="sli"><strong class="ph b">nonFinalUtterancesNeeded</strong>: nonFinalUtterancesNeeded; Default false 
</li>

<li class="sli"><strong class="ph b">audioBlobFragmentSize</strong>: specify any audio blob fragment size (in number of bytes) to read from a given      audio file. If you give a size of 0, then the entire file content will be read in one single blob and 
     sent to the WatsonSTT operator for transcription.

</li>

<li class="sli"><strong class="ph b">numberOfSTTEngines</strong>: the number of stt engines
</li>

<li class="sli"><strong class="ph b">sttUri</strong>: the stt service uri; Default wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize
</li>

<li class="sli"><strong class="ph b">sttBaseLanguageModel</strong>: base language model; Default: en-US_NarrowbandModel
</li>

<li class="sli"><strong class="ph b">contentType</strong>: content type; Default audio/wav
</li>

<li class="sli"><strong class="ph b">baseModelVersion</strong>: base model version
</li>

<li class="sli"><strong class="ph b">customizationId</strong>: "customizationId"
</li>

<li class="sli"><strong class="ph b">acousticCustomizationId</strong>: acousticCustomizationId
</li>

<li class="sli"><strong class="ph b">customizationWeight</strong>: customizationWeight; Default 0.30
</li>

<li class="sli"><strong class="ph b">sttBatchSize</strong>: stt batch size
</li>

<li class="sli"><strong class="ph b">maxUtteranceAlternatives</strong>: maxUtteranceAlternatives; Default 1
</li>

<li class="sli"><strong class="ph b">sttRequestLogging</strong>: sttRequestLogging; Default false
</li>

<li class="sli"><strong class="ph b">filterProfanity</strong>: filterProfanity; Default false
</li>

<li class="sli"><strong class="ph b">sttJsonResponseDebugging</strong>: sttJsonResponseDebugging; Default false
</li>

<li class="sli"><strong class="ph b">wordAlternativesThreshold</strong>: "wordAlternativesThreshold; Default 0.0
</li>

<li class="sli"><strong class="ph b">smartFormattingNeeded</strong>: smartFormattingNeeded; Default false
</li>

<li class="sli"><strong class="ph b">keywordsSpottingThreshold</strong>: "keywordsSpottingThreshold; Default 0.0
</li>

<li class="sli"><strong class="ph b">keywordsToBeSpotted</strong>: keywordsToBeSpotted
</li>

<li class="sli"><strong class="ph b">websocketLoggingNeeded</strong>: websocketLoggingNeeded; Default false
</li>

<li class="sli"><strong class="ph b">cpuYieldTimeInAudioSenderThread</strong>: cpuYieldTimeInAudioSenderThread; Default 0.001
</li>

<li class="sli"><strong class="ph b">sttLiveMetricsUpdateNeeded</strong>: sttLiveMetricsUpdateNeeded; Default true
</li>

<li class="sli"><strong class="ph b">fileEndEmitWindowMarker</strong>: Emit a window punctuation at file end if true. Send an empty blob at file end      if false.

</li>

</ul>

</div>

<div class="section">
</div>

<div class="section">
</div>

<div class="section"><h2 class="title sectiontitle splhead-2">SPL Source Code</h2>
  
</div>


<div class="section">
   <pre class="pre codeblock">

 public composite AudioRawWatsonSTTAllOutput {
 	param
 		// IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the 
 		// Watson STT cloud service. For the STT service on IBM Public Cloud, 
 		// one must use the unexpired IAM access token (generated by using your 
 		// IBM Public cloud STT service instance's API key). 
 		// So, user must provide here his/her API key. We have some logic below that 
 		// will use the user provided API key to generate the IAM access token and 
 		// send that to the WatsonSTT operator.
 		// There is additional logic available below to keep refreshing that
 		// IAM access token periodically in order for it to stay unexpired.
 		// You should leave this submission time value empty when not using STT on IBM public cloud.
 		// https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
 		expression&lt;rstring&gt; $sttApiKey : getSubmissionTimeValue("sttApiKey", "");
 		// Specify either the public cloud IAM Token fetch/refresh URL.
 		expression&lt;rstring&gt; $sttIAMTokenURL : 
 			getSubmissionTimeValue("sttIAMTokenURL", "https://iam.cloud.ibm.com/identity/token");
 		// Specify the IBM STT on Cloud Pak for Data (CP4D i.e. private cloud) access token.
 		// You should leave this submission time value empty when not using STT on CP4D.
 		expression&lt;rstring&gt; $sttOnCP4DAccessToken : getSubmissionTimeValue("sttOnCP4DAccessToken", "");
 		expression&lt;rstring&gt; $audioDir : getSubmissionTimeValue("audioDir", "../../audio-files");
 		expression&lt;boolean&gt; $nonFinalUtterancesNeeded: (boolean)getSubmissionTimeValue("nonFinalUtterancesNeeded", "false");
 		// You can specify any audio blob fragment size (in number of bytes) to read 
 		// from a given audio file. If you give a size of 0, then the entire 
 		// file content will be read in one single blob and sent to the 
 		// WatsonSTT operator for transcription. In general, smaller the
 		// blob fragment size, longer the time it takes to transcribe a given
 		// audio conversation because of too many tuples and packets getting
 		// exchanged with the WatsonSTT operator as well as between the 
 		// WatsonSTT operator and the Watson STT cloud service. So, keeping the 
 		// blob fragment size as much larger as possible as permitted by the 
 		// usecase will result in shorter end-to-end transcription time for a 
 		// given audio conversation.
 		expression&lt;int64&gt; $audioBlobFragmentSize : (int64)
 			getSubmissionTimeValue("audioBlobFragmentSize", "512");
 		// NOTE: This particular application always crashes when multiple 
 		// STT engines (i.e. &gt; 1) are configured to run in distributed mode. 
 		// It works fine sometimes with multiple STT engines in the 
 		// standalone mode where everything is fused. The problem seems to 
 		// appear in the Custom operator code below that reads blob fragments 
 		// from the audio files and feeds it to the STT engines.  
 		// We have to fix it at a later time by testing it both in the
 		// standalone and distributed modes. 
 		// Hence, we are setting the default STT engine count to 1 for now 
 		// so that it can work without crashing in standalone and distributed modes.
 		expression&lt;int32&gt; $numberOfSTTEngines :(int32)
 			getSubmissionTimeValue("numberOfSTTEngines", "1") ;
 		expression&lt;rstring&gt; $sttUri : getSubmissionTimeValue("sttUri",
 			"wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize");
 		expression&lt;rstring&gt; $sttBaseLanguageModel : 
 			getSubmissionTimeValue("sttBaseLanguageModel", "en-US_NarrowbandModel");
 		expression&lt;rstring&gt; $contentType : 
 			getSubmissionTimeValue("contentType", "audio/wav");
 		expression&lt;rstring&gt; $baseModelVersion : 
 			getSubmissionTimeValue("baseModelVersion", "");
 		expression&lt;rstring&gt; $customizationId : 
 			getSubmissionTimeValue("customizationId", "");
 		expression&lt;rstring&gt; $acousticCustomizationId : 
 			getSubmissionTimeValue("acousticCustomizationId", "");
 		expression&lt;float64&gt; $customizationWeight : 
 			(float64)getSubmissionTimeValue("customizationWeight", "0.30");
 		expression&lt;int32&gt; $sttBatchSize : (int32)getSubmissionTimeValue("sttBatchSize", "0");
 		expression&lt;int32&gt; $maxUtteranceAlternatives : 
 			(int32)getSubmissionTimeValue("maxUtteranceAlternatives", "1");	
 		expression&lt;boolean&gt; $sttRequestLogging : 
 			(boolean)getSubmissionTimeValue("sttRequestLogging", "false");
 		expression&lt;boolean&gt; $filterProfanity : 
 			(boolean)getSubmissionTimeValue("filterProfanity", "false");
 		expression&lt;boolean&gt; $sttJsonResponseDebugging : 
 			(boolean)getSubmissionTimeValue("sttJsonResponseDebugging", "false");
 		expression&lt;float64&gt; $wordAlternativesThreshold : 
 			(float64)getSubmissionTimeValue("wordAlternativesThreshold", "0.0");
 		expression&lt;boolean&gt; $smartFormattingNeeded : 
 			(boolean)getSubmissionTimeValue("smartFormattingNeeded", "false");
 		expression&lt;float64&gt; $keywordsSpottingThreshold : 
 			(float64)getSubmissionTimeValue("keywordsSpottingThreshold", "0.0");
 		expression&lt;list&lt;rstring&gt;&gt; $keywordsToBeSpotted : 
 			(list&lt;rstring&gt;)getSubmissionTimeValue("keywordsToBeSpotted", "[]");	
 		expression&lt;boolean&gt; $websocketLoggingNeeded : 
 			(boolean)getSubmissionTimeValue("websocketLoggingNeeded", "false");
 		expression&lt;float64&gt; $cpuYieldTimeInAudioSenderThread : 
 			(float64)getSubmissionTimeValue("cpuYieldTimeInAudioSenderThread", "0.001");
 		expression&lt;boolean&gt; $sttLiveMetricsUpdateNeeded : 
 			(boolean)getSubmissionTimeValue("sttLiveMetricsUpdateNeeded", "true");
 		expression&lt;boolean&gt; $fileEndEmitWindowMarker :
 			(boolean)getSubmissionTimeValue("fileEndEmitWindowMarker", "true");
 
 	type
 		// This STT result type contains many attributes to
 		// demonstrate all the basic and very advanced features of 
 		// the Watson STT service. Not all real life applications will need 
 		// all these attributes. You can decide to include or omit these
 		// attributes based on the specific STT features your application will need. 
 		// Trimming the unused attributes will also help in 
 		// reducing the STT processing overhead and in turn 
 		// help in receiving the STT results faster.
 		// Read the streamsx.sttgateway toolkit documentation to learn about
 		// what features are available, how they work and how different attributes are 
 		// related to those features.
 		STTResult_t = rstring conversationId, int32 utteranceNumber,
 			boolean finalizedUtterance,
 			boolean transcriptionCompleted,
 			rstring sttErrorMessage,
 			float64 confidence,
 			rstring utteranceText,
 			list&lt;rstring&gt; utteranceAlternatives, 
 			list&lt;list&lt;rstring&gt;&gt; wordAlternatives,
 			list&lt;list&lt;float64&gt;&gt; wordAlternativesConfidences,
 			list&lt;float64&gt; wordAlternativesStartTimes,
 			list&lt;float64&gt; wordAlternativesEndTimes,
 			list&lt;rstring&gt; utteranceWords,
 			list&lt;float64&gt; utteranceWordsConfidences,
 			list&lt;float64&gt; utteranceWordsStartTimes,
 			list&lt;float64&gt; utteranceWordsEndTimes,
 			float64 utteranceStartTime,
 			float64 utteranceEndTime,
 			list&lt;int32&gt; utteranceWordsSpeakers,
 			list&lt;float64&gt; utteranceWordsSpeakersConfidences,
 			map&lt;rstring, list&lt;map&lt;rstring, float64&gt;&gt;&gt; keywordsSpottingResults,
 			int32 sequence;
 
 	graph
 		// IMPORTANT: IBM STT service on public cloud requires
 		// an unexpired valid IAM access token to perform the 
 		// speech to text task in a secure manner. You may either 
 		// provide the token in parameter sttOnCP4DAccessToken, or provide 
 		// the parameter sttIAMTokenURL and sttApiKey and the operator IAMAccessTokenGenerator
 		// generates a new access token and then periodically refresh it. 
 		// Output stream of this composite operator is connected to the
 		// second input stream of the WatsonSTT operator that is used below.
 		// For a correct STT operation, user must set only one of these two
 		// submission time parameters to a non-empty value: sttAPIKey or sttOnCP4DAccessToken.
 		(stream&lt;IAMAccessToken&gt; IamAccessToken as IAT)
 			as IamAccessTokenGenerator = IAMAccessTokenGenerator() {
 			param
 				// This operator takes these four parameters.
 				apiKey: $sttApiKey;
 				iamTokenURL: $sttIAMTokenURL;
 				accessToken: $sttOnCP4DAccessToken;
 				// All connection parameter are taken from params.
 				// So if we clean the app config name, we avoid error logs
 				appConfigName: "";
 		}
 
 		// Scan a directory periodically to pick up the audio files and 
 		// send the name of every audio file to the upstream Custom operator.
 		// That operator in turn will read the audio blob contents from that
 		// file and send the blob data either in full or in partial fragments
 		// to the WatsonSTT operator.
 		// Similar idea can be followed to receive the audio blob data from 
 		// the voice network switches (via TCP/UDP) instead of reading from the audio files.
 		//
 		// The output stream from this operator will be connected to the
 		// upstream Custom operator's input stream. That operator will
 		// read the blob data and send it to the WatsonSTT operator.
 		// That operator expects one of its input stream attributes to be 
 		// named as 'speech' with an SPL type of either rstring or blob.
 		// That operator can take as input an  audio filename (rstring) or 
 		// raw binary audio data (blob). In addition to that mandatory 
 		// input stream attribute, additional attributes can be sent 
 		// in the input stream to be auto assigned to the output stream of 
 		// the WatsonSTT operator.
 		stream&lt;rstring fileName&gt; AudioFileName = DirectoryScan() {
 			param
 				directory : $audioDir;
 				pattern : "\\.wav$";
 				// Give sufficient delay here so that the
 				// previous operator can complete generating the
 				// IAM access token and send it to the WatsonSTT operator.
 				sortBy: name;
 				
 			config
 				placement: partitionIsolation;
 		}
 
 		// In this Custom operator we will read the audio blob content and 
 		// send it to the WatsonSTT operator. Read the entire logic in this
 		// Custom operator thoroughly to understand what must be done to send
 		// the audio content as raw binary data to the WatsonSTT operator.
 		// There is an important step that must be performed after sending the
 		// entire blob content either in full or in partial blob fragments to the
 		// STT operator. Please read the commentary inside this operator completely.
 		stream&lt;rstring conversationId, blob speech, int32 sequence&gt; AudioBlobContent as ABC = 
 			Custom(AudioFileName as AFN) {
 			logic
 				// The counter for the sequece id of the results. Sequence should start with 0
 				state: mutable int32 counter = -1;
 				onTuple AFN: {
 					mutable int64 audioBlobFragmentSize =  $audioBlobFragmentSize;
 					
 					// Audio blob fragment size can be any value &gt;= 0.
 					// If it is given as 0, then this operator will read the
 					// entire audio file content into a single blob and send it. 
 					if (audioBlobFragmentSize &lt; 0l) {
 						printStringLn("Invalid blob fragment size " + 
 							(rstring)audioBlobFragmentSize + " given. It must be &gt;= 0.");
 						abort();
 					}
 					
 					mutable uint64 fh = 0ul;
 					mutable int32 err = 0;
 
 					// Read the given binary audio file.
 					fh = fopen(fileName, "rb", err);
 
 					if (err != 0) {
 						appTrc(Trace.error, "Unable to open the audio file " + fileName + 
 							". Error code=" + (rstring)err, "AUDIO_BLOB_READ_ERROR");
 						return;
 					}
 
 					// Get the file size by seeking to the end of the audio file.
 					fseek(fh, 0l, optSEEK_END(), err);
 
 					if (err != 0) {
 						appTrc(Trace.error, "Unable to seek to the end of the audio file " + 
 							fileName + ". Error code=" + (rstring)err, "AUDIO_BLOB_READ_ERROR");
 						fclose(fh, err);
 						return;
 					}
 
 					// Get the current position at the very end of the audio file.
 					int64 fileSize = ftell(fh, err);
 
 					if (err != 0) {
 						appTrc(Trace.error, "Unable to get the size of the audio file " + 
 							fileName + ". Error code=" + (rstring)err, "AUDIO_BLOB_READ_ERROR");
 						fclose(fh, err);
 						return;
 					}
 
 					// Rewind to the top of the audio file.
 					fseek(fh, 0l, optSEEK_SET(), err);
 
 					if (err != 0) {
 						appTrc(Trace.error, "Unable to seek to the top of the audio file " + 
 							fileName + ". Error code=" + (rstring)err, "AUDIO_BLOB_READ_ERROR");
 						fclose(fh, err);
 						return;
 					}
 
 					// prepare output tuple
 					mutable boolean atleastOneBlobFragmentWasSent = false;
 					mutable AudioBlobContent oTuple = {};
 					oTuple.conversationId = AFN.fileName;
 
 					// write at least one empty blob for an empty file
 					if (fileSize == 0l) {
 						appTrc(Trace.error, "Ignore empty file: " + fileName, "AUDIO_BLOB_READ_ERROR");
 					} else {
 
 						if (audioBlobFragmentSize == 0l) {
 							// User has configured to send the entire audio file content in a single blob.
 							audioBlobFragmentSize = fileSize;
 						}
 						// audioBlobFragmentSize must be != 0
 						int32 numberOfBlobFragments = fileSize / audioBlobFragmentSize;
 						int32 numberOfBytesRemaining = fileSize % audioBlobFragmentSize;
 						mutable int32 loopCnt = 0;
 						mutable list&lt;uint8&gt; audioBlob = [];
 						mutable boolean audioBlobReadError = false;
 						
 						// Stay in a loop to read all the blob fragments and send.
 						while(++loopCnt &lt;= numberOfBlobFragments) {
 							++counter;
 							// Read an audio blob fragment from the audio file.
 							clearM(audioBlob);
 							fread(audioBlob, fh, (uint64)audioBlobFragmentSize, err);
 	
 							if (err != 0) {
 								appTrc(Trace.error, "Unable to read the binary contents of the audio file " +
 									fileName + ". Error code=" + (rstring)err, "AUDIO_BLOB_READ_ERROR");
 								audioBlobReadError = true;
 								break;
 							}
 							
 							// Send this blob data now.
 							oTuple.speech = (blob)audioBlob;
 							oTuple.sequence = counter;
 							submit(oTuple, ABC);
 							atleastOneBlobFragmentWasSent = true;
 						} // End of while(++loopCnt &lt;= numberOfBlobFragments)
 						
 						// See if there are still any remaining bytes that didn't manage to 
 						// fit in a full blob in the previous while loop. 
 						// (i.e. less than the configured audo blob fragment size).
 						while (numberOfBytesRemaining &gt; 0 &amp;&amp; audioBlobReadError == false) {
 							++counter;
 							// Read the remaining bytes.
 							clearM(audioBlob);
 							fread(audioBlob, fh, (uint64)numberOfBytesRemaining, err);
 	
 							if (err != 0) {
 								appTrc(Trace.error, "Unable to read the binary contents of the audio file " +
 									fileName + ". Error code=" + (rstring)err, "AUDIO_BLOB_READ_ERROR");
 								break;
 							}
 	
 							// Send this blob data now.
 							oTuple.speech = (blob)audioBlob;
 							oTuple.sequence = counter;
 							submit(oTuple, ABC);
 							atleastOneBlobFragmentWasSent = true;
 							// This must be the very last statement in this while loop.
 							// We must only do a single iteration of this while loop.
 							break;
 						}
 	
 						fclose(fh, err);
 					}
 					
 					// *** IMPORTANT LOGIC *** IMPORTANT LOGIC *** IMPORTANT LOGIC ***
 					//	
 					// If we have sent at least one blob fragment to the
 					// WatsonSTT operator for transcription, then we must send window marker to 
 					// indicate the end of the audio conversation. If we don't do this, 
 					// the WatsonSTT operator will not function correctly. 
 					//
 					if (atleastOneBlobFragmentWasSent == true) {
 						if ($fileEndEmitWindowMarker) {
 							submit(Sys.WindowMarker, ABC);
 						} else {
 							//this code 
 							//clearM(oTuple.speech);
 							//causes a SIGSEV if this operator is not fused with its downstream operator
 							++counter;
 							blob newBlob = [];
 							oTuple.speech = newBlob;
 							oTuple.sequence = counter;
 							// Send an empty blob as the very last one for this audio conversation.
 							submit(Sys.WindowMarker, ABC);
 						}
 					}
 					//
 					// *** IMPORTANT LOGIC *** IMPORTANT LOGIC *** IMPORTANT LOGIC ***
 				} // End of onTuple AFN
 		} // End of the Custom operator.
 
 		// Invoke one or more instances of the WatsonSTT operator.
 		// You can send the audio data to this operator all at once or 
 		// you can send the audio data for the live-use case as it becomes
 		// available from your telephony network switch.
 		// Avoid feeding audio data coming from more than one data source into this 
 		// parallel region which may cause erroneous transcription results.
 		//
 		// NOTE: The WatsonSTT operator allows fusing multiple instances of
 		// this operator into a single PE. This will help in reducing the 
 		// total number of CPU cores used in running the application.
 		// First input stream into this operator is the audio blob content.
 		// Second input stream into this operator is your STT service instance's IAM access token.
 
 		@parallel(width = $numberOfSTTEngines, 
 		partitionBy=[{port=ABC, attributes=[conversationId]}], broadcast=[AT])
 		stream&lt;STTResult_t&gt; STTResult = 
 			WatsonSTT(AudioBlobContent as ABC; IamAccessToken as AT) {
 			logic
 				state: {
 					mutable int32 _conversationCnt = 0;
 					mutable rstring _conversationId = "";
 				}
 				
 				onTuple ABC: {
 					if (_conversationId != ABC.conversationId) {
 						// There may be many blob fragments arriving for a given audio conversation.
 						// So, display only when the very first blob fragment for a given audio arrives.
 						_conversationId = ABC.conversationId;
 						printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " Channel " + (rstring)getChannel() + 
 							", Speech input " + (rstring)++_conversationCnt +
 							": " + _conversationId);
 					}
 				}
 
 			// Just to demonstrate, we are using all the operator parameters below.
 			// Except for the first three parameters, every other parameter is an
 			// optional one. In real-life applications, such optional parameters
 			// can be omitted unless you want to change the default behavior of them.
 			param
 				sttResultMode: partial;
 				nonFinalUtterancesNeeded: $nonFinalUtterancesNeeded;
 				uri: $sttUri;
 				baseLanguageModel: $sttBaseLanguageModel;
 				contentType: $contentType;
 				sttRequestLogging: $sttRequestLogging;
 				filterProfanity: $filterProfanity;
 				sttJsonResponseDebugging: $sttJsonResponseDebugging;
 				maxUtteranceAlternatives: $maxUtteranceAlternatives;
 				wordAlternativesThreshold: $wordAlternativesThreshold;
 				smartFormattingNeeded: $smartFormattingNeeded;
 				keywordsSpottingThreshold: $keywordsSpottingThreshold;
 				keywordsToBeSpotted: $keywordsToBeSpotted;
 				websocketLoggingNeeded: $websocketLoggingNeeded;
 				cpuYieldTimeInAudioSenderThread: $cpuYieldTimeInAudioSenderThread;
 				sttLiveMetricsUpdateNeeded : $sttLiveMetricsUpdateNeeded;
 								
 				// Use the following operator parameters as needed.
 				// Point to a specific version of the base model if needed.
 				//
 				// e-g: "en-US_NarrowbandModel.v07-06082016.06202016"
 				baseModelVersion: $baseModelVersion;
 				// Language model customization id to be used for the transcription.
 				// e-g: "74f4807e-b5ff-4866-824e-6bba1a84fe96"
 				customizationId: $customizationId;
 				// Acoustic model customization id to be used for the transcription.
 				// e-g: "259c622d-82a4-8142-79ca-9cab3771ef31"
 				acousticCustomizationId: $acousticCustomizationId;
 				// Relative weight to be given to the words in the custom Language model.
 				customizationWeight: $customizationWeight;
 
 			// Just for demonstrative purposes, we are showing below the output attribute
 			// assignments using all the available custom output functions. In your
 			// real-life applications, it is sufficient to do the assignments via
 			// custom output functions only as needed.
 			//
 			// Some of the important output functions that must be used to check
 			// the result of the transcription are:
 			// getSTTErrorMessage --&gt; It tells whether the transcription succeeded or not.
 			// isFinalizedUtterance --&gt; In sttResultMode partial, it tells whether this is a 
 			//                          partial utterance or a finalized utterance.
 			// isTranscriptionCompleted --&gt; It tells whether the transcription is 
 			//                              completed for the current audio conversation or not.
 			//
 			output
 				STTResult: conversationId = conversationId, 
 					utteranceNumber = getUtteranceNumber(),
 					utteranceText = getUtteranceText(),
 					finalizedUtterance = isFinalizedUtterance(),
 					confidence = getConfidence(),
 					sttErrorMessage = getSTTErrorMessage(),
 					transcriptionCompleted = isTranscriptionCompleted(),
 					// n-best utterance alternative hypotheses.
 					utteranceAlternatives = getUtteranceAlternatives(),
 					// Confusion networks (a.k.a. Consensus)
 					wordAlternatives = getWordAlternatives(),
 					wordAlternativesConfidences = getWordAlternativesConfidences(),
 					wordAlternativesStartTimes = getWordAlternativesStartTimes(),
 					wordAlternativesEndTimes = getWordAlternativesEndTimes(),
 					utteranceWords = getUtteranceWords(),
 					utteranceWordsConfidences = getUtteranceWordsConfidences(),
 					utteranceWordsStartTimes = getUtteranceWordsStartTimes(),
 					utteranceWordsEndTimes = getUtteranceWordsEndTimes(),
 					utteranceStartTime = getUtteranceStartTime(),
 					utteranceEndTime = getUtteranceEndTime(),
 					// Speaker label a.k.a. Speaker id
 					utteranceWordsSpeakers = getUtteranceWordsSpeakers(),
 					utteranceWordsSpeakersConfidences = getUtteranceWordsSpeakersConfidences(),
 					// Results from keywords spotting (matching) in an utterance.
 					keywordsSpottingResults = getKeywordsSpottingResults();
 					
 			// If needed, you can decide not to fuse the WatsonSTT operator instances and
 			// keep each instance of this operator on its own PE (a.k.a Linux process) by
 			// removing the block comment around this config clause.
 			/*
 			config
 				placement : partitionExlocation("sttpartition");
 			*/
 			config
 				threadedPort: queue(ABC, Sys.Wait);
 		}
 		
 		// In a real-life application, there will be additional operators here with the 
 		// necessary logic to look inside the tuples arriving on the STTResult stream and
 		// analyze different kinds of speech to text result attributes returned from the STT service.
 		// 
 		// But, in this simple example we will only collect the results arriving from the 
 		// WatsonSTT operator and display them on stdout.
 		() as MySink1 = Custom(STTResult as SR) {
 			logic
 				state: {
 					mutable int32 _conversationCnt = 0;
 				}
 				
 				onTuple SR: {
 					// If the user gave us a non-zero batch size, we will print a single line
 					// about the batch completion at the very end. In that case, we will 
 					// avoid printing every individual transcription result. This will not
 					// make the file system very busy in the Distributed Mode execution.
 					// This is only useful if someone is interested in doing a 
 					// timing measurement to see how long it takes to transcribe a 
 					// known batch of audio conversations with the sttResultMode operator
 					// parameter to set to a value of complete (get the full text after the
 					// entire conversation is transcribed).
 					if ($sttBatchSize &gt; 0) {
 						++_conversationCnt;
 						
 						if (_conversationCnt == 1) {
 							printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " STT result for conversation " + (rstring) _conversationCnt
 								+ " " + conversationId + " arrived.");
 						} else if (_conversationCnt == $sttBatchSize) {
 							printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " STT result for conversation " + (rstring) _conversationCnt
 								+ " " + conversationId + " arrived.");
 						}
 					} else {
 						// User didn't give a non-zero batch size. In that case,
 						// we will print every transcription result, which will make the
 						// file system busy in the Distributed Mode execution.
 						printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " " + (rstring)++_conversationCnt + 
 							") STT result: " + (rstring)SR);
 					}
 				}
 				
 			config
 				//Place this operator into an own PE to separate the final result printouts from all other printouts
 				placement: partitionIsolation;
 				threadedPort: queue(SR, Sys.Wait);
 		}
 
 	config restartable: false;
 } // End of composite AudioRawWatsonSTT (Main composite)

   </pre>

</div>

</div>


</body>
</html>