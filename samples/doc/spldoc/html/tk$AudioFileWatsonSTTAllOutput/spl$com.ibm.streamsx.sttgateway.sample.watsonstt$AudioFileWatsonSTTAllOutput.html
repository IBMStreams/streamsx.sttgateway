<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en-us" lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="reference"/>
<meta name="DC.Title" content="SPL File AudioFileWatsonSTTAllOutput.spl"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="spldoc_compilationunit"/>
<link rel="stylesheet" type="text/css" href="../../html/commonltr.css"/>
<link rel="stylesheet" type="text/css" href="../../html/spldoc.css"/>
<title>SPL File AudioFileWatsonSTTAllOutput.spl</title>
</head>
<body id="spldoc_compilationunit">


<h1 class="title topictitle1">SPL File <tt class="ph tt">AudioFileWatsonSTTAllOutput.spl</tt></h1>

<div class="body refbody">
<div class="section">
<p class="p">
<a class="xref" href="../toolkits/toolkits.html">Gateway to the IBM Speech To Text (STT) cloud service samples</a> &gt; <a class="xref" href="tk$AudioFileWatsonSTTAllOutput.html">AudioFileWatsonSTTAllOutput 1.0.6</a> &gt; <a class="xref" href="ns$com.ibm.streamsx.sttgateway.sample.watsonstt.html">com.ibm.streamsx.sttgateway.sample.watsonstt</a> &gt; AudioFileWatsonSTTAllOutput.spl</p>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Content</h2>
  
  <dl class="dl">
    <dt class="dt dlterm"/>
<dd class="dd"/>

    
      <dt class="dt dlterm splhead-2">Operators</dt>

      <dd class="dd">
<ul class="sl simple">
<li class="sli"><strong class="ph b"><a class="xref" href="spl$com.ibm.streamsx.sttgateway.sample.watsonstt$AudioFileWatsonSTTAllOutput.html#spldoc_compilationunit__composite_operator__AudioFileWatsonSTTAllOutput">AudioFileWatsonSTTAllOutput</a></strong>: This example demonstrates transcription of audio files into text using the WatsonSTT operator.
</li>

</ul>

      </dd>

    
  </dl>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Composites</h2>
  
</div>

<div class="section" id="spldoc_compilationunit__composite_operator__AudioFileWatsonSTTAllOutput"><h2 class="title sectiontitle splpart">composite AudioFileWatsonSTTAllOutput</h2>
  
</div>

<div class="section splgraph">
  <embed class="image" src="../../image/tk$AudioFileWatsonSTTAllOutput/op$com.ibm.streamsx.sttgateway.sample.watsonstt$AudioFileWatsonSTTAllOutput.svg" width="442" height="154"/>
</div>

<div class="section">

<p class="p">This example demonstrates transcription of audio files into text using the WatsonSTT operator.  This example code is suitable for processing audio data that is already saved into  a set of audio files (WAV, MP3 etc.) and made available within a specific directory.
</p>

<p class="p">This example shows how to pick up the audio files available in a given directory and simply send the name of the file to the WatsonSTT operator to get the transcription results. If you are looking to receive the audio data directly from the voice network switches via TCP/UDP (OR) read the binary audio content from the WAV files on your own and send the audio blob data (either in full or in partial blob fragments) to the WatsonSTT operator, then please refer to a different example project named AudioRawWatsonSTT.
</p>

<p class="p">WAV files can be in uncompressed PCM, 16-bit little endian,  8 kHz (narrowband) or 16 KHz (broadband) sampling rate, mono format. 
</p>

<div class="p">To either downsample or upsample a WAV file, you can use the following ffmpeg command: 
<pre class="pre codeblock">
ffmpeg -i MyFile.wav -ac 1 -ar 8000 MyNewFile.wav
</pre>


</div>

<p class="p">You can build this example from command line via the make command by using the Makefile available in the top-level directory of this example. It will be necessary to export the STREAMS_STTGATEWAY_TOOLKIT environment variable by pointing it to the full path of your  streamsx.sttgateway/com.ibm.streamsx.sttgateway directory.
</p>

<p class="p">If you want to build this example inside the Streams Studio, there are certain build configuration settings needed. Please refer to the streamsx.sttgateway toolkit documentation to learn more about those Streams Studio configuration settings.
</p>

<p class="p">IMPORTANT: The WatsonSTT operator uses Websocket to communicate with the   Watson STT cloud service. For the STT service on IBM Public Cloud,   one must use the unexpired IAM access token (generated by using your   IBM Public cloud STT service instance's API key).   So, user must provide here his/her API key. We have some logic below that   will use the user provided API key to generate the IAM access token and   send that to the WatsonSTT operator.  There is additional logic available below to keep refreshing that  IAM access token periodically in order for it to stay unexpired.  You should leave this submission time value empty when not using STT on IBM public cloud.  https://cloud.ibm.com/docs/services/speech-to-text?topic=speech-to-text-websockets#WSopen
</p>

</div>

<div class="section"><h2 class="title sectiontitle">Parameters</h2>

<ul class="sl simple">
<li class="sli"><strong class="ph b">sttApiKey</strong>: Specify either the public cloud IAM Token fetch/refresh URL. 
</li>

<li class="sli"><strong class="ph b">sttIAMTokenURL</strong>: Specify either the public cloud IAM Token fetch/refresh URL. Default https://iam.cloud.ibm.com/identity/token 
</li>

<li class="sli"><strong class="ph b">sttOnCP4DAccessToken</strong>: Specify the IBM STT on Cloud Pak for Data (CP4D i.e. private cloud) access token.  You should leave this submission time value empty when not using STT on CP4D. 
</li>

<li class="sli"><strong class="ph b">audioDir</strong>: directory with the audio files
</li>

<li class="sli"><strong class="ph b">nonFinalUtterancesNeeded</strong>: nonFinalUtterancesNeeded; Default false
</li>

<li class="sli"><strong class="ph b">numberOfSTTEngines</strong>: the number of stt engines
</li>

<li class="sli"><strong class="ph b">sttUri</strong>: the stt service uri; Default wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize
</li>

<li class="sli"><strong class="ph b">sttBaseLanguageModel</strong>: base language model; Default: en-US_NarrowbandModel
</li>

<li class="sli"><strong class="ph b">contentType</strong>: content type; Default audio/wav
</li>

<li class="sli"><strong class="ph b">baseModelVersion</strong>: base model version
</li>

<li class="sli"><strong class="ph b">customizationId</strong>: "customizationId"
</li>

<li class="sli"><strong class="ph b">acousticCustomizationId</strong>: acousticCustomizationId
</li>

<li class="sli"><strong class="ph b">customizationWeight</strong>: customizationWeight; Default 0.30
</li>

<li class="sli"><strong class="ph b">sttBatchSize</strong>: stt batch size
</li>

<li class="sli"><strong class="ph b">maxUtteranceAlternatives</strong>: maxUtteranceAlternatives; Default 1
</li>

<li class="sli"><strong class="ph b">sttRequestLogging</strong>: sttRequestLogging; Default false
</li>

<li class="sli"><strong class="ph b">filterProfanity</strong>: filterProfanity; Default false
</li>

<li class="sli"><strong class="ph b">sttJsonResponseDebugging</strong>: sttJsonResponseDebugging; Default false
</li>

<li class="sli"><strong class="ph b">wordAlternativesThreshold</strong>: "wordAlternativesThreshold; Default 0.0
</li>

<li class="sli"><strong class="ph b">smartFormattingNeeded</strong>: smartFormattingNeeded; Default false
</li>

<li class="sli"><strong class="ph b">keywordsSpottingThreshold</strong>: "keywordsSpottingThreshold; Default 0.0
</li>

<li class="sli"><strong class="ph b">keywordsToBeSpotted</strong>: keywordsToBeSpotted
</li>

<li class="sli"><strong class="ph b">websocketLoggingNeeded</strong>: websocketLoggingNeeded; Default false
</li>

<li class="sli"><strong class="ph b">cpuYieldTimeInAudioSenderThread</strong>: cpuYieldTimeInAudioSenderThread; Default 0.001
</li>

<li class="sli"><strong class="ph b">sttLiveMetricsUpdateNeeded</strong>: sttLiveMetricsUpdateNeeded; Default true
</li>

</ul>

</div>

<div class="section">
</div>

<div class="section">
</div>

<div class="section"><h2 class="title sectiontitle splhead-2">SPL Source Code</h2>
  
</div>


<div class="section">
   <pre class="pre codeblock">

 public composite AudioFileWatsonSTTAllOutput {
 	param
 		expression&lt;rstring&gt; $sttApiKey : getSubmissionTimeValue("sttApiKey", "");
 		expression&lt;rstring&gt; $sttIAMTokenURL : 
 			getSubmissionTimeValue("sttIAMTokenURL", "https://iam.cloud.ibm.com/identity/token");
 		expression&lt;rstring&gt; $sttOnCP4DAccessToken : getSubmissionTimeValue("sttOnCP4DAccessToken", "");
 		expression&lt;rstring&gt; $audioDir : getSubmissionTimeValue("audioDir", "../../audio-files");
 		expression&lt;boolean&gt; $nonFinalUtterancesNeeded: (boolean)getSubmissionTimeValue("nonFinalUtterancesNeeded", "false");
 		expression&lt;int32&gt; $numberOfSTTEngines :(int32)
 			getSubmissionTimeValue("numberOfSTTEngines", "5") ;
 		expression&lt;rstring&gt; $sttUri : getSubmissionTimeValue("sttUri",
 			"wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize");
 		expression&lt;rstring&gt; $sttBaseLanguageModel : 
 			getSubmissionTimeValue("sttBaseLanguageModel", "en-US_NarrowbandModel");
 		expression&lt;rstring&gt; $contentType : 
 			getSubmissionTimeValue("contentType", "audio/wav");
 		expression&lt;rstring&gt; $baseModelVersion : 
 			getSubmissionTimeValue("baseModelVersion", "");
 		expression&lt;rstring&gt; $customizationId : 
 			getSubmissionTimeValue("customizationId", "");
 		expression&lt;rstring&gt; $acousticCustomizationId : 
 			getSubmissionTimeValue("acousticCustomizationId", "");
 		expression&lt;float64&gt; $customizationWeight : 
 			(float64)getSubmissionTimeValue("customizationWeight", "0.30");
 		expression&lt;int32&gt; $sttBatchSize : (int32)getSubmissionTimeValue("sttBatchSize", "0");
 		expression&lt;int32&gt; $maxUtteranceAlternatives : 
 			(int32)getSubmissionTimeValue("maxUtteranceAlternatives", "1");	
 		expression&lt;boolean&gt; $sttRequestLogging : 
 			(boolean)getSubmissionTimeValue("sttRequestLogging", "false");
 		expression&lt;boolean&gt; $filterProfanity : 
 			(boolean)getSubmissionTimeValue("filterProfanity", "false");
 		expression&lt;boolean&gt; $sttJsonResponseDebugging : 
 			(boolean)getSubmissionTimeValue("sttJsonResponseDebugging", "false");
 		expression&lt;float64&gt; $wordAlternativesThreshold : 
 			(float64)getSubmissionTimeValue("wordAlternativesThreshold", "0.0");
 		expression&lt;boolean&gt; $smartFormattingNeeded : 
 			(boolean)getSubmissionTimeValue("smartFormattingNeeded", "false");
 		expression&lt;float64&gt; $keywordsSpottingThreshold : 
 			(float64)getSubmissionTimeValue("keywordsSpottingThreshold", "0.0");
 		expression&lt;list&lt;rstring&gt;&gt; $keywordsToBeSpotted : 
 			(list&lt;rstring&gt;)getSubmissionTimeValue("keywordsToBeSpotted", "[]");	
 		expression&lt;boolean&gt; $websocketLoggingNeeded : 
 			(boolean)getSubmissionTimeValue("websocketLoggingNeeded", "false");
 		expression&lt;float64&gt; $cpuYieldTimeInAudioSenderThread : 
 			(float64)getSubmissionTimeValue("cpuYieldTimeInAudioSenderThread", "0.001");
 		expression&lt;boolean&gt; $sttLiveMetricsUpdateNeeded : 
 			(boolean)getSubmissionTimeValue("sttLiveMetricsUpdateNeeded", "true");
 
 	type
 		// This STT result type contains many attributes to
 		// demonstrate all the basic and very advanced features of 
 		// the Watson STT service. Not all real life applications will need 
 		// all these attributes. You can decide to include or omit these
 		// attributes based on the specific STT features your application will need. 
 		// Trimming the unused attributes will also help in 
 		// reducing the STT processing overhead and in turn 
 		// help in receiving the STT results faster.
 		// Read the streamsx.sttgateway toolkit documentation to learn about
 		// what features are available, how they work and how different attributes are 
 		// related to those features.
 		STTResult_t = rstring conversationId, int32 utteranceNumber,
 			boolean finalizedUtterance,
 			boolean transcriptionCompleted,
 			rstring sttErrorMessage,
 			float64 confidence,
 			rstring utteranceText,
 			list&lt;rstring&gt; utteranceAlternatives, 
 			list&lt;list&lt;rstring&gt;&gt; wordAlternatives,
 			list&lt;list&lt;float64&gt;&gt; wordAlternativesConfidences,
 			list&lt;float64&gt; wordAlternativesStartTimes,
 			list&lt;float64&gt; wordAlternativesEndTimes,
 			list&lt;rstring&gt; utteranceWords,
 			list&lt;float64&gt; utteranceWordsConfidences,
 			list&lt;float64&gt; utteranceWordsStartTimes,
 			list&lt;float64&gt; utteranceWordsEndTimes,
 			float64 utteranceStartTime,
 			float64 utteranceEndTime,
 			list&lt;int32&gt; utteranceWordsSpeakers,
 			list&lt;float64&gt; utteranceWordsSpeakersConfidences,
 			map&lt;rstring, list&lt;map&lt;rstring, float64&gt;&gt;&gt; keywordsSpottingResults,
 			int32 sequence;
 
 	graph
 		// IMPORTANT: IBM STT service on public cloud requires
 		// an unexpired valid IAM access token to perform the 
 		// speech to text task in a secure manner. You may either 
 		// provide the token in parameter sttOnCP4DAccessToken, or provide 
 		// the parameter sttIAMTokenURL and sttApiKey and the operator IAMAccessTokenGenerator
 		// generates a new access token and then periodically refresh it. 
 		// Output stream of this composite operator is connected to the
 		// second input stream of the WatsonSTT operator that is used below.
 		// For a correct STT operation, user must set only one of these two
 		// submission time parameters to a non-empty value: sttAPIKey or sttOnCP4DAccessToken.
 		(stream&lt;IAMAccessToken&gt; IamAccessToken as IAT)
 			as IamAccessTokenGenerator = IAMAccessTokenGenerator() {
 			param
 				// This operator takes these four parameters.
 				apiKey: $sttApiKey;
 				iamTokenURL: $sttIAMTokenURL;
 				accessToken: $sttOnCP4DAccessToken;
 				// All connection parameter are taken from params.
 				// So if we clean the app config name, we avoid error logs
 				appConfigName: "";
 		}
 
 		// Scan a directory periodically to pick up the audio files and 
 		// send the name of that audio file to the WatsonSTT operator.
 		//
 		// The output stream from this operator will be connected to the
 		// WatsonSTT operator's input stream. That operator expects one of its
 		// input stream attributes to be named as 'speech' with an SPL type of
 		// either rstring or blob. That operator can take as input an 
 		// audio filename (rstring) or raw binary audio data (blob).
 		// In this example, we have to simply send the name of the audio file to the
 		// WatsonSTT operator which will read the binary audio data on its own from that file.
 		// In addition to that mandatory input stream attribute, additional
 		// attributes can be sent in the input stream to be auto assigned to
 		// the output stream of the WatsonSTT operator.
 		stream&lt;rstring speech, int32 sequence&gt; AudioFileName as O = DirectoryScan() {
 			logic state:
 				mutable int32 counter = 0;
 			param
 				directory : $audioDir;
 				pattern : "\\.wav$";
 				// Give sufficient delay here so that the
 				// previous operator can complete generating the
 				// IAM access token and send it to the WatsonSTT operator.
 				sortBy: name;
 			output O:
 				sequence = counter++;
 			config
 				placement: partitionIsolation;
 		}
 		
 		// The WatsonSTT Operator required that each conversation end is flagged with
 		// Window Punctuation.
 		stream&lt;I&gt; AudioFileNameFlagged as O = Punctor(AudioFileName as I) {
 			param
 				punctuate: true;
 				position: after;
 		}
 		
 		// Invoke one or more instances of the WatsonSTT operator.
 		// Avoid feeding audio data coming from more than one data source into this 
 		// parallel region which may cause erroneous transcription results.
 		// NOTE: The WatsonSTT operator allows fusing multiple instances of
 		// this operator into a single PE. This will help in reducing the 
 		// total number of CPU cores used in running the application.
 		// First input stream into this operator is the fully qualified audio file name.
 		// Second input stream into this operator is your STT service instance's IAM access token.
 		@parallel(width = $numberOfSTTEngines, broadcast=[AT])
 		stream&lt;STTResult_t&gt; STTResult = 
 			WatsonSTT(AudioFileNameFlagged as AFN; IamAccessToken as AT) {
 			logic
 				state: {
 					mutable int32 _conversationCnt = 0;
 				}
 				
 				onTuple AFN: {
 					printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " Channel " + (rstring)getChannel() + 
 						", Speech input " + (rstring)++_conversationCnt +
 						": " + (rstring)AFN.speech);
 				}
 			
 			// Just to demonstrate, we are using all the operator parameters below.
 			// Except for the first three parameters, every other parameter is an
 			// optional one. In real-life applications, such optional parameters
 			// can be omitted unless you want to change the default behavior of them.
 			param
 				sttResultMode: partial;
 				nonFinalUtterancesNeeded: $nonFinalUtterancesNeeded;
 				uri: $sttUri;
 				baseLanguageModel: $sttBaseLanguageModel;
 				contentType: $contentType;
 				sttRequestLogging: $sttRequestLogging;
 				filterProfanity: $filterProfanity;
 				sttJsonResponseDebugging: $sttJsonResponseDebugging;
 				maxUtteranceAlternatives: $maxUtteranceAlternatives;
 				wordAlternativesThreshold: $wordAlternativesThreshold;
 				smartFormattingNeeded: $smartFormattingNeeded;
 				keywordsSpottingThreshold: $keywordsSpottingThreshold;
 				keywordsToBeSpotted: $keywordsToBeSpotted;
 				websocketLoggingNeeded: $websocketLoggingNeeded;
 				cpuYieldTimeInAudioSenderThread: $cpuYieldTimeInAudioSenderThread;
 				sttLiveMetricsUpdateNeeded : $sttLiveMetricsUpdateNeeded;
 				
 				// Use the following operator parameters as needed.
 				// Point to a specific version of the base model if needed.
 				//
 				// e-g: "en-US_NarrowbandModel.v07-06082016.06202016"
 				baseModelVersion: $baseModelVersion;
 				// Language model customization id to be used for the transcription.
 				// e-g: "74f4807e-b5ff-4866-824e-6bba1a84fe96"
 				customizationId: $customizationId;
 				// Acoustic model customization id to be used for the transcription.
 				// e-g: "259c622d-82a4-8142-79ca-9cab3771ef31"
 				acousticCustomizationId: $acousticCustomizationId;
 				// Relative weight to be given to the words in the custom Language model.
 				customizationWeight: $customizationWeight;
 			
 			// Just for demonstrative purposes, we are showing below the output attribute
 			// assignments using all the available custom output functions. In your
 			// real-life applications, it is sufficient to do the assignments via
 			// custom output functions only as needed.
 			//
 			// Some of the important output functions that must be used to check
 			// the result of the transcription are:
 			// getSTTErrorMessage --&gt; It tells whether the transcription succeeded or not.
 			// isFinalizedUtterance --&gt; In sttResultMode partial, it tells whether this is a 
 			//                          partial utterance or a finalized utterance.
 			// isTranscriptionCompleted --&gt; It tells whether the transcription is 
 			//                              completed for the current audio conversation or not.
 			//
 			output
 				STTResult: conversationId = speech, 
 					utteranceNumber = getUtteranceNumber(),
 					utteranceText = getUtteranceText(),
 					finalizedUtterance = isFinalizedUtterance(),
 					confidence = getConfidence(),
 					sttErrorMessage = getSTTErrorMessage(),
 					transcriptionCompleted = isTranscriptionCompleted(),
 					// n-best utterance alternative hypotheses.
 					utteranceAlternatives = getUtteranceAlternatives(),
 					// Confusion networks (a.k.a. Consensus)
 					wordAlternatives = getWordAlternatives(),
 					wordAlternativesConfidences = getWordAlternativesConfidences(),
 					wordAlternativesStartTimes = getWordAlternativesStartTimes(),
 					wordAlternativesEndTimes = getWordAlternativesEndTimes(),
 					utteranceWords = getUtteranceWords(),
 					utteranceWordsConfidences = getUtteranceWordsConfidences(),
 					utteranceWordsStartTimes = getUtteranceWordsStartTimes(),
 					utteranceWordsEndTimes = getUtteranceWordsEndTimes(),
 					utteranceStartTime = getUtteranceStartTime(),
 					utteranceEndTime = getUtteranceEndTime(),
 					// Speaker label a.k.a. Speaker id
 					utteranceWordsSpeakers = getUtteranceWordsSpeakers(),
 					utteranceWordsSpeakersConfidences = getUtteranceWordsSpeakersConfidences(),
 					// Results from keywords spotting (matching) in an utterance.
 					keywordsSpottingResults = getKeywordsSpottingResults();
 
 			// If needed, you can decide not to fuse the WatsonSTT operator instances and
 			// keep each instance of this operator on its own PE (a.k.a Linux process) by
 			// removing the block comment around this config clause.
 			/*
 			config
 				placement : partitionExlocation("sttpartition");
 			*/
 		}
 		
 		// In a real-life application, there will be additional operators here with the 
 		// necessary logic to look inside the tuples arriving on the STTResult stream and
 		// analyze different kinds of speech to text result attributes returned from the STT service.
 		// 
 		// But, in this simple example we will only collect the results arriving from the 
 		// WatsonSTT operator and display them on stdout.
 		() as MySink1 = Custom(STTResult as SR) {
 			logic
 				state: {
 					mutable int32 _conversationCnt = 0;
 				}
 				
 				onTuple SR: {
 					// If the user gave us a non-zero batch size, we will print a single line
 					// about the batch completion at the very end. In that case, we will 
 					// avoid printing every individual transcription result. This will not
 					// make the file system very busy in the Distributed Mode execution.
 					// This is only useful if someone is interested in doing a 
 					// timing measurement to see how long it takes to transcribe a 
 					// known batch of audio conversations with the sttResultMode operator
 					// parameter to set to a value of complete (get the full text after the
 					// entire conversation is transcribed).
 					if ($sttBatchSize &gt; 0) {
 						++_conversationCnt;
 						
 						if (_conversationCnt == 1) {
 							printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " STT result for conversation " + (rstring) _conversationCnt
 								+ " " + conversationId + " arrived.");
 						} else if (_conversationCnt == $sttBatchSize) {
 							printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " STT result for conversation " + (rstring) _conversationCnt
 								+ " " + conversationId + " arrived.");
 						}
 					} else {
 						// User didn't give a non-zero batch size. In that case,
 						// we will print every transcription result, which will make the
 						// file system busy in the Distributed Mode execution.
 						printStringLn(hourMinuteSecondMillisec(getTimestamp()) + " " + (rstring)++_conversationCnt + 
 							") STT result: " + (rstring)SR);
 					}
 				}
 				
 			config
 				//Place this operator into an own PE to separate the final result printouts from all other printouts
 				placement: partitionIsolation;
 				threadedPort: queue(SR, Sys.Wait);
 		}
 
 	config restartable: false;
 } // End of composite AudioFileWatsonSTT (Main composite)

   </pre>

</div>

</div>


</body>
</html>