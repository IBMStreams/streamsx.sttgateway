use com.ibm.streamsx.sttgateway.watson::IAMAccessTokenGenerator;
use com.ibm.streamsx.sttgateway.watson::IAMAccessToken;
use com.ibm.streamsx.sttgateway.watson::WatsonSTT;
use com.ibm.streamsx.testframe::FileSink1;

composite WatsonSTTFileOutFunct {
	param
		expression<rstring> $apiKey :      getSubmissionTimeValue("apiKey", "invalid");
		expression<rstring> $audioDir:     getSubmissionTimeValue("audioDir");
		
		expression<rstring> $sttBaseLanguageModel : getSubmissionTimeValue("sttBaseLanguageModel", "en-US_NarrowbandModel");
		expression<rstring> $contentType : getSubmissionTimeValue("contentType", "audio/wav");
		expression<int32> $sttResultMode : (int32)getSubmissionTimeValue("sttResultMode");
		expression<int32> $maxUtteranceAlternatives : (int32)getSubmissionTimeValue("maxUtteranceAlternatives", "1");	
		expression<rstring> $iamTokenURL : getSubmissionTimeValue("iamTokenURL", "https://iam.cloud.ibm.com/identity/token");
		expression<rstring> $uri : getSubmissionTimeValue("uri", "wss://stream.watsonplatform.net/speech-to-text/api/v1/recognize");
		
		
		expression<list<rstring>> $filesList :
				["01-call-center-10sec.wav", "02-call-center-25sec.wav", "03-call-center-28sec.wav",
				"04-empty-audio.wav", "05-gettysburg-address-2min.wav", "07-ibm-earnings-2min.wav",
				"08-ibm-watson-ai-3min.wav", "10-invalid-audio.wav", "12-jfk-speech-12sec.wav"];
				
	type
		STTResult = rstring conversationId, int32 utteranceNumber,
			rstring utteranceText, boolean finalizedUtterance,
			float32 confidence, rstring fullTranscriptionText,
			rstring sttErrorMessage,
			//<!*Mode3>list<rstring> utteranceAlternatives,
			//<!*Mode3>list<list<rstring>> wordAlternatives,
			//<!*Mode3>list<list<float64>> wordAlternativesConfidences,
			//<!*Mode3>list<float64> wordAlternativesStartTimes,
			//<!*Mode3>list<float64> wordAlternativesEndTimes,
			//<!*Mode3>list<rstring> utteranceWords,
			//<!*Mode3>list<float64> utteranceWordsConfidences,
			//<!*Mode3>list<float64> utteranceWordsStartTimes,
			//<!*Mode3>list<float64> utteranceWordsEndTimes,
			float64 utteranceStartTime,
			float64 utteranceEndTime,
			//<!*Mode3>list<int32> utteranceWordsSpeakers,
			//<!*Mode3>list<float64> utteranceWordsSpeakersConfidences,
			map<rstring, list<map<rstring, float64>>> keywordsSpottingResults,
			boolean transcriptionCompleted,
			uint64 myseq;
		
	graph
		
		stream<rstring speech, uint64 myseq> FileNameStream as O = Beacon() {
			param
				iterations: size($filesList);
				//<!*TokenDelay>initDelay: 5.0;
			output O:
				speech = $audioDir + "/" + $filesList[IterationCount()],
				myseq = IterationCount();
			config
				//<fused*>placement : partitionColocation("somePartitionColocationId");
				//<unFused*>placement : partitionIsolation;
		}
		
		stream<I> FileNameStreamPunct as O = Punctor(FileNameStream as I) {
			param
				punctuate : true;
				position: after;
			config
				//<fused*>placement : partitionColocation("somePartitionColocationId");
				//<unFused*>placement : partitionIsolation;
		}
		
		stream<IAMAccessToken> IAMAccessTokenStream = IAMAccessTokenGenerator() {
			param
				appConfigName: "";
				apiKey: $apiKey;
				iamTokenURL: $iamTokenURL;
				//<*TokenDelay>initDelay: 10.0;
			config
				//<fused*>placement : partitionColocation("somePartitionColocationId");
				//<unFused*>placement : partitionIsolation;
		}

		stream<STTResult> STTResultStream as O = WatsonSTT(FileNameStreamPunct as I; IAMAccessTokenStream) {
			param
				uri: $uri;
				baseLanguageModel: $sttBaseLanguageModel;
				contentType: $contentType;
				sttResultMode: $sttResultMode;
				
			output O:
				conversationId = speech, 
				utteranceNumber = getUtteranceNumber(),
				utteranceText = getUtteranceText(),
				finalizedUtterance = isFinalizedUtterance(),
				//<!*Mode3>confidence = getConfidence(),
				fullTranscriptionText = getFullTranscriptionText(),
				sttErrorMessage = getSTTErrorMessage(),
				transcriptionCompleted = isTranscriptionCompleted(),
				// n-best utterance alternative hypotheses.
				//<!*Mode3>utteranceAlternatives = getUtteranceAlternatives(),
				// Confusion networks (a.k.a. Consensus)
				//<!*Mode3>wordAlternatives = getWordAlternatives(),
				//<!*Mode3>wordAlternativesConfidences = getWordAlternativesConfidences(),
				//<!*Mode3>wordAlternativesStartTimes = getWordAlternativesStartTimes(),
				//<!*Mode3>wordAlternativesEndTimes = getWordAlternativesEndTimes(),
				//<!*Mode3>utteranceWords = getUtteranceWords(),
				//<!*Mode3>utteranceWordsConfidences = getUtteranceWordsConfidences(),
				//<!*Mode3>utteranceWordsStartTimes = getUtteranceWordsStartTimes(),
				//<!*Mode3>utteranceWordsEndTimes = getUtteranceWordsEndTimes(),
				utteranceStartTime = getUtteranceStartTime(),
				utteranceEndTime = getUtteranceEndTime(),
				// Speaker label a.k.a. Speaker id
				//<!*Mode3>utteranceWordsSpeakers = getUtteranceWordsSpeakers(),
				//<!*Mode3>utteranceWordsSpeakersConfidences = getUtteranceWordsSpeakersConfidences(),
				// Results from keywords spotting (matching) in an utterance.
				keywordsSpottingResults = getKeywordsSpottingResults();
			config
				//<fused*>placement : partitionColocation("somePartitionColocationId");
				//<unFused*>placement : partitionIsolation;
			
		}
		
		() as Sink = FileSink1(STTResultStream) {
			config
				//<fused*>placement : partitionColocation("somePartitionColocationId");
				//<unFused*>placement : partitionIsolation;
		}

	config
		restartable: false;
}
